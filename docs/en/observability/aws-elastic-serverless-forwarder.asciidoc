:aws: AWS

[[aws-elastic-serverless-forwarder]]
= Elastic Serverless Forwarder

++++
<titleabbrev>AWS Elastic Serverless Forwarder</titleabbrev>
++++

The Elastic Serverless Forwarder is an Amazon Web Services ({aws}) Lambda function that ships logs from your {aws} environment to Elastic.

[discrete]
[[aws-serverless-forwarder-overview]]
= Overview
Th Elastic Serverless Forwarder can forward {aws} data to self-managed or cloud-hosted Elastic environments. It supports the following inputs:

- Amazon S3 SQS (Simple Queue Service) event notifications
- Amazon Kinesis Data Streams
- Amazon CloudWatch Logs subscription filters
- Direct Amazon SQS message payload

image::aws-serverless-lambda-flow.png[AWS Lambda flow]

The Lambda deployment sets up an SQS queue automatically. This is used to trigger a new function invocation that Lambda uses to continue from exactly where the last function run was terminated. By default a Lambda function runs for a maximum of 15 minutes. When processing event data thereâ€™s a possibility that the function may be exited by {aws} in the middle of processing. The function handles this scenario gracefully by keeping track of the last offset processed.

Any exception or fail scenarios related to ingestion are handled by Lambda gracefully using a replay queue. Data in the replay queue is stored as individual events. Lambda keeps track of any failed events and writes them to a replay queue that can then be consumed by adding an additional SQS trigger via Lambda.

You can use the <<sample-s3-config-file,config.yaml>> file to configure the service for each input type, including information such as SQS queue ARN and Elasticsearch connection details. You can create multiple input sections within the configuration file to map different inputs to specific log types.

The Lambda function also supports writing directly to an index, alias, or custom data stream. This enables existing Elasticsearch users to re-use index templates, ingest pipelines, or dashboards that are already created and connected to business processes.

The best way to get started is to install the appropriate https://docs.elastic.co/en/integrations[integrations] via Kibana. These integrations include pre-built dashboards, ingest node configurations, and other assets that help you get the most out of the data you ingest. The integrations use https://www.elastic.co/guide/en/elasticsearch/reference/current/data-streams.html[data streams] with specific https://www.elastic.co/blog/an-introduction-to-the-elastic-data-stream-naming-scheme[naming conventions] that provide you with more granular controls and flexibility on managing data ingestion.

[discrete]
[[aws-serverless-forwarder-inputs]]
= Inputs

[discrete]
[[aws-serverless-forwarder-inputs-direct]]
== Direct Amazon SQS message payload

The Lambda function can ingest logs contained within the payload of an Amazon SQS body record and send them to Elastic. The SQS queue serves as a trigger for the Lambda function. When a new record gets written to an SQS queue, the Lambda function triggers.

You can set up a separate SQS queue for each type of log. The config parameter for Elasticsearch output `es_datastream_name` is mandatory. If this value is set to an Elasticsearch data stream, the type of log must be correctly defined with configuration parameters. A single configuration file can have many input sections, pointing to different SQS queues that match specific log types.

[discrete]
[[aws-serverless-forwarder-inputs-s3]]
== Amazon S3 SQS Event Notifications

The Lambda function can ingest logs contained in an Amazon Simple Storage Service (S3) bucket through an SQS notification (`s3:ObjectCreated`) and send them to Elastic. The SQS queue serves as a trigger for the Lambda function. When a new log file is written to an S3 bucket and meets the criteria (as configured including prefix/suffix), an SQS notification is generated that triggers the Lambda function.

You can set up separate SQS queues for each type of log (`aws.vpcflow`, `aws.cloudtrail`, `aws.waf`, for example). A single configuration file can have many input sections, pointing to different SQS queues that match specific log types. The `es_datastream_name` parameter in the config file is optional. Lambda supports automatic routing of various {aws} service logs to the corresponding data streams for further processing and storage in the Elasticsearch cluster. It supports automatic routing of `aws.cloudtrail`, `aws.cloudwatch_logs`, `aws.elb_logs`, `aws.firewall_logs`, `aws.vpcflow`, and `aws.waf` logs.

For other log types, you can optionally set the `es_datastream_name` value in the configuration file according to the naming convention of the Elasticsearch data stream and integration.  If the `es_datastream_name` is not specified, and the log cannot be matched with any of the above {aws} services, then the dataset will be set to `generic` and the namespace set to `default`, pointing to the data stream name `logs-generic-default`.

For more information on creating SQS event notifications for S3 buckets, read the https://docs.aws.amazon.com/AmazonS3/latest/userguide/ways-to-add-notification-config-to-bucket.html[{aws} documentation].

NOTE: You must set a visibility timeout of `910` seconds for any SQS queues you want to use as a trigger. This is 10 seconds greater than the Elastic Serverless Forwarder Lambda timeout.

[discrete]
[[aws-serverless-forwarder-inputs-kinesis]]
== Amazon Kinesis Data Streams

The Lambda function can ingest logs contained in the payload of a Kinesis Data Stream record and send them to Elastic. The Kinesis Data Stream serves as a trigger for the Lambda function. When a new record gets written to a Kinesis Data Stream, the Lambda function triggers.

You can set up separate Kinesis Data Streams for each type of log. The `es_datastream_name` parameter in the config file is mandatory. If this value is set to an Elasticsearch data stream, the type of log must be correctly defined with configuration parameters. A single configuration file can have many input sections, pointing to different data streams that match specific log types.

[discrete]
[[aws-serverless-forwarder-inputs-cloudwatch]]
== Amazon CloudWatch Logs subscription filters

The Lambda function can ingest logs contained in the message payload of CloudWatch Logs events and send them to Elastic. The CloudWatch Logs service serves as a trigger for the Lambda function. When a new record gets written to a Kinesis data stream, the Lambda function triggers.

You can set up separate CloudWatch Logs groups for each type of log. The `es_datastream_name` parameter in the config file is mandatory. If this value is set to an Elasticsearch data stream, the type of log must be correctly defined with configuration parameters. A single configuration file can have many input sections, pointing to different CloudWatch Logs groups that match specific log types.

[discrete]
[[aws-serverless-forwarder-get-started]]
= Get started

- <<deploy-elastic-serverless-forwarder,Deploy Elastic Serverless Forwarder>>

- <<configure-elastic-serverless-forwarder,Configure Elastic Serverless Forwarder>>

- <<aws-serverless-use-tags-filters,Use tags and filters>>

- <<aws-serverless-json-content,JSON content discovery>>

- <<aws-serverless-manage-multiline-messages,Manage multiline messages>>

- <<aws-serverless-route-service-logs,Route AWS service logs>>

- <<aws-serverless-troubleshooting,Troubleshooting>>

:leveloffset: -2
include::deploy-elastic-serverless-forwarder.asciidoc[leveloffset=+3]

include::configure-elastic-serverless-forwarder.asciidoc[leveloffset=+3]

include::aws-serverless-use-tags-filters.asciidoc[leveloffset=+3]

include::aws-serverless-json-content.asciidoc[leveloffset=+3]

include::aws-serverless-manage-multiline-messages.asciidoc[leveloffset=+3]

include::aws-serverless-route-service-logs.asciidoc[leveloffset=+3]

include::aws-serverless-troubleshooting.asciidoc[leveloffset=+3]
:leveloffset: +2
