[discrete]
[[monitor-kubernetes-health-and-performance-metrics]]
== Part 2: Monitor health and performance metrics

[Author: @ChrsMark]

Collecting metrics about Kubernetes clusters and the workloads running on top of
them is a key aspect of Kubernetes observability. However, collecting metrics
from Kubernetes poses some challenges. You need to collect metrics about the
resources running on "physical" machines as well as the containers and pods.
Specifically, you need to monitor the health and performance of:

* The hosts where Kubernetes components are running. Each host produces metrics
like CPU, memory, disk utilization, and disk and network I/O.

* Kubernetes containers, which produce their own set of metrics.

* The applications running as Kubernetes Pods, such as application servers and
databases, each producing its own set of metrics.

Instead of using multiple technologies to collect metrics, you deploy
{metricbeat} to monitor all layers of your technology stack.

[discrete]
=== Deploy {metricbeat} to collect metrics

You'll use {metricbeat} to collect metrics from Pods running in your Kubernetes
cluster as well as metrics from the Kubernetes cluster itself.

{metricbeat} modules provide a quick and easy way to pick up metrics from
various sources and ship them to {es} as {ecs-ref}/index.html[ECS]-compatible
events, ready to be correlated with logs, uptime and APM data.

To deploy {metricbeat} to your Kubernetes cluster:

. **Download the {metricbeat} deployment manifest.**
+
["source", "sh", subs="attributes"]
------------------------------------------------
curl -L -O https://raw.githubusercontent.com/elastic/beats/{branch}/deploy/kubernetes/metricbeat-kubernetes.yaml
------------------------------------------------

. **Set the connection information for {es}.**
+
By default, {metricbeat} sends events to an existing {es} deployment, if present.
To specify a different destination, change the following parameters in the
manifest file:
+
[source,yaml]
------------------------------------------------
- name: ELASTICSEARCH_HOST
value: elasticsearch
- name: ELASTICSEARCH_PORT
value: "9200"
- name: ELASTICSEARCH_USERNAME
value: elastic
- name: ELASTICSEARCH_PASSWORD
value: changeme
------------------------------------------------
+
Those settings can be consumed by a Kubernetes secret. To
create a secret:
+
["source", "sh", subs="attributes"]
------------------------------------------------
$ echo -n 'changeme' | base64
Y2hhbmdlbWU=
$ kubectl create secret generic es-secret --from-literal='password=Y2hhbmdlbWU='
------------------------------------------------
+
Use the secret value in {metricbeat}'s env:
+
[source,yaml]
+
------------------------------------------------
env:
- name: ELASTICSEARCH_PASSWORD
valueFrom:
secretKeyRef:
name: es-secret
key: password
------------------------------------------------

. FIND THE RIGHT PLACE FOR THIS
+
As you can see, {metricbeat} will run on each node in the cluster as a Daemonset's Pod.
To collect system-level metrics, various key paths are mounted from the host to
the Pod:
+
[source,yaml]
------------------------------------------------
- name: proc
  hostPath:
  path: /proc
- name: cgroup
  hostPath:
  path: /sys/fs/cgroup
------------------------------------------------

. **Collect system metrics.**
+
To collect system-level metrics from the running node, configure the system
module: 
+
[source,yaml]
------------------------------------------------
- module: system
  period: 10s
  metricsets:
    - cpu
    - load
    - memory
    - network
    - process
    - process_summary
------------------------------------------------

. *Collect metrics from each Kubernetes node.*
+
Because {metribceat} is running on each node, you can collect metrics from the
Kubelet API. These metrics provide important information about the state of the
Kubernetes Node, Pods, Containers, and other resources. 
+
To collect these metrics, enable the following metricsets in the `kubernetes`
module: //TODO: Indicate if these are enabled by default in the deployment manifest.
+
[source,yaml]
------------------------------------------------
- module: kubernetes
  metricsets:
    - node
    - system
    - pod
    - container
    - volume
------------------------------------------------
+
These metricsets collect metrics from the Kubelet API and therefore require
access to the specific endpoints. Depending on the version and configuration of
Kubernetes nodes, kubelet might provide a read-only HTTP port (typically
10255), which is used in some configuration examples. But in general, and
lately, this endpoint requires SSL (HTTPS) access (to port 10250 by default) and
token-based authentication.

. **Collect Kubernetes state metrics.**
+
To collect state metrics, enable metricsets that begin with the `state_` prefix.
Set the `hosts` field to point to the kube-state-metrics service within the
cluster.
+
Because the kube-state-metrics service provides cluster-wide metrics, there’s no
need to fetch them per node. In order to ensure this singleton approach, {metricbeat}
leverages a leader election method, where one Pod holds a leader lock and is
responsible for collecting cluster-wide metrics. For more information about
leader election settings, see
{metricbeat-ref}}/configuration-autodiscover.html[Autodiscover]. //TODO: Improve this para.
+
[source,yaml]
------------------------------------------------
metricbeat.autodiscover:
    providers:
    - type: kubernetes
      scope: cluster
      node: ${NODE_NAME}
      unique: true
      templates:
        - config:
            - module: kubernetes
              hosts: ["kube-state-metrics:8080"]
              period: 10s
              add_metadata: true
              metricsets:
                - state_node
                - state_deployment
                - state_daemonset
                - state_replicaset
                - state_pod
                - state_container
                - state_cronjob
                - state_resourcequota
                - state_statefulset
------------------------------------------------
+
NOTE: If your Kubernetes cluster contains a large number of large nodes, the Pod
that collects cluster-level metrics might face performance issues caused by
resource limitations. In this case, avoid using the leader election strategy and
instead run a dedicated, standalone {metribceat} instance using a Deployment in
addition to the DaemonSet.

. **Collect application-specific metrics (use hint-based autodiscovery).**
+
{metricbeat} supports autodiscovery based on hints from the provider. The hints
system looks for hints in Kubernetes Pod annotations or Docker labels that have
the prefix `co.elastic.metrics`. When a container starts, {metricbeat} checks
for hints and launches the proper configuration. The hints tell {metricbeat} how
to get metrics for the given container. To enable hint-based autodiscovery, set
`hints.enabled: true`:
+
[source,yaml]
------------------------------------------------
metricbeat.autodiscover:
  providers:
    - type: kubernetes
      hints.enabled: true
------------------------------------------------
+
You can annotate Kubernetes Pods with useful info to spin up {metricbeat}
modules:
+
[source,yaml]
------------------------------------------------
apiVersion: v1
kind: Pod
metadata:
    name: nginx-autodiscover
    annotations:
        co.elastic.metrics/module: nginx
        co.elastic.metrics/metricsets: stubstatus
        co.elastic.metrics/hosts: '${data.host}:80'
        co.elastic.metrics/period: 10s
------------------------------------------------

. **Collect metrics from Prometheus.**
+
To enrich your collection resources, you can use the Prometheus module to
collect metrics from every application that runs on the cluster and exposes a
Prometheus exporter. For instance, let's say that the cluster runs multiple
applications that expose Prometheus metrics with the default Prometheus
standards. Assuming these applications are annotated properly, you can define
an extra autodiscover provider to automatically identify the applications and
start collecting exposed metrics by using the Prometheus module:
+
[source,yaml]
------------------------------------------------
metricbeat.autodiscover:
  providers:
    - type: kubernetes
      include_annotations: ["prometheus.io.scrape"]
      templates:
        - condition:
            contains:
              kubernetes.annotations.prometheus.io/scrape: "true"
          config:
            - module: prometheus
              metricsets: ["collector"]
              hosts: "${data.host}:${data.port}"
------------------------------------------------
+
This configuration launches a prometheus module for all containers of pods
annotated with `prometheus.io/scrape=true`.

. **Add metadata to events.** 
+
{metricbeat} provides processors that you can use in your configuration to
enrich events with metadata coming from Docker, Kubernetes, hosts, and cloud
providers. For example:
+
[source,yaml]
------------------------------------------------
processors:
- add_cloud_metadata:
- add_host_metadata:
------------------------------------------------
+
This metadata allows correlation of metrics with the hosts, Kubernetes pods,
Docker containers, and cloud-provider infrastructure metadata and with other
pieces of observability puzzle, such as application performance monitoring data
and logs.

. **Deploy {metricbeat} as a DaemonSet on Kubernetes.**
+
{metricbeat} gets some metrics from
https://github.com/kubernetes/kube-state-metrics#usage[kube-state-metrics].
If kube-state-metrics is not already running, deploy it now. To learn how,
see the Kubernetes deployment
https://github.com/kubernetes/kube-state-metrics#kubernetes-deployment[docs])
+
To deploy {metricbeat} to Kubernetes, run:
+
[source,shell]
------------------------------------------------
kubectl create -f metricbeat-kubernetes.yaml
------------------------------------------------
+
To check the status, run:
+
[source,shell]
------------------------------------------------
$ kubectl --namespace=kube-system  get ds/metricbeat

NAME       DESIRED   CURRENT   READY     UP-TO-DATE   AVAILABLE   NODE-SELECTOR   AGE
metricbeat   32        32        0         32           0           <none>          1m
------------------------------------------------
+
Metrics should start flowing to {es}.

//DO WE NEED TO SHOW THIS? 

[discrete]
==== Red Hat OpenShift configuration

If you're using Red Hat OpenShift, you need to specify additional settings in
the manifest file and enable the container to run as privileged.

. Modify the `DaemonSet` container spec in the manifest file:
+
[source,yaml]
-----
  securityContext:
    runAsUser: 0
    privileged: true
-----

. In the manifest file, edit the metricbeat-daemonset-modules ConfigMap, and
specify the following settings under `kubernetes.yml` in the data section:
+
[source,yaml]
-----
kubernetes.yml: |-
    - module: kubernetes
      metricsets:
        - node
        - system
        - pod
        - container
        - volume
      period: 10s
      host: ${NODE_NAME}
      hosts: ["https://${NODE_NAME}:10250"]
      bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
      ssl.certificate_authorities:
        - /path/to/kubelet-service-ca.crt
-----
+
NOTE: `kubelet-service-ca.crt` can be any CA bundle that contains the issuer of
the certificate used in the Kubelet API. According to each specific installation
of Openshift this can be found either in secrets or in configmaps. In some
installations it can be available as part of the service account secret, in
`/var/run/secrets/kubernetes.io/serviceaccount/service-ca.crt`. If you're using
the
https://github.com/openshift/installer/blob/master/docs/user/gcp/install.md[Openshift
installer] for GCP then the following configmap can be mounted in {metricbeat}
Pod and use `ca-bundle.crt` in `ssl.certificate_authorities`:
+
[source,yaml]
----
Name:         kubelet-serving-ca
Namespace:    openshift-kube-apiserver
Labels:       <none>
Annotations:  <none>

Data
====
ca-bundle.crt:
----

. Under the `metricbeat` ClusterRole, add the following resources:
+
[source,yaml]
-----
- nodes/metrics
- nodes/stats
-----

. Grant the `metricbeat` service account access to the privileged SCC:
+
[source,shell]
-----
oc adm policy add-scc-to-user privileged system:serviceaccount:kube-system:filebeat
-----
+
This command enables the container to be privileged as an administrator for
OpenShift.

. Override the default node selector for the `kube-system` namespace (or your
custom namespace) to allow for scheduling on any node:
+
[source,shell]
----
oc patch namespace kube-system -p \
'{"metadata": {"annotations": {"openshift.io/node-selector": ""}}}'
----
+
This command sets the node selector for the project to an empty string. If you
don't run this command, the default node selector will skip master nodes.


[discrete]
=== View performance and health metrics

The {metricbeat} configuration in this tutorial drives the following views in the
https://www.elastic.co/infrastructure-monitoring[Metrics app].

//TODO: Add specific guided steps here.

Feel free to click around and review those. Notice how everywhere you go in
{kib}, there is a search bar that allows you to, you know, search for things.
It’s a great way to filter the views and zoom into things when you are
looking for that needle in a haystack. In this tutorial, there's only one host:

//TODO: add screenshots

[discrete]
==== Out-of-the-box {kib} dashboards

{metricbeat} ships with a variety of pre-built {kib} dashboards that you can
add to your cluster with
a single setup command.

//TODO: Add more info about the setup required. We should probably just document
//the command here.

You can use these dashboards as they are, or as a starting point for custom
dashboards tailored to your needs. Here are dashboards that will help clearly
display the data from your tutorial environment.
