[discrete]
[[kubernetes-monitoring-architecture]]
== Monitoring architecture

The {stack} provides 4 main components for monitoring Kubernetes:

1. Lightweight agents, called {beats}, to collect observability data. Some
{beats} include pre-configured data collection modules to ease the collection
and parsing of data for common applications such as Apache, MySQL, and Kafka.

2. APM (described later) to monitor, detect, and diagnose complex application
performance issues.

3. {es} for storing and searching your data.

4. Observability apps in {kib} for visualizing and managing your observability
data.

//REVIEWERS: I just threw some icons into this diagram for services that
//users might want to monitor. Let me know if I'm putting together services that
// would never run in the same pod in the real world.

image::images/k8s-monitoring-architecture.png[Kubernetes monitoring architecture]

//TODO: Need work APM into the diagram, or mention that it's not covered.

{beats} agents are deployed to Kubernetes as DaemonSets. This deployment
architecture ensures that the agents are available to capture both system and
application-level observability data.

**{filebeat}**: Collects logs from pods, containers, and applications running on
Kubernetes.

{filebeat} communicates with the Kubernetes API server to retrieve information
about the pods running on the host, all the metadata annotations, and the
location of the log files.

When autodiscovery is configured, {filebeat} automatically discovers what
kind of components are running in a pod and applies the logging modules needed
to capture logs for those components.

**{metricbeat}**: Collects and preprocesses system and service metrics, such as
information about running processes, as well as CPU, memory, disk, and network
utilization numbers.

Because {metricbeat} runs on each node, it can collect metrics from the Kubelet
API. These metrics provide important information about the state of the
Kubernetes nodes, pods, containers, and other resources.

For cluster-wide metrics, {metricbeat} accesses the kube-state-metrics
service directly or gets metrics scraped by Prometheus.

When hints-based autodiscovery is configured, {metricbeat} looks for hints
in Kubernetes pod annotations or Docker labels and launches the proper
configuration to collect application metrics.


**Other {beats} (not shown)**: Collect and process other types of data, such as
Uptime data and network traffic.

[discrete]
[[beats-metadata]]
=== Metadata

All {beats} agents provide processors for adding metadata to events. The
metadata is valuable for grouping and exploring related data. For example, when
you're analyzing container logs, you want to know the host and container name,
and you want to be able to correlate logs, metrics, and traces.

The default deployments include processors, when needed, for enriching events
with cloud, Kubernetes, and host metadata.

image::images/metadata-processors.png[Metadata processors for cloud, Kubernetes, and host metadata]

//REVIEWERS: Can you confirm that the info in the diagram is correct? I want
//to provide examples without giving field names in case field names change
//again.

Now that you have a basic understanding of the monitoring architecture, let's
learn how to deploy monitoring to your Kubernetes environment.

[discrete]
== Before you begin

To monitor Kubernetes, you need {es} for storing and searching your
observability data, and {kib} for visualizing and managing it. For more
information, see <<spin-up-stack>>.

//TODO: Provide setup instructions for the Petclinic sample app???
