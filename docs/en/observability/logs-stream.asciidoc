[[logs-stream]]
= Stream a log file

In this guide, you'll learn how to send a log file to Elasticsearch using a standalone {agent}, configure the {agent} and your data streams using the `elastic-agent.yml` file, and query your logs using the data streams you've set up. 

[discrete]
[[logs-stream-prereq]]
== Prerequisites

include::logs-metrics-get-started.asciidoc[tag=monitoring-prereqs]

[discrete]
[[logs-stream-install-config-agent]]
== Install and configure the standalone {agent}

Complete the following steps to install and configure the standalone {agent} and send your log data to {es}:

. <<logs-stream-extract-agent, Download and extract the {agent} installation package.>>
. <<logs-stream-install-agent, Install and start the {agent}.>>
. <<logs-stream-agent-config, Configure the {agent}.>>

[discrete]
[[logs-stream-extract-agent]]
=== Step 1: Download and extract the {agent} installation package

On your host, download and extract the installation package that corresponds with your system:

include::{ingest-docs-root}/docs/en/ingest-management/tab-widgets/download-widget.asciidoc[]

[discrete]
[[logs-stream-install-agent]]
=== Step 2: Install and start the {agent}
After downloading and extracting the installation package, you're ready to install the {agent}. From the agent directory, run the install command that corresponds with your system:

NOTE: On macOS, Linux (tar package), and Windows, run the `install` command to
install and start {agent} as a managed service and start the service. The DEB and RPM
packages include a service unit for Linux systems with
systemd, For these systems, you must enable and start the service.

include::{ingest-docs-root}/docs/en/ingest-management/tab-widgets/run-standalone-widget.asciidoc[]

During installation, you're prompted with some questions:

. When asked if you want to install the agent as a service, enter `Y`. 
. When asked if you want to enroll the agent in Fleet, enter `n`.

[discrete]
[[logs-stream-agent-config]]
=== Step 3: Configure the {agent}

With your agent installed, configure it by updating the `elastic-agent.yml` file. 

[discrete]
[[logs-stream-yml-location]]
==== Locate your configuration file

After installing the agent, you'll find the `elastic-agent.yml` in one of the following locations according to your system:

include::tab-widgets/logs/agent-location-widget.asciidoc[]

[discrete]
[[logs-stream-example-config]]
==== Update your configuration file

The following is an example of a standalone {agent} configuration. To configure your {agent}, replace the contents of the `elastic-agent.yml` file with this configuration:

[source,yaml]
----
outputs:
  default:
    type: elasticsearch
    hosts: '<your-elasticsearch-endpoint>:<port>'
    api_key: 'your-api-key'
inputs:
  - id: your-log-id
    type: filestream
    streams:
      - id: your-log-stream-id
        data_stream:
          dataset: generic
        paths:
          - /var/log/your-logs.log
----

Next, set the values for the following fields:

. `hosts` – Copy the {es} endpoint from your deployment's page and add the port (the default port is `443`). For example, `https://my-deployment.es.us-central1.gcp.cloud.es.io:443`.
+
--
[role="screenshot"]
image::images/es-endpoint-cluster-id.png[{es} endpoint and cluster id location, 50%]
--
. `api-key` – Use an API key to grant the agent access to {es}. To create an API key for your agent, see {fleet-guide}/grant-access-to-elasticsearch.html#create-api-key-standalone-agent[Create API keys for standalone agents].
+
NOTE: The API key format should be `<id>:<key>`. Make sure you selected *Beats* when you created your API key. Base64 encoded API keys are not currently supported in this configuration.
. `inputs.id` – A unique identifier for your input.
. `type` – The type of input. For collecting logs, set this to `filestream`.
. `streams.id` – A unique identifier for your stream of log data. 
. `data_stream.dataset` – The name for your dataset data stream. You can name this data stream anything that signifies the source of the data. The default value is `generic`.
. `paths` – The path to your log files. You can also use a pattern like `/var/log/foo.log*`.

[discrete]
[[logs-stream-restart-agent]]
==== Restart the {agent}

After updating your configuration file, you need to restart the {agent}:

First, stop the {agent} and its related executables using the command that works with your system:

include::{ingest-docs-root}/docs/en/ingest-management/tab-widgets/stop-widget.asciidoc[]

Next, restart the {agent} using the command that works with your system:

include::{ingest-docs-root}/docs/en/ingest-management/tab-widgets/start-widget.asciidoc[]

[discrete]
[[logs-stream-query-datastreams]]
== View and search your data

With your {agent} and data streams configured, you can now view, filter, and search your log data. In {kib}, navigate to *Observability → Logs → Stream*, and use the search bar to search for your `data_stream.type` and `data_stream.dataset`. 

See the following examples for ways to search specific data types and datasets:

- `data_stream.type: logs` – shows `logs` data streams.
- `data_stream.dataset: nginx.access` – shows data streams with an `nginx.access` dataset.

The following example shows the search results for logs with an `apm.error` dataset and a `default` namespace:

--
[role="screenshot"]
image::images/stream-logs-example.png[example search query on the logs stream page in {kib}]
--

[discrete]
[[logs-stream-troubleshooting]]
== Troubleshooting 

If you're not seeing your log files in {kib}, check the following in the `elastic-agent.yml` file:

- Verify that the path to your logs file under `paths` is correct.
- Verify that your API key is in `<id>:<key>` format. If not, your API key may be in an unsupported format, and you'll need to create an API key in *Beats* format. 

If you're still running into issues, see {fleet-guide}/fleet-troubleshooting.html[{agent} troubleshooting] and {fleet-guide}/elastic-agent-configuration.html[Configure standalone Elastic Agents].

[discrete]
[[logs-stream-enhance-logs]]
== Get the most out of your log data

Now that you've sent your log data to {es}, this section helps you make sure you're getting the most out of it. You can do this by taking your unstructured log data and using ingest pipelines to extract structured fields. Extracting structured fields from your log data makes it easier to search, analyze, and filter your log data. 

Let's look at the following log data example:

[source,log]
----
2023-08-08T13:45:12.123Z WARN 192.168.1.101 Disk usage exceeds 90%.
----

This log is made up of the following potential fields:

- *timestamp*: "2023-08-08T13:45:12.123Z"
- *log.level*: "WARN"
- *host.ip*: "192.168.1.101"
- *message*: "Disk usage exceeds 90%."

Extracting these structured fields from your log data lets you search or filter your data in the following ways:

- *timestamp* – Extracting this field lets you sort logs by date and time. This is helpful when you want to view your logs in the order that they occurred or identify when issues happened.
- *log.level* – Extracting this field lets you filter logs by severity. This is helpful if you want to  focus on high-severity WARN or ERROR-level logs, and reduce noise by filtering out low-severity INFO-level logs.
- *host.ip* – Extracting this field lets you filter logs by the hosts' IP addresses. This is helpful if you want to focus on specific hosts that you’re having issues with or if you want to find disparities between hosts.

[discrete]
[[logs-stream-extract-timestamp]]
=== Extract the `@timestamp` field

To extract the `@timestamp` field, you'll use a ingest pipeline with a `dissect` processor and a `date` processor. 

The `dissect` processor takes your unstructured stream log message and puts it into structured fields. 

The `date` processor then takes the`timestamp` field from the dissect processor and turns it into a date field. The date processor can also set the timezone and change the output format.

If we look at the example log data from the previous section:

[source,log]
----
2023-08-08T13:45:12.123Z WARN 192.168.1.101 Disk usage exceeds 90%.
----

For the dissect processor, you need to set the following values:
//add link for dissect processor

- the `field` you're extracting data from (`message` in this case).
- the `pattern` of your log data. To extract the timestamp from the example, use the following pattern:
+
[source,JSON]
----
%{@timestamp} %{message}
----
+
This puts the timestamp (2023-08-08T13:45:12.123Z) in the `@timestamp` field and the rest of the message (WARN 192.168.1.101 Disk usage exceeds 90%.) in the `message` field.

For the `date` processor, you need to set following values:
//add link for date processor

- The `field` you're extracting the date from (`@timestamp` in this case).
- The `formats` of the current timestamp (`ISO8601` in this case). The date processor also accepts Java time patterns or the following formats: `ISO8601`, `UNIX`, `UNIX_MS`, `TAI64N`.

Optionally, you can set the following values for teh `date` processor:
- The `timezone` to use when parsing the date.
- The `output_format` if you want to change the format when writing the date to the `@timestamp` field. Must be a valid java time pattern.
//need link for java time pattern

Define your ingest pipeline with the dissect and date processors with the following:

[source,JSON]
----
PUT _ingest/pipeline/my-pipeline
{
  "description": "Pipeline to extract timestamp from log data",
  "processors": [
    {
      "dissect": {
        "field": "message",
        "patterns": "%{@timestamp} %{message}"
      }
    },
    {
      "date": {
        "field": "@timestamp",
        "formats": ["current-timestamp-format"],
        "timezone": ["desired-timezone"],
        "output_format": ["desired-timezone-output-format"]
      }
    }
  ]
}
----

[discrete]
[[logs-stream-extract-loglevel]]
=== Extract the `log.level` field



[discrete]
[[logs-stream-extract-hostip]]
=== Extract the `host.ip` field