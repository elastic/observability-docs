[[gcp-dataflow]]
=== GCP Dataflow templates 

In this tutorial, you'll learn how to ship logs directly from the Google Cloud
Console with the Dataflow template for analyzing GCP Audit Logs in the {stack}.

[discrete]
==== What you'll learn

You'll learn how to:

- Export GCP audit logs through Pub/Sub topics and subscriptions.
- Ingest logs using https://cloud.google.com/dataflow[Google Dataflow] and
view those logs in {kib}.

[discrete]
==== Before you begin

Create a deployment using our hosted {ess} on {ess-trial}[{ecloud}].
The deployment includes an {es} cluster for storing and searching your data,
and {kib} for visualizing and managing your data.

[discrete]
==== Step 1: Install the GCP integration 

Youâ€™ll start with installing the Elastic GCP integration to add pre-built
dashboards, ingest node configurations, and other assets that help you get
the most of the GCP logs you ingest.

. Go to *Integrations* in {kib} and search for `gcp`.
+
image::monitor-gcp-kibana-integrations.png[{kib} integrations]

. Click the Elastic Google Cloud Platform (GCP) integration to see more details about it, then click
*Add Google Cloud Platform (GCP)*.
+
image::monitor-gcp-integration.png[GCP integration]

. Click *Save integration*.

****
This tutorial assumes the Elastic cluster is already running. To continue, you'll need
your *Cloud ID* and an *API Key*.

To find the Cloud ID of your https://cloud.elastic.co/deployments[deployment],
go to the deployment's *Overview* page.

image:monitor-gcp-cloud-id.png[Cloud ID]

Use {kibana-ref}/api-keys.html#create-api-key[{kib}] to create a
Base64-encoded API key to authenticate on your deployment.

[IMPORTANT]
====
You can optionally restrict the privileges of your API Key; otherwise they'll
be a point in time snapshot of permissions of the authenticated user.
For this tutorial the data is written to the `logs-gcp.audit-default` data streams.
====
****
[discrete]
==== Step 2: Create a Pub/Sub topic and subscription

Before configuring the Dataflow template, create a Pub/Sub
topic and subscription from your Google Cloud Console where you can send your
logs from Google Operations Suite.
include::gcp-topic.asciidoc[]

[discrete]
==== Step 3: Configure the Google Dataflow template

After creating a Pub/Sub topic and subscription, go to the *Dataflow Jobs* page
and configure your template to use them. Use the search bar to find the page:

image::monitor-gcp-dataflow-jobs.png[GCP Dataflow Jobs]

To create a job, click *Create Job From Template*.
Set *Job name* as `auditlogs-stream` and select `Pub/Sub to Elasticsearch` from
the *Dataflow template* dropdown menu:

image::monitor-gcp-dataflow-pub-sub-elasticsearch.png[GCP Dataflow Pub/Sub to {es}]

Before running the job, fill in required parameters:

image::monitor-gcp-dataflow-required-parameters.png[GCP Dataflow Required Parameters]

[NOTE]
====
For *Cloud Pub/Sub subscription*, use the subscription you created in the previous step.
For *Cloud ID* and *Base64-encoded API Key*, use the values you got earlier.
If you don't have an *Error output topic*, create one like you did
in the previous step.
====

After filling the required parameters, click *Show Optional Parameters* and add
`audit` as the log type parameter.

image::monitor-gcp-dataflow-optional-parameters.png[GCP Dataflow Optional Parameters]

When you are all set, click *Run Job* and wait for Dataflow to execute the
template, which takes a few minutes.

Finally, navigate to {kib} to see your logs parsed and visualized in the
*[Logs GCP] Audit* dashboard.

image::monitor-gcp-dataflow-audit-dashboard.png[GCP audit overview dashboard]

Besides collecting audit logs from your Google Cloud Platform, you can also use
Dataflow integrations to ingest data directly into Elastic from
https://www.elastic.co/blog/ingest-data-directly-from-google-bigquery-into-elastic-using-google-dataflow[Google BigQuery]
and
https://www.elastic.co/blog/ingest-data-directly-from-google-cloud-storage-into-elastic-using-google-dataflow[Google Cloud Storage].


