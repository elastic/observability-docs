[[monitor-aws-firehose]]
== Monitor Amazon Web Services ({aws}) with Amazon Data Firehose

++++
<titleabbrev>Monitor {aws} with Amazon Data Firehose</titleabbrev>
++++

Amazon Data Firehose is a popular service that allows you to send your VPC flow logs data to Elastic in minutes without a single line of code and without building or managing your own data ingestion and delivery infrastructure. Amazon Data Firehose Helps you answer questions like what percentage of your traffic is getting dropped, and how much traffic is getting generated for specific sources and destinations.

[discrete]
[[aws-elastic-firehose-what-you-learn]]
=== What you'll learn

In this tutorial, you'll learn how to:

- Install AWS integration in {kib}
- Create a delivery stream in Amazon Data Firehose
- Specify the destination settings for your Firehose stream
- Send data to the Firehose delivery stream

[discrete]
[[aws-elastic-firehose-before-you-begin]]
=== Before you begin

Create a deployment using our hosted {ess} on {ess-trial}[{ecloud}].
The deployment includes an {es} cluster for storing and searching your data, and {kib} for visualizing and managing your data. You also need an AWS account with permissions to pull the necessary data from AWS.

[discrete]
[[firehose-step-one]]
=== Step 1: Install AWS integration in {kib}

. Install AWS integrations to load index templates, ingest pipelines, and dashboards into {kib}. In {kib}, navigate to *Management* > *Integrations* in the sidebar. Find the AWS Integration by browsing the catalog.

. Navigate to the *Settings* tab and click *Install AWS assets*. Confirm by clicking *Install AWS* in the popup.

. Install AWS Firehose integration assets in Kibana. 

NOTE: Firehose integration is currently in beta. Make sure to enable *Display beta integrations*.

[discrete]
[[firehose-step-two]]
=== Step 2: Create a delivery stream in Amazon Data Firehose

. Go to the https://console.aws.amazon.com/[AWS console] and navigate to Amazon Data Firehose.  

. Click *Create Firehose stream* and choose the source and destination of your Firehose stream. Unless you are streaming data from Kinesis Data Streams, set source to `Direct PUT` and destination to `Elastic`. 

. Provide a meaningful *Firehose stream name* that will allow you to identify this delivery stream later.

NOTE: For advanced use cases, source records can be transformed by invoking a custom Lambda function. When using Elastic integrations, this should not be required.

[discrete]
[[firehose-step-three]]
=== Step 3: Specify the destination settings for your Firehose stream

. From the *Destination settings* panel, specify the following settings:
+
* *Elastic endpoint URL*: Enter the Elastic endpoint URL of your Elasticsearch cluster. To find the Elasticsearch endpoint, go to the Elastic Cloud console and select *Connection details*. Here is an example of how it looks like: `https://my-deployment.es.us-east-1.aws.elastic-cloud.com`.
+
* *API key*: Enter the encoded Elastic API key. To create an API key, go to the Elastic Cloud console, select *Connection details* and click *Create and manage API keys*. If you are using an API key with *Restrict privileges*, make sure to review the Indices privileges to provide at least "auto_configure" & "write" permissions for the indices you will be using with this delivery stream. 
+
* *Content encoding*: For a better network efficiency, leave content encoding set to GZIP. 
+
* *Retry duration*: Determines how long Firehose continues retrying the request in the event of an error. A duration of 60-300s should be suitable for most use cases.
+
* *Parameters*:
+
  ** `es_datastream_name`: Elastic recommends setting the `es_datastream_name` parameter to `logs-awsfirehose-default` to leverage the routing rules defined in this integration. If this parameter is not specified, data is sent to the `logs-generic-default` data stream by default.
  ** `include_cw_extracted_fields`: This parameter is optional and can be set when using a CloudWatch logs subscription filter as the Firehose data source. When set to true, extracted fields generated by the filter pattern in the subscription filter will be collected. Setting this parameter can add many fields into each record and may significantly increase data volume in Elasticsearch. As such, use of this parameter should be carefully considered and used only when the extracted fields are required for specific filtering and/or aggregation.

. In the *Backup settings* panel, it is recommended to configure S3 backup for failed records. Itâ€™s then possible to configure workflows to automatically retry failed records, for example by using {esf-ref}/aws-elastic-serverless-forwarder.html[Elastic Serverless Forwarder].

[discrete]
[[firehose-step-four]]
=== Step 4: Send data to the Firehose delivery stream

You can configure a variety of log sources to send data to Firehose delivery streams. Refer to the https://docs.aws.amazon.com/firehose/latest/dev/basic-write.html[AWS documentation] for more information.
Several services support writing data directly to delivery streams, including Cloudwatch logs. Alternatively, you can also use https://aws.amazon.com/dms/[AWS Database Migration Service (DMS)] to create streaming data pipelines to Firehose.
For example, a typical workflow for sending VPC Flow Logs to Firehose would be the following:

- Publish VPC Flow Logs to a Cloudwatch log group. Refer to the AWS documentation https://docs.aws.amazon.com/vpc/latest/userguide/flow-logs-cwl.html[about publishing flow logs].
- Create a subscription filter in the CloudWatch log group to the Firehose delivery stream. Refer to the AWS documentation https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/SubscriptionFilters.html#FirehoseExample[about using subscription filters].


For more information on Amazon Data Firehose, you can also check the https://docs.elastic.co/integrations/awsfirehose[Amazon Data Firehose Integrations documentation]. 

