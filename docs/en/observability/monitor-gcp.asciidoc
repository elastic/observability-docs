:tutorial: tutorial
:gcp: GCP

[[monitor-gcp]]
== Monitor Google Cloud Platform

In this tutorial, you'll learn how to monitor your Google Cloud Platform ({gcp})
deployments using Elastic Observability: Logs and Metrics.

[discrete]
=== What you'll learn

You'll learn how to:

- Set up a {gcp} Service Account.
- Ingest metrics using the {metricbeat-ref}/metricbeat-module-gcp.html[{metricbeat}
Google Cloud Platform module] and view those metrics in {kib}.
- Export GCP audit logs through Pub/Sub topics.
- Ingest logs using the {filebeat-ref}/filebeat-module-gcp.html[{filebeat}
Google Cloud module] and view those logs in {kib}.
- Ingest logs using Google Dataflow and view those logs in {kib}.

[discrete]
=== Before you begin

Create a deployment using our hosted {ess} on {ess-trial}[{ecloud}].
The deployment includes an {es} cluster for storing and searching your data,
and {kib} for visualizing and managing your data.
For more information, see <<spin-up-stack,Spin up the Elastic Stack>>.

[discrete]
=== Step 1: Setup a Service Account

Google Cloud Platform implements https://cloud.google.com/compute/docs/access/service-accounts[service
accounts] as a way to access APIs securely. To monitor {gcp} with
Elastic, you will need a service account. The easiest way is to use a predefined
service account that {gcp} https://cloud.google.com/compute/docs/access/service-accounts?hl=en#default_service_account[creates automatically].
Alternatively, you can create a new service account.
This {tutorial} creates a new one.

First, to access the service account menu, click
*Menu* -> *IAM & Admin* -> *Service Accounts*.

image::monitor-gcp-service-account-menu.png[Service account menu]

Next, click *Create Service Account*. Define the new service account
name (for example, "gcp-monitor") and the description (for example, "Service
account to monitor {gcp} services using the Elastic Stack").

image::monitor-gcp-service-account-name.png[Service account name]

[IMPORTANT]
====
Make sure to select the correct roles.
====

To monitor {gcp} services, you need to add these roles to the
service account:

*Compute Viewer*:

image::monitor-gcp-service-account-roles-compute-viewer.png[Service account roles compute viewer]

*Monitoring Viewer*:

image::monitor-gcp-service-account-roles-monitoring-viewer.png[Service account roles monitoring viewer]

*Pub/Sub Subscriber*:

image::monitor-gcp-service-account-roles-pubsub-subscriber.png[Service account roles pub/sub subscriber]

The final result should be the following:

image::monitor-gcp-service-account-roles-final.png[Service account roles result]

Click *Continue*, then skip granting users access to this service. Finally,
click *Done*. The service account is now ready to be used.

Next, to use the service account, click *Manage keys*.

image::monitor-gcp-service-account-manage-keys.png[Service account manage keys]

Then, add a new JSON key type by selecting *Create new key*.

image::monitor-gcp-service-account-create-key.png[Service account create key]

After that, the credential file is downloaded.
Keep this file in an accessible place to use later.

[discrete]
=== Step 2: Install and configure {metricbeat}

[NOTE]
====
This {tutorial} assumes the Elastic cluster is already running. Make sure you have your *cloud ID* and your *credentials* on hand.
====

To monitor {gcp} using the {stack}, you need two main components:
an Elastic deployment to store and analyze the data and an agent to collect
and ship the data.

Two agents can be used to monitor {gcp}: {metricbeat} is used to
monitor metrics, and {filebeat} to monitor logs. You can run the agents on any
machine. This {tutorial} uses a small {gcp} instance, e2-small (2 vCPUs,
2 GB memory), with an Ubuntu distribution.

:leveloffset: +3
include::{shared}/install-configure-metricbeat.asciidoc[]
:leveloffset: -3

Now that the output is working, you are going to set up the input (GCP).

[discrete]
=== Step 3: Configure {metricbeat} Google Cloud Platform module

To collect metrics from Google Cloud Platform, use the
{metricbeat-ref}/metricbeat-module-gcp.html[Google Cloud Platform]
module. This module periodically fetches monitoring metrics from Google Cloud
Platform using
https://cloud.google.com/monitoring/api/metrics_gcp[Stackdriver Monitoring API]
for Google Cloud Platform services.

[WARNING]
====
Extra GCP charges on Stackdriver Monitoring API requests may be generated by
this module. Please see
{metricbeat-ref}/metricbeat-module-gcp.html#gcp-api-requests[rough estimation of the number of API calls]
for more details.
====

. Enable the GCP module.
+
[source,bash]
----
./metricbeat modules enable gcp
----

. Edit the `modules.d/gcp.yml` file to configure which metrics to
collect.
+
[source,yml]
----
- module: gcp
  metricsets:
    - compute <1>
  zone: "" <2>
  project_id: "your-project-id" <3>
  period: 1m <4>
  credentials_file_path: "/home/ubuntu/credentials.json" <5>
----
+
<1> The `compute` metricset is a predefined metricset that collects some
GCP compute metrics.
<2> Defines which zones to monitor, an empty value collects data from *all* zones
<3> Collects metrics within the `your-project-id` project-id.
<4> Collects metrics every minute
<5> The GCP credential file that you generated earlier. (Don't forget to create
the file if it does not exist and use the correct full path).

. To check if {metricbeat} can collect data, test the input by running the
following command:
+
[source,bash]
----
./metricbeat test modules gcp
----
+
{metricbeat} will print {gcp} metrics to the terminal, if the setup is correct.

. When the input and output are ready, start {metricbeat} to collect the data.
+
[source,bash]
----
./metricbeat -e
----

. Finally, log into {kib} and open the *[{metricbeat} GCP] Compute Overview*
dashboard.
+
image:monitor-gcp-compute-overview-dashboard.png[{metricbeat} compute overview dashboard]

[discrete]
=== Step 4: Install and configure {filebeat}

Now that {metricbeat} is up and running, configure {filebeat} to
collect Google Cloud logs.

:leveloffset: +3
include::{shared}/install-configure-filebeat.asciidoc[]
:leveloffset: -3

Now that the output is working, you are going to set up the input ({gcp}).

[discrete]
=== Step 5: Configure {filebeat} Google Cloud module

To collect logs from Google Cloud Platform, use the
{filebeat-ref}/filebeat-module-gcp.html[Google Cloud Platform]
module. This module periodically fetches logs that have been exported from
Stackdriver to a Google Pub/Sub topic sink. There are three available filesets:
`audit`, `vpcflow`, `firewall`. This tutorial covers the `audit` fileset.

. Go to the *Logs Router* page to configure {gcp} to export logs to a Pub/Sub
topic. Use the search bar to find the page:
+
image::monitor-gcp-navigate-logs-router.png[Navigate to Logs Router page]
+
To set up the logs routing sink, click  *Create sink*.
Set *sink name* as `monitor-gcp-audit-sink`. Select the *Cloud Pub/Sub topic* as the
*sink service* and *Create new Cloud Pub/Sub topic* named `monitor-gcp-audit`:
+
image::monitor-gcp-create-pubsub-topic.png[Create Pub/Sub topic]
+
Finally, under *Choose logs to include in sink*, add
`logName:"cloudaudit.googleapis.com"` (it includes all audit logs).
Click *create sink*.  It will look something like the following:
+
image::monitor-gcp-create-sink.png[Create logs routing sink]

. Now that GCP is configured to export audit logs, enable {filebeat} Google Cloud module.
+
[source,bash]
----
./filebeat modules enable gcp
----

. Edit the `modules.d/gcp.yml` file with the following configurations.
+
[source,yml]
----
- module: gcp
  vpcflow:
    enabled: false <1>
  firewall:
    enabled: false <1>
  audit:
    enabled: true <2>
    var.project_id: "elastic-education" <3>
    var.topic: "monitor-gcp-audit" <4>
    var.subscription_name: "monitor-gcp-audit-sub" <5>
    var.credentials_file: "/home/ubuntu/credentials.json" <6>
----
+
<1> Disables both `vpcflow` and `firewall` filesets.
<2> Enables the `audit` fileset.
<3> Collects data within the `elastic-education` project-id.
<4> Collects logs from the `monitor-gcp-audit` topic.
<5> Google Cloud Pub/Sub topic subscription name. If the subscription does not
exist it will be created.
<6> The GCP credential file that you generated earlier. (Don't forget to create
the file if it does not exist and to use the correct full path).

. Start {filebeat} to collect the logs.
+
[source,bash]
----
./filebeat -e
----

. Finally, log into {kib} and open the *[{filebeat} GCP] Audit*
dashboard.
+
image::monitor-gcp-audit-overview-dashboard.png[{filebeat} audit overview dashboard]

[discrete]
=== Google Dataflow 

If you don't want to provision VM and install data shippers due to process and
management overhead, you can skip this step and ingest data directly from
Pub/Sub to Elastic with https://cloud.google.com/dataflow[Google Dataflow].
In this section you will learn how to ship logs directly from the Google Cloud
Console with the Dataflow template for analyzing GCP Audit Logs in the Elastic
Stack.

First, access *Integrations* in {kib} and search for `gcp`.

image::monitor-gcp-kibana-integrations.png[{kib} integrations]

Next, access the Elastic GCP integration and click
*Add Google Cloud Platform (GCP)*.

image::monitor-gcp-integration.png[GCP integration]

Then, click *Save integration* to include prebuilt dashboards,
ingest node configurations, and other assets that help you get the most of the
GCP logs you ingest.

Before configuring the Dataflow template, you need to create a Pub/Sub topic
and subscription from your Google Cloud Console, as demonstrated in the
previous section.

Now, go to the *Dataflow Jobs* page to configure your template.
Use the search bar to find the page:

image::monitor-gcp-dataflow-jobs.png[GCP Dataflow Jobs]

To create a job, click *Create Job From Template*.
Set *Job name* as `auditlogs-stream` and select `Pub/Sub to Elasticsearch` from
the *Dataflow template* dropdown menu:

image::monitor-gcp-dataflow-pub-sub-elasticsearch.png[GCP Dataflow Pub/Sub to Elasticsearch]

Before running the job, you will need to fill in required parameters:

image::monitor-gcp-dataflow-required-parameters.png[GCP Dataflow Required Parameters]

  - The Cloud *Pub/Sub subscription* to consume from. You can find it in the
https://console.cloud.google.com/cloudpubsub/subscription[Pub/Sub Subscriptions] page of your GCP account.
  - The Cloud ID of the https://cloud.elastic.co/deployments[deployment] you
created at the beginning of this tutorial. You can find it at *Overview* page.
  - The error output topic for failed inserts. You can find it or create one
at the https://console.cloud.google.com/cloudpubsub/topic[Pub/Sub] page of your GCP account.
  - The base64 encoded API key to authenticate on your deployment.
You can create one through {kibana-ref}/api-keys.html[{kib}].

After filling the required parameters, click *Show Optinal Parameters* and add
`audit` as the log type parameter.

image::monitor-gcp-dataflow-optional-parameters.png[GCP Dataflow Optional Parameters]

When you are all set, click *Run Job* and wait Dataflow to execute the
template, which takes about few minutes.

Finally, navigate to {kib} to see your logs parsed and visualized in the
*[Logs GCP] Audit* dashboard.

image::monitor-gcp-dataflow-audit-dashboard.png[GCP audit overview dashboard]

Besides collecting audit logs from your Google Cloud Platform, you can also use
Dataflow integrations to ingest data directly into Elastic from
https://www.elastic.co/blog/ingest-data-directly-from-google-bigquery-into-elastic-using-google-dataflow[Google BigQuery]
and
https://www.elastic.co/blog/ingest-data-directly-from-google-cloud-storage-into-elastic-using-google-dataflow[Google Cloud Storage].


