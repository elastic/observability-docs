[[apm-tail-based-samling-config]]
= Tail-based sampling

****
image:./binary-yes-fm-yes.svg[supported deployment methods]

Most options on this page are supported by all APM Server deployment methods when writing to {es}.
If you are using a different <<apm-configuring-output,output>>, tail-based sampling is _not_ supported.
****

Tail-based sampling configuration options.

include::./tab-widgets/sampling-config-widget.asciidoc[]

[float]
[[apm-configuration-tbs]]
== Top-level tail-based sampling settings

See <<apm-tail-based-sampling>> to learn more.

:input-type: ref
// tag::tbs-top[]

[float]
[id="sampling-tail-enabled-{input-type}"]
=== Enable tail-based sampling
Set to `true` to enable tail based sampling.
Disabled by default. (bool)

|====
| APM Server binary | `sampling.tail.enabled`
| Fleet-managed     | `Enable tail-based sampling`
|====

[float]
[id="sampling-tail-interval-{input-type}"]
=== Interval
Synchronization interval for multiple APM Servers.
Should be in the order of tens of seconds or low minutes.
Default: `1m` (1 minute). (duration)

|====
| APM Server binary | `sampling.tail.interval`
| Fleet-managed     | `Interval`
|====

[float]
[id="sampling-tail-policies-{input-type}"]
=== Policies
Criteria used to match a root transaction to a sample rate.

Policies map trace events to a sample rate.
Each policy must specify a sample rate.
Trace events are matched to policies in the order specified.
All policy conditions must be true for a trace event to match.
Each policy list should conclude with a policy that only specifies a sample rate.
This final policy is used to catch remaining trace events that don't match a stricter policy.
(`[]policy`)

|====
| APM Server binary | `sampling.tail.policies`
| Fleet-managed     | `Policies`
|====

[float]
[id="sampling-tail-storage_limit-{input-type}"]
=== Storage limit
The amount of storage space allocated for trace events matching tail sampling policies. Caution: Setting this limit higher than the allowed space may cause APM Server to become unhealthy.

If the configured storage limit is insufficient, it logs "configured storage limit reached". The event will bypass sampling and will always be indexed when storage limit is reached.

Default: `3GB`. (text)

|====
| APM Server binary | `sampling.tail.storage_limit`
| Fleet-managed     | `Storage limit`
|====

// end::tbs-top[]

[float]
[[apm-configuration-tbs-policy]]
== Policy-level tail-based sampling settings

See <<apm-tail-based-sampling>> to learn more.

// tag::tbs-policy[]

[float]
[id="sampling-tail-sample-rate-{input-type}"]
=== **`sample_rate`**

The sample rate to apply to trace events matching this policy.
Required in each policy.

The sample rate must be greater than or equal to `0` and less than or equal to `1`.
For example, a `sample_rate` of `0.01` means that 1% of trace events matching the policy will be sampled.
A `sample_rate` of `1` means that 100% of trace events matching the policy will be sampled. (int)

[float]
[id="sampling-tail-trace-name-{input-type}"]
=== **`trace.name`**

The trace name for events to match a policy.
A match occurs when the configured `trace.name` matches the `transaction.name` of the root transaction of a trace.
A root transaction is any transaction without a `parent.id`. (string)

[float]
[id="sampling-tail-trace-outcome-{input-type}"]
=== **`trace.outcome`**

The trace outcome for events to match a policy.
A match occurs when the configured `trace.outcome` matches a trace's `event.outcome` field.
Trace outcome can be `success`, `failure`, or `unknown`. (string)

[float]
[id="sampling-tail-service-name-{input-type}"]
=== **`service.name`**

The service name for events to match a policy. (string)

[float]
[id="sampling-tail-service-environment-{input-type}"]
=== **`service.environment`**

The service environment for events to match a policy. (string)

// end::tbs-policy[]
:!input-type:

[float]
[[sampling-tail-monitoring-ref]]
== Monitoring tail-based sampling

APM Server produces metrics to monitor the performance and estimate the workload being processed by tail-based sampling. In order to use these metrics, you need to [enable monitoring for the APM Server](/solutions/observability/apps/monitor-apm-server.md). The following metrics are produced by the tail-based sampler (note that the metrics might have a different prefix,  for example `beat.stats` for ECH deployments, based on how the APM Server is running):

[float]
[[sampling-tail-monitoring-dynamic-service-group-ref]]
=== `apm-server.sampling.tail.dynamic_service_groups`

This metric tracks the number of dynamic services that the tail-based sampler is tracking per policy. Dynamic services are created for tail-based sampling policies that are defined without a `service.name`.

This is a counter metric so, should be visualized with `counter_rate`.

[float]
[[sampling-tail-monitoring-events-processed-ref]]
=== `apm-server.sampling.tail.events.processed`

This metric tracks the total number of events (including both transaction and span) processed by the tail-based sampler.

This is a counter metric so, should be visualized with `counter_rate`.

[float]
[[sampling-tail-monitoring-events-stored-ref]]
=== `apm-server.sampling.tail.events.stored`

This metric tracks the total number of events stored by the tail-based sampler in the database. Events are stored when the full trace is not yet available to make the sampling decision. This value is directly proportional to the storage required by the tail-based sampler to function.

This is a counter metric so, should be visualized with `counter_rate`.

[float]
[[sampling-tail-monitoring-events-dropped-ref]]
=== `apm-server.sampling.tail.events.dropped`

This metric tracks the total number of events dropped by the tail-based sampler. Only the events that are actually dropped by the tail-based sampler are reported as dropped. Additionally, any events that were stored by the processor but never indexed will not be counted by this metric.

This is a counter metric so, should be visualized with `counter_rate`.

[float]
[[sampling-tail-monitoring-storage-lsm-size-ref]]
=== `apm-server.sampling.tail.storage.lsm_size`

This metric tracks the storage size of the log-structured merge trees used by the tail-based sampling database in bytes. This metric is one part of the total disk space used by the tail-based sampler. See <<sampling-tail-monitoring-storage-total-size-ref>> for details on how to monitor total disk size used by the tail-based sampler.

[float]
[[sampling-tail-monitoring-storage-value-log-size-ref]]
=== `apm-server.sampling.tail.storage.value_log_size`

This metric tracks the storage size for value log files used by the tail-based sampling database in bytes. This metric is one part of the total disk space used by the tail-based sampler. See <<sampling-tail-monitoring-storage-total-size-ref>> for details on how to monitor total disk size used by the tail-based sampler.

[float]
[[sampling-tail-monitoring-storage-total-size-ref]]
=== Total storage size

Total storage size is the sum of the <<sampling-tail-monitoring-storage-lsm-size-ref>> and <<sampling-tail-monitoring-storage-value-log-size-ref>>. It is the most crucial metric to track storage requirements for tail-based sampler, especially for big deployments with large distributed traces. Deployments using tail-based sampling extensively should set up alerts and monitoring on this metric.

This metric can also be used to get an estimate of the storage requirements for tail-based sampler before increasing load by extrapolating the metric based on the current usage. It is important to note that before doing any estimation the tail-based sampler should be allowed to run for at least a few TTL cycles and that the estimate will only be useful for similar load patterns.
