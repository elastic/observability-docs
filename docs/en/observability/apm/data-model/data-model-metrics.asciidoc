[[apm-data-model-metrics]]
= Metrics

status_badge::[serverless, ga]
status_badge::[stack, ga]

include::{observability-docs-root}/docs/en/observability/partials/apm-server-vs-mis.asciidoc[]

**Metrics** measure the state of a system by gathering information on a regular interval. There are two types of APM metrics:

* **System metrics**: Basic infrastructure and application metrics.
* **Calculated metrics**: Aggregated trace event metrics used to power visualizations in the Applications UI.

[float]
== System metrics

APM agents automatically pick up basic host-level metrics,
including system and process-level CPU and memory metrics.
Agent specific metrics are also available,
like {apm-java-ref-v}/metrics.html[JVM metrics] in the Java Agent,
and {apm-go-ref-v}/metrics.html[Go runtime] metrics in the Go Agent.

Infrastructure and application metrics are important sources of information when debugging production systems,
which is why we've made it easy to filter metrics for specific hosts or containers in the {kib} <<apm-metrics,metrics overview>>.

TIP: Most agents limit keyword fields to 1024 characters,
non-keyword fields (e.g. `system.memory.total`) to 10,000 characters.

Metrics are stored in metric indices.

For a full list of tracked metrics, see the relevant agent documentation:

* {apm-go-ref-v}/metrics.html[Go]
* {apm-java-ref-v}/metrics.html[Java]
* {apm-node-ref-v}/metrics.html[Node.js]
* {apm-py-ref-v}/metrics.html[Python]
* {apm-ruby-ref-v}/metrics.html[Ruby]

[float]
=== Example system metric document

This example shows what system metric documents can look like when indexed in {es}.

[%collapsible]
.Expand {es} document
====

This example contains JVM metrics produced by the {apm-java-agent}.
and contains two related metrics: `jvm.gc.time` and `jvm.gc.count`. These are accompanied by various fields describing
the environment in which the metrics were captured: service name, host name, Kubernetes pod UID, container ID, process ID, and more.
These fields make it possible to search and aggregate across various dimensions, such as by service, host, and Kubernetes pod.

// Temporarily remove for status-badge test
// [source,json]
// ----
// include::{apm-server-root}/docs/data/elasticsearch/metricset.json[]
// ----
====

[float]
== Calculated metrics

APM agents and either {apm-server-or-mis} calculate metrics from trace events to power visualizations in the Applications UI.

Calculated metrics are an implementation detail and while we aim for stability for these data models,
the dimensions and concrete limits for aggregations are subject to change within minor version updates.

These metrics are described below.

[float]
=== Breakdown metrics

To power the <<apm-transactions,Time spent by span type>> graph,
agents collect summarized metrics about the timings of spans and transactions,
broken down by span type.

*`span.self_time.count`* and *`span.self_time.sum.us`*::
+
--
These metrics measure the "self-time" for a span type, and optional subtype,
within a transaction group. Together these metrics can be used to calculate
the average duration and percentage of time spent on each type of operation
within a transaction group.

These metric documents can be identified by searching for `metricset.name: span_breakdown`.

You can filter and group by these dimensions:

* `transaction.name`: The name of the enclosing transaction group, for example `GET /`
* `transaction.type`: The type of the enclosing transaction, for example `request`
* `span.type`: The type of the span, for example `app`, `template` or `db`
* `span.subtype`: The sub-type of the span, for example `mysql` (optional)
--

[float]
=== Example breakdown metric document

This example shows what breakdown metric documents can look like when indexed in {es}.

[%collapsible]
.Expand {es} document
====

// Temporarily remove for status-badge test
// [source,json]
// ----
// include::{apm-server-root}/docs/data/elasticsearch/span_breakdown.json[]
// ----
====

[float]
=== Transaction metrics

To power <<apm-ui,{kib} Applications UI>> visualizations,
{apm-server-or-mis} aggregates transaction events into latency distribution metrics.

*`transaction.duration.summary`* and *`transaction.duration.histogram`*::
+
--
These metrics represent the latency summary and latency distribution of transaction groups,
used to power transaction-oriented visualizations and analytics in Elastic APM.

These metric documents can be identified by searching for `metricset.name: transaction`.

You can filter and group by these dimensions (some of which are optional, for example `container.id`):

* `agent.name`: The name of the {apm-agent} that instrumented the transaction, for example `java`
* `cloud.account.id`: The cloud account id of the service that served the transaction
* `cloud.account.name`: The cloud account name of the service that served the transaction
* `cloud.availability_zone`: The cloud availability zone hosting the service instance that served the transaction
* `cloud.machine.type`: The cloud machine type or instance type of the service that served the transaction
* `cloud.project.id`: The cloud project identifier of the service that served the transaction
* `cloud.project.name`: The cloud project name of the service that served the transaction
* `cloud.provider`: The cloud provider hosting the service instance that served the transaction
* `cloud.region`: The cloud region hosting the service instance that served the transaction
* `cloud.service.name`: The cloud service name of the service that served the transaction
* `container.id`: The container ID of the service that served the transaction
* `event.outcome`: The outcome of the transaction, for example `success`
* `faas.coldstart`: Whether the _serverless_ service that served the transaction had a cold start
* `faas.id`: The unique identifier of the invoked serverless function
* `faas.name`: The name of the lambda function
* `faas.trigger.type`: The trigger type that the lambda function was executed by of the service that served the transaction
* `faas.version`: The version of the lambda function
* `host.hostname`: The detected hostname of the service that served the transaction
* `host.name`: The user-defined name of the host or the detected hostname of the service that served the transaction
* `host.os.platform`: The platform name of the service that served the transaction, for example `linux`
* `kubernetes.pod.name`: The name of the Kubernetes pod running the service that served the transaction
* `labels`: Key-value object containing string labels set globally by the APM agents. This dimension is not present for RUM agents.
* `metricset.interval`: A string with the aggregation interval the metricset represents.
* `numeric_labels`: Key-value object containing numeric labels set globally by the APM agents. This dimension is not present for RUM agents.
* `service.environment`: The environment of the service that served the transaction
* `service.language.name`: The language name of the service that served the transaction, for example `Go`
* `service.language.version`: The language version of the service that served the transaction
* `service.name`: The name of the service that served the transaction
* `service.node.name`: The name of the service instance that served the transaction
* `service.runtime.name`: The runtime name of the service that served the transaction, for example `jRuby`
* `service.runtime.version`: The runtime version that served the transaction
* `service.version`: The version of the service that served the transaction
* `transaction.name`: The name of the transaction, for example `GET /`
* `transaction.result`: The result of the transaction, for example `HTTP 2xx`
* `transaction.root`: A boolean flag indicating whether the transaction is the root of a trace
* `transaction.type`: The type of the transaction, for example `request`
--

The `@timestamp` field of these documents holds the start of the aggregation interval.

[float]
=== Example transaction document

This example shows what transaction documents can look like when indexed in {es}.

[%collapsible]
.Expand {es} document
====

// Temporarily remove for status-badge test
// [source,json]
// ----
// include::{apm-server-root}/docs/data/elasticsearch/transaction_metric.json[]
// ----
====

[float]
=== Service-transaction metrics

To power <<apm-ui,{kib} Applications UI>> visualizations,
{apm-server-or-mis} aggregates transaction events into service-transaction metrics.
Service-transaction metrics are similar to transaction metrics, but they usually
have a much lower cardinality as they have significantly fewer dimensions.
The UI uses them when fewer details of the transactions are needed.

*`transaction.duration.summary`* and *`transaction.duration.histogram`*::
+
--
These metrics represent the latency summary and latency distribution of service transaction groups,
used to power service-oriented visualizations and analytics in Elastic APM.

These metric documents can be identified by searching for `metricset.name: service_transaction`.

You can filter and group by these dimensions:

* `agent.name`: The name of the {apm-agent} that instrumented the operation, for example `java`
* `labels`: Key-value object containing string labels set globally by the APM agents. This dimension is not present for RUM agents.
* `metricset.interval`: A string with the aggregation interval the metricset represents.
* `numeric_labels`: Key-value object containing numeric labels set globally by the APM agents. This dimension is not present for RUM agents.
* `service.environment`: The environment of the service that made the request
* `service.language.name`: The language name of the service that served the transaction, for example `Go`
* `service.name`: The name of the service that made the request
* `transaction.type`: The type of the enclosing transaction, for example `request`
--

The `@timestamp` field of these documents holds the start of the aggregation interval.

[float]
=== Example service-transaction document

This example shows what service-transaction documents can look like when indexed in {es}.

[%collapsible]
.Expand {es} document
====

// Temporarily remove for status-badge test
// [source,json]
// ----
// include::{apm-server-root}/docs/data/elasticsearch/service_transaction_metric.json[]
// ----
====

[float]
=== Service-destination metrics

To power <<apm-ui,{kib} Applications UI>> visualizations,
{apm-server-or-mis} aggregates span events into service-destination metrics.

*`span.destination.service.response_time.count`* and *`span.destination.service.response_time.sum.us`*::
+
--
These metrics measure the count and total duration of requests from one service to another service.
These are used to calculate the throughput and latency of requests to backend services such as databases in
<<apm-service-maps,Service maps>>.

These metric documents can be identified by searching for `metricset.name: service_destination`.

You can filter and group by these dimensions:

* `agent.name`: The name of the {apm-agent} that instrumented the operation, for example `java`
* `event.outcome`: The outcome of the operation, for example `success`
* `labels`: Key-value object containing string labels set globally by the APM agents. This dimension is not present for RUM agents.
* `metricset.interval`: A string with the aggregation interval the metricset represents.
* `numeric_labels`: Key-value object containing numeric labels set globally by the APM agents. This dimension is not present for RUM agents.
* `service.environment`: The environment of the service that made the request
* `service.language.name`: The language name of the service that served the transaction, for example `Go`
* `service.name`: The name of the service that made the request
* `service.target.name`: The target service name, for example `customer_db`
* `service.target.type`: The target service type, for example `mysql`
* `span.destination.service.resource`: The destination service resource, for example `mysql`
* `span.name`: The name of the operation, for example `SELECT FROM table_name`.
--

The `@timestamp` field of these documents holds the start of the aggregation interval.

[float]
=== Example service-destination document

This example shows what service-destination documents can look like when indexed in {es}.

[%collapsible]
.Expand {es} document
====

// Temporarily remove for status-badge test
// [source,json]
// ----
// include::{apm-server-root}/docs/data/elasticsearch/service_destination_metric.json[]
// ----
====

[float]
=== Service-summary metrics

To power <<apm-ui,{kib} Applications UI>> visualizations,
{apm-server-or-mis} aggregates transaction, error, log, and metric events into service-summary metrics.

These metric documents can be identified by searching for `metricset.name: service_summary`.

You can filter and group by these dimensions:

* `agent.name`: The name of the {apm-agent} that instrumented the operation, for example `java`
* `labels`: Key-value object containing string labels set globally by the APM agents. This dimension is not present for RUM agents.
* `metricset.interval`: A string with the aggregation interval the metricset represents.
* `numeric_labels`: Key-value object containing numeric labels set globally by the APM agents. This dimension is not present for RUM agents.
* `service.environment`: The environment of the service that made the request
* `service.language.name`: The language name of the service that served the transaction, for example `Go`
* `service.name`: The name of the service that made the request

The `@timestamp` field of these documents holds the start of the aggregation interval.

[float]
=== Example service-summary document

This example shows what service-summary documents can look like when indexed in {es}.

[%collapsible]
.Expand {es} document
====

// Temporarily remove for status-badge test
// [source,json]
// ----
// include::{apm-server-root}/docs/data/elasticsearch/service_summary_metric.json[]
// ----
====

[float]
== Data streams

Metrics are stored in the following data streams:

include::{observability-docs-root}/docs/en/observability/apm/manage-storage/data-streams.asciidoc[tag=metrics-data-streams]

See <<apm-data-streams>> to learn more.

[float]
== Aggregated metrics: limits and overflows

For all aggregated metrics, namely transaction, service-transaction, service-destination, and service-summary metrics,
there are limits on the number of unique groups tracked at any given time.

// Do these limits apply to the managed intake service on serverless?
[float]
=== Limits

Note that all the below limits may change in the future with further improvements.

* For all the following metrics, they share a limit of 1000 services per GB of APM Server.
** For transaction metrics, there is an additional limit of 5000 total transaction groups per GB of APM Server,
and each service may only consume up to 10% of the transaction groups,
which is 500 transaction groups per service per GB of APM Server.
** For service-transaction metrics, there is an additional limit of 1000 total service transaction groups per GB of APM Server,
and each service may only consume up to 10% of the service transaction groups,
which is 100 service transaction groups per service per GB of APM Server.
** For service-destination metrics, there is an additional limit of 5000 total service destination groups per GB of APM Server
starting with 10000 service destination groups for 1 GB APM Server,
and each service may only consume up to 10% of the service destination groups,
which is 1000 service destination groups for 1GB APM Server with 500 increment per GB of APM Server.
** For service-summary metrics, there is no additional limit.

In the above, a service is defined as a combination of `service.name`, `service.environment`, `service.language.name` and `agent.name`.

[float]
=== Overflows

When a dimension has a high cardinality and exceeds the limit, the metrics will be aggregated
under a dedicated overflow bucket.

For example, when `transaction.name` has a lot of unique values and reaches the limit
of unique transaction groups tracked, any transactions with new `transaction.name` will be aggregated under
`transaction.name`: `_other`.

// Does this apply to the managed intake service?
Another example of how the transaction group limit may be reached is if `transaction.name` contains just a few unique values,
but the service is deployed on a lot of different hosts. As `host.name` is part of the aggregation key for transaction metrics,
the max transaction group limit is reached for a service that has 100 instances, 10 different transaction names, and
4 transaction results per transaction name when connected to an APM Server with 8GB of RAM.
Once this limit is reached, any new combinations of `transaction.name`, `transaction.result`, and
`host.name` for that service will be aggregated under `transaction.name`: `_other`.

// Does this apply to the managed intake service?
This issue can be resolved by increasing memory available to APM Server, or by ensuring that the dimensions do not use values
that are based on parameters that can change. For example, user ids, product ids, order numbers, query parameters, etc.,
should be stripped away from the dimensions. For the same reason, avoid high cardinality global labels (`labels.\*` and `numeric_labels.*`).

Aggregated metrics do not consider global labels from RUM agents in order to protect {apm-server} from using too much memory.
