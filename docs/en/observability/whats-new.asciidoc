[[whats-new]]
= What's new in {minor-version}

Here are the highlights of what's new and improved in {minor-version}.

Other versions:
{observability-guide-all}/8.8/whats-new.html[8.8] |
{observability-guide-all}/8.7/whats-new.html[8.7] |
{observability-guide-all}/8.6/whats-new.html[8.6] |
{observability-guide-all}/8.5/whats-new.html[8.5] |
{observability-guide-all}/8.4/whats-new.html[8.4] |
{observability-guide-all}/8.3/whats-new.html[8.3] |
{observability-guide-all}/8.2/whats-new.html[8.2] |
{observability-guide-all}/8.1/whats-new.html[8.1] |
{observability-guide-all}/8.0/whats-new.html[8.0] |
{observability-guide-all}/7.17/whats-new.html[7.17]

// tag::whats-new[]

[discrete]
== Introducing Elastic AI Assistant use cases for Observability

preview:[] Elastic Observability is excited to announce the initial (technical preview) release of the Elastic AI Assistant for Observability. Key observability workflows have been enhanced with generative AI (GAI) to help improve troubleshooting processes and provide automated explanations for information that is not easily understood. This will lead to better root cause analysis and improved MTTx.

Elastic AI Assistant for Observability is incorporating generative AI in the following user workflows:

* **Elastic AI Assistant for log message:** Provides the ability to use generative AI to look up the meaning of the log message details and help you find related messages.
+
video::https://static-www.elastic.co/v3/assets/bltefdd0b53724fa2ce/bltf93d3d7ebbad9b82/64bef299320b7e5eedca7e54/LogAssistant.mov[width=640]

* **Elastic AI Assistant for APM errors:** Provides the ability to explain an error or stack trace in APM and suggest remediations.
+
video::https://static-www.elastic.co/v3/assets/bltefdd0b53724fa2ce/blt21e5befafed14b34/64bef0c6320b7e631bca7e3b/APMErrorAssistant.mov[width=640]

* **Elastic AI Assistant for log alerts:** Provides the ability to use machine learning (ML) to explain what caused the spike in log messages and to use that information with generative AI to provide potential root cause and remediation steps.
+
video::https://static-www.elastic.co/v3/assets/bltefdd0b53724fa2ce/bltf11ea707bfac2fde/64bef2b0d85ca66ef576f2bd/LogThresholdAssistant.mov[width=640]

* **Elastic AI Assistant for host processes:** Provides the ability to get details on a process and how to optimize the process for resources like CPU or memory.
+
video::https://static-www.elastic.co/v3/assets/bltefdd0b53724fa2ce/blt6f28aca285b49413/64bef27c7b7f98524d9feadc/HostProcessAssistant.mov[width=640]

* **Elastic AI Assistant for profiling:** Provides the ability to explain the most expensive processes/functions in your organization and suggest optimizations — the AI assistant can use generative AI to explain any function and provide details on how to optimize the function.
+
video::https://static-www.elastic.co/v3/assets/bltefdd0b53724fa2ce/blt45dc9c45747a265c/64bef2c1ab018e62ae51db2c/ProfilingAssistant.mov[width=640]

All of the Elastic AI Assistant use cases mentioned above will be provided in 8.9 as a technical preview with the exception of log alerts (due to ML licensing requirements).

This feature is available by configuring the Elastic AI Assistant, selecting the model, and entering in your credentials. Initially we will be supporting Azure OpenAI and OpenAI.

Ready to get started? See the {observability-guide}/obs-ai-assistant.html[documentation]. Here's a quick overview of the process:

. Ensure you have an API key for https://platform.openai.com/docs/api-reference[OpenAI] or
https://learn.microsoft.com/en-us/azure/cognitive-services/openai/reference[Azure OpenAI Service].
. Edit your `kibana.yaml` file (either for self-managed or Elastic Cloud deployments) with one of the following configurations:
+
**OpenAI:**
+
[source,yml]
----
xpack.observability.aiAssistant.enabled: true
xpack.observability.aiAssistant.provider.openAI.apiKey: <insert API key>
xpack.observability.aiAssistant.provider.openAI.model: <insert model name, e.g. gpt-4>
----
+
**Azure OpenAIService:**
+
[source,yml]
----
xpack.observability.aiAssistant.enabled: true
xpack.observability.aiAssistant.provider.azureOpenAI.deploymentId: <insert deployment ID>
xpack.observability.aiAssistant.provider.azureOpenAI.resourceName: <insert resource name>
xpack.observability.aiAssistant.provider.azureOpenAI.apiKey: <insert API key>
----

Read more about Elastic Observability GAI support in the following blogs:

* https://www.elastic.co/blog/kubernetes-errors-elastic-observability-logs-openai[Learn how to analyze Kubernetes errors with OpenAI in Elastic Observability]
* https://www.elastic.co/blog/chatgpt-elasticsearch-apm-instrumentation-performance-cost-analysis[ChatGPT and Elasticsearch: APM instrumentation, performance, and cost analysis]
* https://www.elastic.co/blog/chatgpt-elasticsearch-faceting-filtering-more-context[ChatGPT and Elasticsearch: Faceting, filtering, and more context]

[discrete]
== Collect metrics from linked Amazon CloudWatch accounts

Amazon CloudWatch support for https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Cross-Account-Cross-Region.html[cross-account monitoring] enables Cloudwatch customers to set up a single monitoring account that can access Cloudwatch metrics from multiple other sharing accounts. We have enhanced our AWS integration to support cross-account metrics. When enabled, metrics from the monitoring account and the shared accounts are included. To include metrics from linked sharing accounts, the `IncludeLinkedAccounts` parameter must be included in our ListMetrics API calls. The resulting response contains metrics for the shared accounts and the monitoring account. The owning account is also listed for each returned metric, which will allow us to ensure that `cloud.account.id` is correct on all reported metrics.

To learn more about the `IncludeLinkedAccounts` parameter, see the https://docs.elastic.co/en/integrations/aws#cross-account-observability[AWS integration documentation].

image::images/wn-89-collect-metrics.png[]

[discrete]
== Hosts available in Beta

beta:[] The {observability-guide}/analyze-hosts.html[Hosts] page is now available in beta to all customers.

The Hosts page provides a metrics-driven view of your infrastructure backed by an easy-to-use interface called Lens. On the Hosts page, you can view health and performance metrics to help you quickly:

Analyze and compare hosts without having to build new dashboards.
Identify which hosts trigger the most alerts.
Troubleshoot and resolve issues quickly.
View historical data to rule out false alerts and identify root causes.
Filter and search the data to focus on the hosts you care about the most.

image::images/wn-89-hosts.png[]

[discrete]
== Analyze the storage footprint of your APM data with storage explorer

With Elastic Observability 8.9, APM Storage Explorer is now Generally Available. Storage explorer enables you to view the total and relative storage footprint of APM documents for each instrumented service. This provides visibility into the storage impact of configuration changes such as sampling rates, and will help you to improve storage requirement forecasts.

Learn more in the {kibana-ref}/storage-explorer.html[docs].

image::images/wn-89-storage-explorer.png[]

[discrete]
== Unlock up to 70% metrics storage savings with TSDS enabled integrations

The latest versions of Elastic Observability’s most popular observability integrations now use the storage cost-efficient https://www.elastic.co/blog/whats-new-elasticsearch-8-7-0[time series index mode] for metrics by default. Kubernetes, Nginx, System, AWS, Azure, RabbitMQ, Redis, and more popular Elastic Observability integrations are {ref}/tsds.html[time series data stream (TSDS)] enabled integrations.

image::images/wn-89-unlock-metrics.png[]

Elastic released the time series data stream functionality to GA in 8.7. Elastic time series data stream (TSDS) stores metrics in indices optimized for a time series database (https://en.wikipedia.org/wiki/Time_series_database[TSDB]), which is optimized to store time series metrics.

When using the TSDS-enabled version for an integration collecting metrics, the benefits that you can realize are the following:

* **Up to 70% less disk space**: With TSDS seamlessly enabled in the integration for your platforms, you can enjoy up to a significant 70% reduction in disk storage for your metrics data. This reduction translates to cost savings and increased scalability for your infrastructure.
* **Streamlined data management**: TSDS simplifies the storage and retrieval of your time stamped metrics data, making it effortless to organize and analyze your valuable insights.
* **Out-of-the-box functionality**: Thanks to the native integration of TSDS, you can leverage its powerful features without any user input or additional configuration. The integration seamlessly optimizes storage efficiency for your metrics data, allowing you to focus on deriving meaningful insights.

For more information, or to learn how to do this in {kib}, read our https://www.elastic.co/blog/70-percent-storage-savings-for-metrics-with-elastic-observability[blog post].

image::images/wn-89-unlock-metrics-2.png[]

NOTE: Index size comparison for metrics stored using standard (30.4GB) vs time series (5.9GB) mode

[discrete]
== Rules as code

The Terraform Elastic Stack provider is released with a new capability that allows users to manage their https://registry.terraform.io/providers/elastic/elasticstack/latest/docs/resources/kibana_alerting_rule[alerting rules] and https://registry.terraform.io/providers/elastic/elasticstack/latest/docs/resources/kibana_action_connector[connectors] within Kibana. This empowers users to automate manual processes, manage multiple clusters from a single place, and unlock more use cases like version control.

image::images/wn-89-rules-as-metrics.png[]

[discrete]
== Enhancing Service Level Objectives with Multi-window burn rates & custom metrics

Starting in 8.9, the SLO burn rate rule allows you to set multi-window burn rates, which provide a powerful way to implement SLO-based alerting. The SLO alert rule can use multiple burn rates and time windows, and fire alerts when burn rates surpass a specified threshold. This option retains the benefits of alerting on burn rates and provides a method to make sure lower error rates will not be overlooked.

image::images/wn-89-enhancing-slos.png[]

Multiple burn rates allow you to adjust the alert to give appropriate priority based on how quickly you have to respond. If an issue will exhaust the error budget within hours or a few days, sending an active notification is appropriate.

Also introduced in 8.9 for SLOs is the new custom metric option for the KQL Service Level Indicator (SLI). For example, this allows users to base their alerts on a ratio of error rate compared to the overall rate of messages.

// end::whats-new[]
