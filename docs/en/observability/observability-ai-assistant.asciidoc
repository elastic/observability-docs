[[obs-ai-assistant]]
= Observability AI Assistant

preview::[]

The Observability AI Assistant is a large language model (LLM) integration that helps Elastic Observability users by adding context, explaining errors and messages, and suggesting remediation in the {observability} interface. You can connect the Observability AI Assistant to either the OpenAI or Microsoft Azure LLM service. 

You can find Observability AI Assistant prompts throughout {observability}:

[role="screenshot"]
image::images/obs-assistant.gif[Observability AI assistant preview, 90%]

[IMPORTANT]
====
The Observability AI Assistant is in technical preview, and its capabilities are still developing. Users should leverage it sensibly as the reliability of its responses might vary. Always cross-verify any returned advice for accurate threat detection and response, insights, and query generation.

Also, the data you provide to the Observability AI assistant is _not_ anonymized, and is stored and processed by the third-party AI provider. This includes any data used in conversations for analysis or context, such as alert or event data, detection rule configurations, and queries. Therefore, be careful about sharing any confidential or sensitive details while using this feature.
====

[discrete]
[[obs-ai-requirements]]
= Requirements

You need following to use the Observability AI Assistant:

* {stack} version 8.9 and later.
* This feature requires an https://www.elastic.co/pricing[Enterprise subscription]
* An account with a third-party generative AI provider that supports function calling. The Observability AI Assistant supports the following providers:
** OpenAI `gpt-3.5`+  
** Azure OpenAI Service apiVersion `2023-07-01-preview`+

[discrete]
[[obs-ai-set-up]]
= Set up the Observability AI Assistant

Complete the following steps to use the Observability AI Assistant:

. Create an API key with your AI provider to authenticate requests from the Observability AI Assistant. You'll use this in the next step. Refer to your provider's documentation for generating API keys:
+
* https://platform.openai.com/docs/api-reference[OpenAI]
* https://learn.microsoft.com/en-us/azure/cognitive-services/openai/reference[Azure OpenAI Service]

. Create a {kibana-ref}/gen-ai-action-type.html[Generative AI connector] using the AI provider's API key and URL to authenticate communication between {elastic-sec} and the provider.
You can do this in {kib} from *{stack-manage-app}* -> *{connectors-ui}*, or from within AI Assistant.
+
NOTE: 

[discrete]
[[obs-ai-interact]]
= Interact with the Observability AI Assistant

You can find Observability AI Assistant prompts throughout {observability} that provide the following information:

- *Universal Profiling* – explains the most expensive libraries and functions in your fleet and provides optimization suggestions.
- *Application performance monitoring (APM)* – explains APM errors and provides remediation suggestions.
- *Infrastructure Observability* – explains the processes running on a host.
- *Logs* – explains log messages and generates search patterns to find similar issues.
- *Alerting* – provides possible log spike causes and remediation suggestions.

For example, in the log details, you'll see prompts for *What's this message?* and *How do I find similar log messages?*: 

[role="screenshot"]
image::images/obs-ai-logs-prompts.png[]

Clicking a prompt generates a message specific to that log entry:

[role="screenshot"]
image::images/obs-ai-logs.gif[Observability AI assistant example, 75%]