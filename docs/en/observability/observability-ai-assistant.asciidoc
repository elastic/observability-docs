[[obs-ai-assistant]]
= Observability AI Assistant

preview::[]

The AI Assistant uses generative AI, powered by a {kibana-ref}/gen-ai-action-type.html[connector] for OpenAI or Azure OpenAI Service, to provide {observability} users with:

* *Contextual insights* — open prompts throughout {observability} that explain errors and messages and suggest remediation. 
* *Chat* —  have conversations with the AI Assistant. Chat uses function calling to request, analyze, and visualize your data.

[role="screenshot"]
image::images/obs-assistant2.gif[Observability AI assistant preview]

[discrete]
[[obs-ai-requirements]]
== Requirements

The AI Assistant requires the following:

* {stack} version 8.9 and later.
* An https://www.elastic.co/pricing[Enterprise subscription]
* An account with a third-party generative AI provider that supports function calling. The Observability AI Assistant supports the following providers:
** OpenAI `gpt-4`+.
** Azure OpenAI Service `gpt-4`(0613) or `gpt-4-32k`(0613) with API version `2023-07-01-preview` or more recent. 

[discrete]
[[data-information]]
== Your data and the AI Assistant

Elastic does not store or examine prompts or results used by the AI Assistant, or use this data for model training. This includes anything you send the model, such as alert or event data, detection rule configurations, queries, and prompts. However, any data you provide to the AI Assistant will be processed by the third-party provider you chose when setting up the Generative AI connector as part of the assistant setup.

Elastic does not control third-party tools, and assumes no responsibility or liability for their content, operation, or use, nor for any loss or damage that may arise from your using such tools. Please exercise caution when using AI tools with personal, sensitive, or confidential information. Any data you submit may be used by the provider for AI training or other purposes. There is no guarantee that the provider will keep any information you provide secure or confidential. You should familiarize yourself with the privacy practices and terms of use of any generative AI tools prior to use.

[discrete]
[[obs-ai-set-up]]
== Set up the AI Assistant

To use the AI Assistant:

. Create an API key from your AI provider to authenticate requests from the AI Assistant. You'll use this in the next step. Refer to your provider's documentation for information on generating API keys:
+
* https://platform.openai.com/docs/api-reference[OpenAI]
* https://learn.microsoft.com/en-us/azure/cognitive-services/openai/reference[Azure OpenAI Service]

. From *{stack-manage-app}* -> *{connectors-ui}* in {kib}, create a {kibana-ref}/gen-ai-action-type.html[Generative AI connector]. 
. Authenticate communication between {observability} and the AI provider by providing the following information:
.. Enter the AI provider's API endpoint URL in the *URL* field.
.. Enter the API key you created in the previous step in the *API Key* field. 

[discrete]
[[obs-ai-interact]]
== Interact with the AI Assistant

You can chat with the AI Assistant or interact with contextual prompts located throughout {observability}. See the following sections for more on interacting with the AI Assistant.

[discrete]
[[obs-ai-chat]]
=== AI Assistant chat

Click *AI Assistant* in the upper-right corner of any {observability} application to start the chat:

[role="screenshot"]
image::images/ai-assistant-button.png[Observability AI assistant preview]

This opens the AI Assistant flyout, where you can ask the assistant questions about your instance:

[role="screenshot"]
image::images/obs-ai-chat.png[Observability AI assistant chat, 60%]

The AI Assistant uses functions to include relevant context in the chat conversation through text, data, and visual components. Both you and the AI Assistant can suggest functions. You can also edit the AI Assistant's function suggestions and inspect function responses. 

The following table lists available functions:

[horizontal]
`summarize`:: Summarize parts of the conversation.
`recall`:: Recall previous learnings.
`lens`:: Create custom visualizations, using {kibana-ref}/lens.html[Lens], that you can add to dashboards.
`elasticsearch`:: Call Elasticsearch APIs on your behalf.
`kibana`:: Call Kibana APIs on your behalf.
`alerts`:: Get alerts for {observability}
`get_apm_timeseries`:: Display different APM metrics (such as throughput, failure rate, or latency) for any service or all services and any or all of their dependencies. Displayed both as a time series and as a single statistic. Additionally, the function  returns any changes, such as spikes, step and trend changes, or dips. You can also use it to compare data by requesting two different time ranges, or, for instance, two different service versions.
`get_apm_error_document`:: Get a sample error document based on the grouping name. This also includes the stacktrace of the error, which might hint to the cause.
`get_apm_correlations`:: Get field values that are more prominent in the foreground set than the background set. This can be useful in determining what attributes (like error.message, service.node.name or transaction.name) are contributing to, for instance, a higher latency. Another option is a time-based comparison, where you compare before and after a change point.
`get_apm_downstream_dependencies`:: Get the downstream dependencies (services or uninstrumented backends) for a service. This allows you to map the downstream dependency name to a service, by returning both `span.destination.service.resource` and s`ervice.name`. Use this to drill down further if needed.
`get_apm_service_summary`:: Get a summary of a single service, including the language, service version, deployments, the environments, and the infrastructure that it is running in, for instance on how many pods, and a list of its downstream dependencies. It also returns active alerts and anomalies.
`get_apm_services_list`:: Get the list of monitored services, their health status, and alerts.

[discrete]
[[obs-ai-prompts]]
=== AI Assistant contextual prompts

AI Assistant contextual prompts throughout {observability} provide the following information:

- *Universal Profiling* — explains the most expensive libraries and functions in your fleet and provides optimization suggestions.
- *Application performance monitoring (APM)* — explains APM errors and provides remediation suggestions.
- *Infrastructure Observability* — explains the processes running on a host.
- *Logs* — explains log messages and generates search patterns to find similar issues.
- *Alerting* — provides possible log spike causes and remediation suggestions.

For example, in the log details, you'll see prompts for *What's this message?* and *How do I find similar log messages?*: 

[role="screenshot"]
image::images/obs-ai-logs-prompts.png[]

Clicking a prompt generates a message specific to that log entry:

[role="screenshot"]
image::images/obs-ai-logs.gif[Observability AI assistant example, 75%]

You can continue a conversation from a contextual prompt by clicking *Start chat* to open the AI Assistant chat.