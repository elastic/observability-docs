[[obs-ai-assistant]]
= Observability AI Assistant

preview::[]

The AI Assistant uses generative AI, powered by a {kibana-ref}/gen-ai-action-type.html[connector] for OpenAI or Azure OpenAI Service, to provide {observability} users with:

* *Contextual insights* — open prompts throughout {observability} that explain errors and messages and suggest remediation. 
* *Chat* —  have conversations with the AI Assistant. Chat uses function calling to request, analyze, and visualize your data.

[role="screenshot"]
image::images/obs-assistant2.gif[Observability AI assistant preview]

[discrete]
[[obs-ai-requirements]]
== Requirements

The AI Assistant requires the following:

* {stack} version 8.9 and later.
* An https://www.elastic.co/pricing[Enterprise subscription]
* An account with a third-party generative AI provider that supports function calling. The Observability AI Assistant supports the following providers:
** OpenAI `gpt-4`+.
** Azure OpenAI Service `gpt-4`(0613) or `gpt-4-32k`(0613) with API version `2023-07-01-preview` or more recent. 

[discrete]
[[data-information]]
== Your data and the AI Assistant

Elastic does not store or examine prompts or results used by the AI Assistant, or use this data for model training. This includes anything you send the model, such as alert or event data, detection rule configurations, queries, and prompts. However, any data you provide to the AI Assistant will be processed by the third-party provider you chose when setting up the Generative AI connector as part of the assistant setup.

Elastic does not control third-party tools, and assumes no responsibility or liability for their content, operation, or use, nor for any loss or damage that may arise from your using such tools. Please exercise caution when using AI tools with personal, sensitive, or confidential information. Any data you submit may be used by the provider for AI training or other purposes. There is no guarantee that the provider will keep any information you provide secure or confidential. You should familiarize yourself with the privacy practices and terms of use of any generative AI tools prior to use.

[discrete]
[[obs-ai-set-up]]
== Set up the AI Assistant

To use the AI Assistant:

. Create an API key from your AI provider to authenticate requests from the AI Assistant. You'll use this in the next step. Refer to your provider's documentation for information on generating API keys:
+
* https://platform.openai.com/docs/api-reference[OpenAI]
* https://learn.microsoft.com/en-us/azure/cognitive-services/openai/reference[Azure OpenAI Service]

. From *{stack-manage-app}* -> *{connectors-ui}* in {kib}, create a {kibana-ref}/gen-ai-action-type.html[Generative AI connector]. 
. Authenticate communication between {observability} and the AI provider by providing the following information:
.. Enter the AI provider's API endpoint URL in the *URL* field.
.. Enter the API key you created in the previous step in the *API Key* field. 

[discrete]
[[obs-ai-add-data]]
== Add data to the AI Assistant knowledge base

The Observability AI Assistant's internal Knowledge Base is an index that contains information that the Assistant can recall at any time during a conversation, using our semantic search engine ELSER, allowing to obtain RAG (Retrieval Augmented Generation) responses based on internal knowledge.

You can enrich the Knowledge Base with internal information (such as Runbooks, Github Issues, Internal docs, Slack messages...) so the AI Assistant can have this internal context to provide specific assistance. Be aware that the AI provider that you configure may collect telemetry when using the AI Assistant, contact your AI provider for information on how data is collected.

The current process:

Ingest external info (e.g. Github Issues, Markdown files, Jira tickets, text files...) into Elasticsearch:
Using Elasticsearch Workplace Search connectors. For example, the Github connector can automatically capture, sync and index Issues, Markdown files, Pull Requests and Repos
Using Elasticsearch's Index API
Reindex that info into the AI Assistants Knowledge Base Index, using it's semantic seach pipeline. You can use the query below in Management > Dev Tools, making sure to replace the following:
InternalDocsIndex : name of the index where your internal docs are stored
text_field : name of the field with the text of your internal docs
timestamp : name of the field of the timestamp in your internal docs
public : (true or false) if true makes a document available to all users in the defined Kibana Space (if is defined) or in all spaces (if is not defined), if false document will restricted to the user indicated in
space : (can be null) if defined, restricts the internal document to be available in a specific Kibana Space
user.name : (can be null) if defined, restricts the internal document to be available for a specific user
You can add a "query" filter to index only certain docs
POST _reindex
{
    "source": {
        "index": "<InternalDocsIndex>",
        "_source": [
            "<text_field>",
            "<timestamp>",
            "namespace",
            "is_correction",
            "public",
            "confidence"
        ]
    },
    "dest": {
        "index": ".kibana-observability-ai-assistant-kb-000001",
        "pipeline": ".kibana-observability-ai-assistant-kb-ingest-pipeline"
    },
    "script": {
        "inline": "ctx._source.text = ctx._source.remove(\"<text_field>\");ctx._source.namespace=\"<space>\";ctx._source.is_correction=false;ctx._source.public=<public>;ctx._source.confidence=\"high\";ctx._source['@timestamp'] = ctx._source.remove(\"<timestamp>\");ctx._source['user.name'] = \"<user.name>\""
    }
}

[discrete]
[[obs-ai-interact]]
== Interact with the AI Assistant

You can chat with the AI Assistant or interact with contextual prompts located throughout {observability}. See the following sections for more on interacting with the AI Assistant.

[discrete]
[[obs-ai-chat]]
=== AI Assistant chat

Click *AI Assistant* in the upper-right corner of any {observability} application to start the chat:

[role="screenshot"]
image::images/ai-assistant-button.png[Observability AI assistant preview]

This opens the AI Assistant flyout, where you can ask the assistant questions about your instance:

[role="screenshot"]
image::images/obs-ai-chat.png[Observability AI assistant chat, 60%]

The AI Assistant uses functions to include relevant context in the chat conversation through text, data, and visual components. Both you and the AI Assistant can suggest functions. You can also edit the AI Assistant's function suggestions and inspect function responses. 

The following table lists available functions:

[horizontal]
`summarize`:: Summarize parts of the conversation.
`recall`:: Recall previous learnings.
`lens`:: Create custom visualizations, using {kibana-ref}/lens.html[Lens], that you can add to dashboards.
`elasticsearch`:: Call Elasticsearch APIs on your behalf.
`kibana`:: Call Kibana APIs on your behalf.
`alerts`:: Get alerts for {observability}
`get_apm_timeseries`:: Display different APM metrics (such as throughput, failure rate, or latency) for any service or all services and any or all of their dependencies. Displayed both as a time series and as a single statistic. Additionally, the function  returns any changes, such as spikes, step and trend changes, or dips. You can also use it to compare data by requesting two different time ranges, or, for instance, two different service versions.
`get_apm_error_document`:: Get a sample error document based on the grouping name. This also includes the stacktrace of the error, which might hint to the cause.
`get_apm_correlations`:: Get field values that are more prominent in the foreground set than the background set. This can be useful in determining what attributes (like error.message, service.node.name or transaction.name) are contributing to, for instance, a higher latency. Another option is a time-based comparison, where you compare before and after a change point.
`get_apm_downstream_dependencies`:: Get the downstream dependencies (services or uninstrumented backends) for a service. This allows you to map the downstream dependency name to a service, by returning both `span.destination.service.resource` and `service.name`. Use this to drill down further if needed.
`get_apm_service_summary`:: Get a summary of a single service, including the language, service version, deployments, the environments, and the infrastructure that it is running in, for instance on how many pods, and a list of its downstream dependencies. It also returns active alerts and anomalies.
`get_apm_services_list`:: Get the list of monitored services, their health status, and alerts.

[discrete]
[[obs-ai-prompts]]
=== AI Assistant contextual prompts

AI Assistant contextual prompts throughout {observability} provide the following information:

- *Universal Profiling* — explains the most expensive libraries and functions in your fleet and provides optimization suggestions.
- *Application performance monitoring (APM)* — explains APM errors and provides remediation suggestions.
- *Infrastructure Observability* — explains the processes running on a host.
- *Logs* — explains log messages and generates search patterns to find similar issues.
- *Alerting* — provides possible log spike causes and remediation suggestions.

For example, in the log details, you'll see prompts for *What's this message?* and *How do I find similar log messages?*: 

[role="screenshot"]
image::images/obs-ai-logs-prompts.png[]

Clicking a prompt generates a message specific to that log entry:

[role="screenshot"]
image::images/obs-ai-logs.gif[Observability AI assistant example, 75%]

You can continue a conversation from a contextual prompt by clicking *Start chat* to open the AI Assistant chat.