== Observability for Java Applications using the Elastic Stack

TLDR; A introduction from a Java developer perspective of how to add Elastic
Observability features to your application.

All of this is intended to be a hands-on approach. You should have some
familiarity with Java, how dependencies work, what a Jar file is etc. You
should try to follow the examples, change your code and then continue. Skipping
is possible, but then it is probably easier to make use of the
https://github.com/spinscale/observability-using-the-elastic-stack-java[GitHub
repository] with the accompanying code.

Let’s be honest, `Observability` as a term gets stretched to whatever fits its
description. There are definitions coming from modeling, that define when a
system is _observable_.

The main idea is to get more insight into your application. Sometimes
this insight is about what happened in the past, sometimes you want to
understand what is happening right now. Sometimes you are interested in
aggregates, sometimes you want to dig into a single event - maybe your
checkout failed once out of 10 times and you need to figure out why.

Everything requires a different level of insight and in the best case
you have all the information available.

If you have a multi-tiered application, you also may want to centralize
the data generated by all of your applications to be able to follow a
request through your whole stack.

IT Trends in the last decade have made the ability to peek into your
applications required

* Microservices resulting in more applications running in more different
languages
* Ephemeral applications: Resources are spun up and down based on
traffic patterns, thus you need to be able to capture data in real-time

=== Data types

Let’s differentiate between data used for Observability

* Logs: Events like a checkout, an exception or a HTTP request
* Metrics: Often aggregates like requests or transactions per minute
* APM: Profiling of your application to find performance bottlenecks
* External: External checks like trying to reach your application all
across the world

==== Data Overview

image:./images/monitor-java-app-introduction-diagram-observability.svg[Observability]



=== The sample Java application

One of the utmost priorities of this introduction is to keep the number
of rolling parts as small as possible along in order to focus on the
problem and not on the tooling or the framework.

Also, you should be able to get up and running on your local system as
fast as possible.

All you need is a modern JDK (OpenJDK 14 at the time of writing).

Following the mantra of using boring technologies with less magic. In the Java
space this usually means trying to not deal with reflection and annotation
driven systems. The web framework used throughout this article, will be
https://javalin.io/[javalin]. Keeping the dependencies as low as possible to
not confuse anyone.

The java application will feature a few endpoints: A main endpoint, an
artificially long running endpoint and one endpoint that needs to poll
another data source first. Lastly there will be a background job
running. All of those should be monitored.

==== Creating an application

Setting up a gradle project, that uses the gradle wrapper (this requires gradle
to be installed locally, alternatively you can always check out the git
repository). An option is to use the following command to bootstrap a small project:

[source,bash]
----
gradle init --type java-application \
  --test-framework junit-jupiter \
  --dsl groovy \
  --project-name javalin-app \
  --package de.spinscale.javalin
----

However, the above call creates a bootstrapped project with lots of comments and
some unwanted features enabled, so let’s hand-roll this instead. Create the
following `build.gradle` file with your favorite editor

[source,gradle]
----
plugins {
  id 'java'
  id 'application'
}

repositories {
  jcenter()
}

dependencies {
  implementation 'io.javalin:javalin:3.9.1'

  testImplementation 'org.junit.jupiter:junit-jupiter-api:5.6.2'
  testRuntimeOnly 'org.junit.jupiter:junit-jupiter-engine:5.6.2'
}

application {
  mainClassName = 'de.spinscale.javalin.App'
}

test {
  useJUnitPlatform()
}
----

Run

[source,bash]
----
echo "rootProject.name = 'javalin-app'" >> settings.gradle

mkdir -p src/main/java/de/spinscale/javalin
mkdir -p src/test/java/de/spinscale/javalin
----

Finally get the gradle wrapper installed (which again requires a
local gradle installation. An easy way to install gradle is to use
https://sdkman.io/[sdkman] and run `sdk install gradle 6.5.1`

[source,bash]
----
gradle wrapper
----

Now you are ready to run `./gradlew clean check` and should see a
successful build - that has nothing build or compiled yet.

==== The main endpoint

Create a javalin server and its first endpoint. Let’s create an
`src/main/java/de/spinscale/javalin/App.java` file

[source,java]
----
package de.spinscale.javalin;

import io.javalin.Javalin;

public class App {
    public static void main(String[] args) {
        Javalin app = Javalin.create().start(7000);
        app.get("/", ctx -> ctx.result("Appsolutely perfect"));
    }
}
----

Run `./gradlew assemble`

You end up with a compiled `App.class` file in the `build` directory,
but there is no way to start the server. Let’s create a so called
fat jar which contains our compiled class along with all needed
dependencies.

Fix the `plugins` part in `build.gradle` to look like this

[source,gradle]
----
plugins {
  id 'com.github.johnrengelman.shadow' version '6.0.0'
  id 'java'
}
----

Now you can run `./gradlew shadowJar`. This will create a file
`build/libs/javalin-app-all.jar`, which now can be run like this

[source,bash]
----
java -jar build/libs/javalin-app-all.jar
----

The `shadowJar` plugin requires some information about its main class. Fix
`build.gradle` one last time and add the following snippet

....
jar {
  manifest {
    attributes 'Main-Class': 'de.spinscale.javalin.App'
  }
}
....

Now rebuild the project and start the server via

[source,bash]
----
java -jar build/libs/javalin-app-all.jar
----

There is a little bit of start up output, but opening another terminal and running
`curl localhost:7000` shows a HTTP response.

Next step: a proper test. Putting everything into the `main()` method makes our
code hard to test. A dedicated handler fixes this.

Refactor the `App` class to this

[source,java]
----
package de.spinscale.javalin;

import io.javalin.Javalin;
import io.javalin.http.Handler;

public class App {
    
    public static void main(String[] args) {
        Javalin app = Javalin.create().start(7000);
        app.get("/", mainHandler());
    }

    static Handler mainHandler() {
        return ctx -> ctx.result("Appsolutely perfect");
    }
}
----

As the `Context` class is a big and complex object, mocking it out and
checking if the correct parameter to the `result()` method was handed
over sounds like a good-enough test for now.

Add a mockito & assertj dependency to the `build.gradle` file

[source,gradle]
----
dependencies {
  implementation 'io.javalin:javalin:3.9.1'

  testImplementation 'org.mockito:mockito-core:3.3.3'
  testImplementation 'org.assertj:assertj-core:3.16.1'
  testImplementation 'org.junit.jupiter:junit-jupiter-api:5.6.2'
  testRuntimeOnly 'org.junit.jupiter:junit-jupiter-engine:5.6.2'
}
----

Now create an `AppTests` class in `src/test/java/de/spinscale/javalin`.

[source,java]
----
package de.spinscale.javalin;

import io.javalin.http.Context;
import org.junit.jupiter.api.Test;

import javax.servlet.http.HttpServletRequest;
import javax.servlet.http.HttpServletResponse;
import java.io.IOException;
import java.nio.charset.StandardCharsets;
import java.util.HashMap;

import static de.spinscale.javalin.App.mainHandler;
import static org.assertj.core.api.Assertions.assertThat;
import static org.mockito.Mockito.mock;

public class AppTests {

    final HttpServletRequest req = mock(HttpServletRequest.class);
    final HttpServletResponse res = mock(HttpServletResponse.class);
    final Context ctx = new Context(req, res, new HashMap<>());

    @Test
    public void testMainHandler() throws Exception {
        mainHandler().handle(ctx);

        String response = resultStreamToString(ctx);
        assertThat(response).isEqualTo("Appsolutely perfect");
    }

    private String resultStreamToString(Context ctx) throws IOException {
        final byte[] bytes = ctx.resultStream().readAllBytes();
        return new String(bytes, StandardCharsets.UTF_8);
    }
}
----

One could go ahead here for hours with more refactoring and
integration tests, but let’s stick with this rather small unit test for
now.

For now, tests pass and the application can be build & packaged via

[source,bash]
----
./gradle clean check shadowJar
----


=== Setting up the Elastic Stack

Before improving our application, let’s get up and running with the
Elastic Stack using https://www.elastic.co/cloud/[Elastic Cloud]. This will
spare you the work to set up anything locally, even though this would work as
well. Not only this saves some setup time, encryption and authentication is
also set up.

Go to https://cloud.elastic.co[cloud.elastic.co] and sign up for an account.
Don’t worry, you will get a free trial, when you up a cluster for the first
time. Also, no need to supply a credit card.

After signing up, you can log in and will be at a page like this one.

image:./images/monitor-java-app-cloud-after-login.png[Cloud After Login]

Click on the blue `Create Deployment` button.

The first step is to name your deployment. Pick something useful like
`observability-javalin-app` as deployment name. Pick your cloud platform
and region (potentially something that is nearby) and leave the Elastic
Stack version as is, as this is always the latest.

image:./images/monitor-java-app-cloud-deployment-1.png[Cloud Deployment Part 1]

There is no need to pick any of those two check boxes when setting up
your deployment.

The next part is about picking special setups for certain use-cases -
which makes sense if you know, what kind of use-case in terms of
indexing, searching and access patterns you have. By default, you are
good with the `I/O Optimized` one.

If you are still on the free tier, you can ignore the pricing part for
now. Click on `Create deployment` and grab a coffee.

image:./images/monitor-java-app-cloud-deployment-2.png[Cloud Deployment Part 2]

You will see a screen for a few minutes now, while your deployment gets
setup.

image:./images/monitor-java-app-cloud-deployment-ongoing.png[Cloud Deployment Ongoing]

*IMPORTANT*: This screen contains a username and a password. This is the admin
access for you cluster with all privileges. Put it in your password manager
make sure you can access your cluster!

Once your deployment is finished, the screen will be updated with the
instances that are running. With the chosen deployment, an Elasticsearch
cluster is started, Kibana and also an APM server. That will come in handy
later on, when getting APM up and running.

image:./images/monitor-java-app-cloud-deployment-finished.png[Cloud Deployment Finished]

If you want to prepare for the next chapter, you copy & paste the cloud
id out of the deployment and also copy the Elasticsearch endpoint in
your notes, so you can access them faster.

In order to see if everything has worked, let’s log into Kibana. Click
on the `Launch` button of the Kibana instance and use the credentials
that are listed on the page to log in.

image:./images/monitor-java-app-kibana-login.gif[Login via Kibana]

Now, time to index some log data!



=== Logging

Application logging needs to be fixed, then those logs should be
indexed into Elasticsearch. Javalin is independent from a concrete logging
implementation, as mentioned in the
https://javalin.io/documentation#logging[documentation].

Remember starting up the application? The output mentioned, there is no
proper logging installed

[source,text]
----
SLF4J: Failed to load class "org.slf4j.impl.StaticLoggerBinder".
SLF4J: Defaulting to no-operation (NOP) logger implementation
SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.
-------------------------------------------------------------------

-------------------------------------------------------------------
Missing dependency 'Slf4j simple'. Add the dependency.

pom.xml:
<dependency>
    <groupId>org.slf4j</groupId>
    <artifactId>slf4j-simple</artifactId>
    <version>1.7.30</version>
</dependency>

build.gradle:
compile "org.slf4j:slf4j-simple:1.7.30"
----

==== Adding a logging implementation

Picking log4j2 as our logging implementation in this example, so
let’s add the dependency to our `build.gradle` file.

[source,gradle]
----
dependencies {
  implementation 'io.javalin:javalin:3.9.1'
  implementation 'org.apache.logging.log4j:log4j-slf4j18-impl:2.13.3'

  ...
}
----

Rebuilding the app via `./gradlew clean shadowJar` and starting it makes
the logging messages vanish. But is it actually logging anything?

Well, let’s change a handler to do some fancy logging

[source,java]
----
public class App {

    private static final Logger logger = LoggerFactory.getLogger(App.class);

    public static void main(String[] args) {
        Javalin app = Javalin.create();
        app.get("/", mainHandler());
        app.start(7000);
    }

    static Handler mainHandler() {
        return ctx -> {
            logger.info("This is an informative logging message, user agent [{}]", ctx.userAgent());
            ctx.result("Appsolutely perfect");
        };
    }
}
----

Note, that the logger call needs to be put within the lambda, otherwise
the log message will only be logged on start up.

Do another rebuild and start the server, call the URL - and see
absolutely nothing?! What’s wrong here?

First, your logging is enabled, when you replace `logger.info` with
`logger.error` you will see a log message.

The problem here is, that the specified log level `INFO` is not logged
currently for this logger. Let’s create a log4j2 configuration in
`src/main/resources/log4j2.xml`

[source,xml]
----
<?xml version="1.0" encoding="UTF-8"?>
<Configuration>
  <Appenders>
    <Console name="Console" target="SYSTEM_OUT">
      <PatternLayout pattern="%d{HH:mm:ss.SSS} [%-5level] %logger{36} %msg%n"/>
    </Console>
  </Appenders>
  <Loggers>
    <Logger name="de.spinscale.javalin.App" level="INFO"/>
    <Root level="ERROR">
      <AppenderRef ref="Console" />
    </Root>
  </Loggers>
</Configuration>
----

This logs by default on level `ERROR`, but for the `App` class there is
an additional configuration, so that all `INFO` logs are logged as well.
After repackaging and restarting log messages like the following will appear
in the terminal

[source,text]
----
17:17:40.019 [INFO ] de.spinscale.javalin.App - This is an informative logging message, user agent [curl/7.64.1]
----

One could argue about the formatting of this log messages, for example
the date is rather bad and not unique. This will be fixed very soon.

==== Request logging

Depending on the application traffic and if this happens somewhere
outside of the application like your k8s ingress layer, it may also make
sense to log each and every request on the application level. Thankfully
javalin has a neat mechanism for this, let’s adapt our `App` class a
little bit.

[source,java]
----
public class App {

    private static final Logger logger = LoggerFactory.getLogger(App.class);

    public static void main(String[] args) {
        Javalin app = Javalin.create(config -> {
            config.requestLogger((ctx, executionTimeMs) -> {
                logger.info("{} {} {} {} \"{}\" {}",
                        ctx.method(),  ctx.url(), ctx.req.getRemoteHost(),
                        ctx.res.getStatus(), ctx.userAgent(), executionTimeMs.longValue());
           });
        });
        app.get("/", mainHandler());
        app.start(7000);
    }

    static Handler mainHandler() {
        return ctx -> {
            logger.info("This is an informative logging message, user agent [{}]", ctx.userAgent());
            ctx.result("Appsolutely perfect");
        };
    }
}
----

After rebuilding & restarting the app, log messages like are logged for each
request:

[source,text]
----
10:43:50.066 [INFO ] de.spinscale.javalin.App - GET / 200 0:0:0:0:0:0:0:1 "curl/7.64.1" 7
----

==== Log with an ISO8601 timestamp

There are two more things to get right, before indexing logs into
Elasticsearch. First, make sure to create a real ISO8601 timestamp.

Luckily this is an easy fix in the `log4j2.xml` file by replacing the
pattern layout like this

[source,text]
----
<PatternLayout pattern="%d{ISO8601_OFFSET_DATE_TIME_HHCMM} [%-5level] %logger{36} %msg%n"/>
----

Now log entries are received containing timestamps like this:

[source,text]
----
2020-07-03T14:25:40,378+02:00 [INFO ] de.spinscale.javalin.App GET / 200 0:0:0:0:0:0:0:1 "curl/7.64.1" 0
----

This saves the needs to do any calculation for timestamps when ingesting
logs, as this is a unique point in time including the timezone. Having a
timezone becomes more important once you are running across data centers
and try to follow data streams.

==== Log into files

There are two options to read logging output. Either from standard out or
using files. The first option is usually used in ephemeral environments like
containers, the second option can be used if logs can be persisted.

Time to improve logging. Write data into a file and to stdout -
also, you can improve the formatting on stdout a little bit and sprinkle
in some color!

This will be our new `log4j2.xml` file

[source,xml]
----
<?xml version="1.0" encoding="UTF-8"?>
<Configuration>
  <Appenders>
    <Console name="Console" target="SYSTEM_OUT">
      <PatternLayout pattern="%highlight{%d{ISO8601_OFFSET_DATE_TIME_HHCMM} [%-5level] %logger{36} %msg%n}"/>
    </Console>
    <File name="JavalinAppLog" fileName="/tmp/javalin/app.log">
      <PatternLayout pattern="%d{ISO8601_OFFSET_DATE_TIME_HHCMM} [%-5level] %logger{36} %msg%n"/>
    </File>
  </Appenders>
  <Loggers>
    <Logger name="de.spinscale.javalin.App" level="INFO"/>
    <Root level="ERROR">
      <AppenderRef ref="Console" />
      <AppenderRef ref="JavalinAppLog" />
    </Root>
  </Loggers>
</Configuration>
----

After restarting the app and sending a request you will logs flowing
into `/tmp/javalin/app.log` - time to move those logs into the Elastic
Stack!

==== Use Filebeat to read logs

Next up is to read the log file and sent it over to Elasticsearch. This
requires a couple of steps, but the first one is to download Filebeat.
You can either download Filebeat from the
https://www.elastic.co/downloads/beats/filebeat[elastic.co website] or
use a package manager like yum, apt or homebrew to install it. You will
find instructions in the download link as well. This example assumes
downloading the tar.gz archive.

Download the archive end unpack it.

==== Creating a secure keystore

In order to not put any passwords in configuration files use the
Filebeat keystore to store
https://www.elastic.co/guide/en/beats/filebeat/current/keystore.html[secure
settings].

First, let’s store the cloud id in the keystore

[source,bash]
----
echo -n "observability-javalin-app:ZXUtY2VudHJhbC0xLmF3cy5jbG91ZC5lcy5pbyQ4NDU5M2I1YmQzYTY0N2NhYjA2MWQ3NTJhZWFhNWEzYyQzYmQwMWE2OTQ2MmQ0N2ExYjdhYTkwMzI0YjJiOTMyYQ==" | ./filebeat keystore add CLOUD_ID --stdin
----

In order to store logs in Elasticsearch with minimal permissions, let’s
create an API key to send data from Filebeat to Elasticsearch.

Log into Kibana as the `elastic` user and Go to
`Management > Dev Tools`, where you will find the console to send
requests. Send this request to create an API key for our Filebeat

[source,console]
----
POST /_security/api_key
{
  "name": "filebeat_javalin-app", 
  "role_descriptors": {
    "filebeat_writer": { 
      "cluster": ["monitor", "read_ilm"],
      "index": [
        {
          "names": ["filebeat-*"],
          "privileges": ["view_index_metadata", "create_doc"]
        }
      ]
    }
  }
}
----

The response contains an `api_key` and an `id` field, which can be put into
the Filebeat keystore via

[source,bash]
----
echo -n "IhrJJHMB4JmIUAPLuM35:1GbfxhkMT8COBB4JWY3pvQ" | ./filebeat keystore add ES_API_KEY --stdin
----

Make sure you specify the `-n` parameter, otherwise you will have
painful debugging sessions, because of adding a newline at the end of
your API key.

You can run `./filebeat keystore list` to see if both settings have been
stored.

==== Loading Filebeat dashboards

Use the `elastic` super user to load the dashboards, as it requires
different privileges than just writing the data.

[source,bash]
----
./filebeat setup -e -E 'cloud.id=${CLOUD_ID}' -E 'cloud.auth=elastic:YOUR_SUPER_SECRET_PASS'
----

Pro-tip: If you do not want to store those credentials in your
`.history` file of your shell, add a space at the beginning of the line.
Depending on the shell configuration these commands will not be added to
the history.

==== Configure Filebeat

Next step is to tell Filebeat, where to read data from and where to sent it
to. Create a fresh `filebeat.yml` file. Most of this is copied from the
default configuration file, minus a ton of comments.

[source,yml]
----
name: javalin-app-shipper

filebeat.inputs:
- type: log
  paths:
    - /tmp/javalin/*.log

cloud.id: ${CLOUD_ID}
output.elasticsearch:
  api_key: ${ES_API_KEY}
----

==== Sending data to Elasticsearch

All right, let’s get Filebeat started by running `./filebeat -e` to log
to the console.

In the log output you should see a line like

[source,text]
----
2020-07-03T15:41:56.532+0200    INFO    log/harvester.go:297    Harvester started for file: /tmp/javalin/app.log
----

Time to create some log entries for our application! You can use a tool
like https://github.com/wg/wrk[wrk] to sent some requests to the
application.

[source,bash]
----
wrk -t1 -c 100 -d10s http://localhost:7000
----

On a three year old notebook, this resulted roughly in 8k requests per
second, and the equivalent of log lines written.

==== Using Kibana: Discover

Open the `Kibana > Discover` app and you will see a screenshot like this

image:./images/monitor-java-app-kibana-discover.png[Kibana Discover]

There is a summary of the documents at the top, but one can dive into each
document. So let’s take a look what a single document looks like

image:./images/monitor-java-app-kibana-expand.png[Kibana Discover Expand]

Clicking on the small arrow next to the `Time` column of the first document
expands the document. There is also a link `View single document` on the
right to see the following view

image:./images/monitor-java-app-kibana-single-document.png[Kibana single document view]

Take a look at the screenshot above.  First, far more data than just the
event is indexed. Information about the offset in the file, information
about the component shipping the logs - you can see the name of the shipper
in the output, and somewhere in between there is a `message` field which
contains log line contents.  There is also an immediate flaw in the request
logging. If the user agent is `null`, something else than `null` should be
returned. Fix that right away and enjoy that actually reading our logs is
crucial and just indexing them will probably not gain us anything.  So, this
is the new request logger

[source,java]
----
Javalin app = Javalin.create(config -> {
    config.requestLogger((ctx, executionTimeMs) -> {
        String userAgent = ctx.userAgent() != null ? ctx.userAgent() : "-";
        logger.info("{} {} {} {} \"{}\" {}",
                ctx.method(), ctx.req.getPathInfo(), ctx.res.getStatus(),
                ctx.req.getRemoteHost(), userAgent, executionTimeMs.longValue());
    });
});
----

If you’re into it, you may also want to fix this in the logging message
in the main handler. And if you’re into speed, you probably would like
to call `ctx.userAgent()` only once.

==== Using Kibana: Logs UI

There is a great UI for logs that has been added rather recently.
You can find it in the Kibana menu under `Observability > Logs`. It
basically is a `tail -f` in your browser, but with the power of search
underneath!

If you want to see the streaming feature at work, run a curl request in
a loop while sleeping

[source,bash]
----
while $(sleep 0.7) ; do curl localhost:7000 ; done
----

Click on `Stream live` at the bottom and you will see continuous log
messages streaming in. You can also highlight certain terms like in this
screenshot.

image:./images/monitor-java-app-kibana-streaming.png[Kibana Logs UI Streaming]

After have indexed logs, they are picked up by Filebeat and
this looks nice. However looking at the indexed document there is big
room for improvement. Looking at one of the documents being indexed,
they contain our log message in a single field. You can verify this by
looking at one of those documents

[source,console]
----
GET filebeat-*/_search
{
  "size": 1
}
----

There are a few things to note:

* Take a look at the `@timestamp` field and compare it with the timestamp
of the log message. It differs, so when filtering based on the
`@timestamp` field, you will not get the results you expect, as the current
`@timestamp` field reflects the timestamp of the event creation within
Filebeat, but not the timestamp, when the log event occurred in the
application.
* The log level cannot be filtered on, same applies to the class name.

This requires structuring logs further and try to extract more data from a
single log line into several fields.

==== Structuring your logs

Let’s take another look at a log message generated by our app:

[source,text]
----
2020-07-03T15:45:01,479+02:00 [INFO ] de.spinscale.javalin.App This is an informative logging message
----

This message has four parts: timestamp, log level, class, message. The
rules of splitting are halfway obvious as well, as most of them involve
white space.

Good news: all beats have solid capabilities to process a log line before
sending it to Elasticsearch by using so called
https://www.elastic.co/guide/en/beats/filebeat/current/filtering-and-enhancing-data.html[processors].
If the capabilities of these processors are not enough, you can always go
another route and let Elasticsearch do the heavy lifting by using
https://www.elastic.co/guide/en/elasticsearch/reference/7.8/ingest.html[an
ingest node]. This is what many modules in Filebeat do. A module in Filebeat
is a way to parse a specific log file format for a certain software.

Let’s try this by using a couple of processors and only Filebeat
configuration.

[source,yaml]
----
processors:
  - add_host_metadata: ~
  - dissect:
      tokenizer: '%{timestamp} [%{log.level}] %{log.logger} %{message_content}'
      field: "message"
      target_prefix: ""
  - timestamp:
      field: "timestamp"
      layouts:
        - '2006-01-02T15:04:05.999Z0700'
      test:
        - '2020-07-18T04:59:51.123+0200'
  - drop_fields:
      fields: [ "message", "timestamp" ]
  - rename:
      fields:
        - from: "message_content"
        - to: "message"
----

The `dissect` processor splits the log message into four parts. One still
wants to have the last part of the original message in the `message` field,
you need to remove the old `message` field first and then rename the field -
there is no in-place replacement with the dissect filter.

There is also a dedicated timestamp parsing so that the `@timestamp`
field will be filled with parsed value. Drop the duplicated
fields, but ensure that a part of the original message is still
available in the `message` field.

[NOTE]
The removal of parts of the original message is debatable!  Keeping the
original message around makes a lot of sense to me. With the above example,
debugging might become hard if parsing the timestamp does not work as
expected.

There is also a slight difference in the parsing of a timestamp as the
go time parser only accepts dots as separator between seconds and
milliseconds, but our default output of the log4j2 is using a comma.

Either one can fix the timestamp in the logging output to look like one
expected from Filebeat. This would result in the following pattern layout

[source,xml]
----
  <PatternLayout pattern="%d{yyyy-MM-dd'T'HH:mm:ss.SSSZ} [%-5level] %logger{36} %msg%n"/>
----

Fixing the timestamp parsing is another way, as you do not always have
full control over your logs and change their format. Imagine using some
third party software. For now, this will be good enough though.

You can restart the Filebeat after the change, and have a look what
changed in an indexed JSON document by running this search (and of
course having another log message indexed):

[source,console]
----
GET filebeat-7.8.0/_search?filter_path=**._source
{
  "size": 1,
  "_source": {
    "excludes": [
      "host.ip",
      "host.mac"
    ]
  },
  "sort": [
    {
      "@timestamp": {
        "order": "desc"
      }
    }
  ]
}
----

this will return a document like this

[source,console-response]
----
{
  "hits" : {
    "hits" : [
      {
        "_source" : {
          "input" : {
            "type" : "log"
          },
          "agent" : {
            "hostname" : "rhincodon",
            "name" : "javalin-app-shipper",
            "id" : "95705f0c-b472-4bcc-8b01-2d387c0d309b",
            "type" : "filebeat",
            "ephemeral_id" : "e4df883f-6073-4a90-a4c4-9e116704f871",
            "version" : "7.8.0"
          },
          "@timestamp" : "2020-07-03T15:11:51.925Z",
          "ecs" : {
            "version" : "1.5.0"
          },
          "log" : {
            "file" : {
              "path" : "/tmp/javalin/app.log"
            },
            "offset" : 1440,
            "level" : "ERROR",
            "logger" : "de.spinscale.javalin.App"
          },
          "host" : {
            "hostname" : "rhincodon",
            "os" : {
              "build" : "19F101",
              "kernel" : "19.5.0",
              "name" : "Mac OS X",
              "family" : "darwin",
              "version" : "10.15.5",
              "platform" : "darwin"
            },
            "name" : "javalin-app-shipper",
            "id" : "C28736BF-0EB3-5A04-BE85-C27A62C99316",
            "architecture" : "x86_64"
          },
          "message" : "This is an informative logging message, user agent [curl/7.64.1]"
        }
      }
    ]
  }
}
----

You can see that the `message` field only contains the last part of our log
message. In addition, there is a `log.level` and `log.logger` field.  There
is one last thing: when the log level is `INFO`, it is logged with an
additional space at the end. One could use a
https://www.elastic.co/guide/en/beats/filebeat/current/processor-script.html[script
processor] and call `trim()`, but I think in this case it might be easier to
fix our logging configuration to not always emit 5 characters, regardless of
the log level length. You can still keep this when writing to standard out,
to have a more cleaned readable log, if you want.

[source,xml]
----
<File name="JavalinAppLog" fileName="/tmp/javalin/app.log">
  <PatternLayout pattern="%d{yyyy-MM-dd'T'HH:mm:ss.SSSZ} [%level] %logger{36} %msg%n"/>
</File>
----

Let’s get to the devil… exceptions :-)

==== Parsing exceptions

Exceptions are a special treat in the case of logging. They are spanning
multiple lines, so the old rule of one message per line does not exist
in case of exceptions.

Let’s add an endpoint, that triggers an exception first and make sure it
is logged by using an exception mapper.

[source,java]
----
app.get("/exception", ctx -> {
    throw new IllegalArgumentException("not yet implemented");
});

app.exception(Exception.class, (e, ctx) -> {
    logger.error("Exception found", e);
    ctx.status(500).result(e.getMessage());
});
----

Now calling `/exception` will return a HTTP 500 error to the client, but
it will leave a stack trace like this in the logs

[source,text]
----
2020-07-06T11:27:29,491+02:00 [ERROR] de.spinscale.javalin.App Exception found
java.lang.IllegalArgumentException: not yet implemented
    at de.spinscale.javalin.App.lambda$main$2(App.java:24) ~[classes/:?]
    at io.javalin.core.security.SecurityUtil.noopAccessManager(SecurityUtil.kt:23) ~[javalin-3.9.1.jar:?]
    at io.javalin.http.JavalinServlet$addHandler$protectedHandler$1.handle(JavalinServlet.kt:119) ~[javalin-3.9.1.jar:?]
    at io.javalin.http.JavalinServlet$service$2$1.invoke(JavalinServlet.kt:45) ~[javalin-3.9.1.jar:?]
    at io.javalin.http.JavalinServlet$service$2$1.invoke(JavalinServlet.kt:24) ~[javalin-3.9.1.jar:?]

  ... goes on and on and on and own ...
----

Checking out this stack trace, there is one attribute that helps parsing it:
It looks different compared to a regular log message as each new line starts
with a white space, thus different from a log message starting with the date
right at the beginning. Let’s add this logic to our beats configuration

[source,yaml]
----
- type: log
  enabled: true
  paths:
    - /tmp/javalin/*.log
  multiline.pattern: ^20
  multiline.negate: true
  multiline.match: after
----

So verbatim translation of the above settings says to treat everything
as part of an existing message, that is not starting with `20` in a line
- the `20` resembles the beginning year of your timestamps. Some users
prefer to wrap the date in `[]` to make this easier to understand.

*NOTE*: This introduces state into your logging. You cannot split a
log file among several processors now, as every log line could still be
belonging to the current event. This is not a bad thing, but again
something to be aware off.

After restarting Filebeat and your javalin app, you can trigger an
exception and will see a looooong stack trace in the `message` field of
your logs.

==== Ensure log rotation

In order to not grow logs endlessly, let’s add some log rotation to your
logging configuration.

[source,xml]
----
<?xml version="1.0" encoding="UTF-8"?>
<Configuration>
  <Appenders>
    <Console name="Console" target="SYSTEM_OUT">
      <PatternLayout pattern="%highlight{%d{ISO8601_OFFSET_DATE_TIME_HHCMM} [%-5level] %logger{36} %msg%n}"/>
    </Console>

    <RollingFile name="JavalinAppLogRolling" fileName="/tmp/javalin/app.log" filePattern="/tmp/javalin/%d{yyyy-MM-dd}-%i.log.gz">
      <PatternLayout pattern="%d{yyyy-MM-dd'T'HH:mm:ss.SSSZ} [%level] %logger{36} %msg%n"/>
      <Policies>
        <TimeBasedTriggeringPolicy />
        <SizeBasedTriggeringPolicy size="50 MB"/>
      </Policies>
      <DefaultRolloverStrategy max="20"/>
    </RollingFile>
  </Appenders>

  <Loggers>
    <Root level="error">
      <AppenderRef ref="Console" />
      <AppenderRef ref="JavalinAppLogRolling" />
    </Root>
  </Loggers>
</Configuration>
----

The sample added a `JavalinAppLogRolling` appender to our configuration, that
uses the same logging pattern as before, but rolls over if a new day
starts or if the log file has reached 50 megabyte. If a new log file is
created, older log files are gzipped as well to take less space on disk.
Note that the size of 50 megabyte refers to the unpacked file size, so
the potentially twenty files on disk will be much smaller each.

==== Using Elasticsearch node ingest feature

The built-in modules are almost entirely using the
https://www.elastic.co/guide/en/elasticsearch/reference/7.8/ingest.html[Node
Ingest] feature of Elasticsearch instead of the beats processors.

Let’s take a look how a proper pipeline would look like using
Elasticsearch and what would need to changed for the Filebeat to work
like before.

One of the nicest parts of the ingest pipeline is the ability to easily
debug by using the
https://www.elastic.co/guide/en/elasticsearch/reference/current/simulate-pipeline-api.html[Simulate
Pipeline API].

So, let’s try to write a pipeline, that is similar to our Filebeat
processors

[source,console]
----
# Store the pipeline in Elasticsearch
PUT _ingest/pipeline/javalin_pipeline
{
  "processors": [
    {
      "dissect": {
        "field": "message",
        "pattern": "%{@timestamp} [%{log.level}] %{log.logger} %{message}"
      }
    },
    {
      "trim": {
        "field": "log.level"
      }
    },
    {
      "date": {
        "field": "@timestamp",
        "formats": [
          "ISO8601"
        ]
      }
    }
  ]
}
  
# Test the pipeline
POST _ingest/pipeline/javalin_pipeline/_simulate
{
  "docs": [
    {
      "_source": {
        "message": "2020-07-06T13:39:51,737+02:00 [INFO ] de.spinscale.javalin.App This is an informative logging message"
      }
    }
  ]
}
----

You can see the created fields of the pipeline in the output which now
look like the Filebeat processors earlier. As the ingest pipeline
works on a document level, you still need to check for exceptions where
the logs are generated and let the Filebeat create a single message out
of that. You could even implement the log level trimming with a single
processor and date parsing was also pretty easy, as the Elasticsearch
ISO8601 parser correctly identifies a comma instead of a dot when
splitting seconds and milliseconds!

Now, on to the Filebeat configuration. First let’s remove all the
processors, except the
https://www.elastic.co/guide/en/beats/filebeat/7.8/add-host-metadata.html[add_host_metadata
processor] to add some host information like the host name and operating
system.

[source,yaml]
----
processors:
  - add_host_metadata: ~
----

Next, revamp the Elasticsearch output to ensure the pipeline will be
referred to when a document is indexed from this beat

[source,yaml]
----
cloud.id: ${CLOUD_ID}
output.elasticsearch:
  api_key: ${ES_API_KEY}
  pipeline: javalin_pipeline
----

Restart your beat and see if logs are flowing in as expected!

So, now you learned about parsing logs in either beats or Elasticsearch
- but what if never needed to think about parsing our logs?

==== Write logs as JSON

Writing out logs as plain text works and is easy to read for humans.
However first writing them out as plain text, and then parsing them using
the `dissect` processors and create JSON again sounds tedious and burns
unneeded CPU cycles.

While log4j2 has a
https://logging.apache.org/log4j/2.x/manual/layouts.html#JSONLayout[JSONLayout],
one can even go further and use a Library called
https://github.com/elastic/ecs-logging-java[ecs-logging-java].

The advantage of ECS logging is in the name, that it uses ECS - the
https://www.elastic.co/guide/en/ecs/current/index.html[Elastic Common
Schema].

____
ECS defines a common set of fields to be used when storing event data in
Elasticsearch, such as logs and metrics.
____

So instead of writing our own logging standard, use an existing one. So,
let’s get the logging dependency into our javalin application.

[source,gradle]
----
dependencies {
  implementation 'io.javalin:javalin:3.9.1'
  implementation 'org.apache.logging.log4j:log4j-slf4j18-impl:2.13.3'
  implementation 'co.elastic.logging:log4j2-ecs-layout:0.4.0'

  testImplementation 'org.mockito:mockito-core:3.3.3'
  testImplementation 'org.assertj:assertj-core:3.16.1'
  testImplementation 'org.junit.jupiter:junit-jupiter-api:5.6.2'
  testRuntimeOnly 'org.junit.jupiter:junit-jupiter-engine:5.6.2'
}
----

The log4j2-ecs-layout ships with a custom `<EcsLayout>` which can be used
in the logging setup for the rolling file appender

[source,xml]
----
<RollingFile name="JavalinAppLogRolling" fileName="/tmp/javalin/app.log" filePattern="/tmp/javalin/%d{yyyy-MM-dd}-%i.log.gz">
  <EcsLayout serviceName="my-javalin-app"/>
  <Policies>
    <TimeBasedTriggeringPolicy />
    <SizeBasedTriggeringPolicy size="50 MB"/>
  </Policies>
  <DefaultRolloverStrategy max="20"/>
</RollingFile>
----

Once you restart your app, you will see pure JSON written to your
log file. When you are triggering an exception, you will see, that the
stack trace is within your single document already! This means, the
Filebeat configuration can become stateless and even more lightweight.
Also, the ingest pipeline on the Elasticsearch side can be deleted
again.

You can configure a few
https://github.com/elastic/ecs-logging-java/tree/master/log4j2-ecs-layout[more
parameters] for the `EcsLayout`, but defaults have been chosen wisely. Let’s
fix the Filebeat configuration:

[source,yaml]
----
filebeat.inputs:
- type: log
  enabled: true
  paths:
    - /tmp/javalin/*.log
  json.keys_under_root: true

name: javalin-app-shipper

cloud.id: ${CLOUD_ID}
output.elasticsearch:
  api_key: ${ES_API_KEY}

# ================================= Processors =================================
processors:
  - add_host_metadata: ~
----

As you can see, just by writing out logs as JSON, our whole logging
setup got a ton easier, so whenever possible, try to directly
write your logs as JSON.





=== Metrics

Time ingest some metrics to get a different angled view of your application.

==== What are Metrics

Metrics are usually point in time values, that can change anytime. The
number of current requests can change any millisecond. You could have a
spike of a 1000 requests and then everything goes back to one requests. This
also means, that these kind of metrics may not be accurate, and you also
want to pull min/max values to get some more indication. Furthermore this
implies, that you need to think about the duration of those metrics as well.
Do you need those once per minute or every 10 seconds?

==== Push versus Pull

A big distinction of acquiring those metrics is push vs. pull. On the
hand, you can configure the system that produces those metrics to push
those into the metrics store. On the other hand you could have the
metrics store or a polling component to retrieve the metrics from the
application. Both are valid cases and there is lots of fight over the
internet what is the best method to gather metrics. Long story short: Do
what suits you and not listening to every voice on the internet is great
trait. If you are interested in more, there is a paragraph in the
https://prometheus.io/docs/introduction/faq/#why-do-you-pull-rather-than-push?[Prometheus
FAQ] or in the
https://prometheus.io/blog/2016/07/23/pull-does-not-scale-or-does-it/[Prometheus blog].

==== Adding metrics to the Java Application

The example will implement a pull based approach. In order to get data into
Elasticsearch, the
https://www.elastic.co/guide/en/beats/metricbeat/current/metricbeat-module-prometheus.html[Metricbeat
Prometheus Module] will be used.

The underlying library used in our app is
http://micrometer.io/[micrometer.io], a vendor neutral application
metrics facade in combination with its
http://micrometer.io/docs/registry/prometheus[Prometheus support] to
implement a pull based model. One could however also use the
http://micrometer.io/docs/registry/elastic[elastic support] to implement
a push based model. This would require use to store credential data of
the Elasticsearch cluster in our app. This example keeps this data in
the surrounding tools - again a matter of taste.

Add some dependencies to our `build.gradle`:

[source,gradle]
----
  // metrics via micrometer
  implementation 'io.micrometer:micrometer-core:1.5.2'
  implementation 'io.micrometer:micrometer-registry-prometheus:1.5.2'
  implementation 'org.apache.commons:commons-lang3:3.10'
----

Now let’s add the micrometer plugin to our Javalin app

[source,java]
----
Javalin app = Javalin.create(config -> {
   ...
   config.registerPlugin(new MicrometerPlugin());
);
----

Lastly, add a new metrics endpoint

[source,java]
----
final Micrometer micrometer = new Micrometer();
app.get("/metrics", ctx -> {
  ctx.status(404);
  if (ctx.basicAuthCredentialsExist()) {
    final BasicAuthCredentials credentials = ctx.basicAuthCredentials();
    if ("metrics".equals(credentials.getUsername()) && "secret".equals(credentials.getPassword())) {
      ctx.status(200).result(micrometer.scrape());
    }
  }
});
----

The `MicroMeter` class here is a self written class, that sets up a
couple of metrics monitor, and also creates the registry
for Prometheus - which in turn provides the text based Prometheus output

[source,java]
----
package de.spinscale.javalin;

import io.micrometer.core.instrument.Metrics;
import io.micrometer.core.instrument.binder.jvm.JvmCompilationMetrics;
import io.micrometer.core.instrument.binder.jvm.JvmGcMetrics;
import io.micrometer.core.instrument.binder.jvm.JvmHeapPressureMetrics;
import io.micrometer.core.instrument.binder.jvm.JvmMemoryMetrics;
import io.micrometer.core.instrument.binder.jvm.JvmThreadMetrics;
import io.micrometer.core.instrument.binder.logging.Log4j2Metrics;
import io.micrometer.core.instrument.binder.system.FileDescriptorMetrics;
import io.micrometer.core.instrument.binder.system.ProcessorMetrics;
import io.micrometer.core.instrument.binder.system.UptimeMetrics;
import io.micrometer.prometheus.PrometheusConfig;
import io.micrometer.prometheus.PrometheusMeterRegistry;

public class Micrometer {

    final PrometheusMeterRegistry registry = new PrometheusMeterRegistry(new PrometheusConfig() {
        @Override
        public String get(String key) {
            return null;
        }

        @Override
        public String prefix() {
            return "javalin";
        }
    });

    public Micrometer() {
        Metrics.addRegistry(registry);
        new JvmGcMetrics().bindTo(Metrics.globalRegistry);
        new JvmHeapPressureMetrics().bindTo(Metrics.globalRegistry);
        new JvmThreadMetrics().bindTo(Metrics.globalRegistry);
        new JvmCompilationMetrics().bindTo(Metrics.globalRegistry);
        new JvmMemoryMetrics().bindTo(Metrics.globalRegistry);
        new Log4j2Metrics().bindTo(Metrics.globalRegistry);
        new UptimeMetrics().bindTo(Metrics.globalRegistry);
        new FileDescriptorMetrics().bindTo(Metrics.globalRegistry);
        new ProcessorMetrics().bindTo(Metrics.globalRegistry);
    }

    public String scrape() {
        return registry.scrape();
    }
}
----

That’s it! Restart your app and poll the metrics endpoint.

[source,bash]
----
curl localhost:7000/metrics -u metrics:secret
----

This returns a line based response with one metric per line. This is the
common Prometheus format.

==== Reading metrics with Metricbeat

Time to download Metricbeat, go ahead and
https://www.elastic.co/downloads/beats/metricbeat[download & install].

Similar to the Filebeat setup run the initial setup of all the dashboards
using the admin user, and then use an API key as well.

[source,console]
----
POST /_security/api_key
{
  "name": "metricbeat_javalin-app",
  "role_descriptors": {
    "metricbeat_writer": {
      "cluster": ["monitor", "read_ilm"],
      "index": [
        {
          "names": ["metricbeat-*"],
          "privileges": ["view_index_metadata", "create_doc"]
        }
      ]
    }
  }
}
----

Don’t forget to do the initial setup like this

[source,bash]
----
./metricbeat setup -e -E 'cloud.id=${CLOUD_ID}' -E 'cloud.auth=elastic:YOUR_SUPER_SECRET_PASS'
----

Then store the combination of `id` and `api_key` fields in the keystore

[source,bash]
----
./metricbeat keystore create
echo -n "IhrJJHMB4JmIUAPLuM35:1GbfxhkMT8COBB4JWY3pvQ" | ./metricbeat keystore add ES_API_KEY --stdin
echo -n "observability-javalin-app:ZXUtY2VudHJhbC0xLmF3cy5jbG91ZC5lcy5pbyQ4NDU5M2I1YmQzYTY0N2NhYjA2MWQ3NTJhZWFhNWEzYyQzYmQwMWE2OTQ2MmQ0N2ExYjdhYTkwMzI0YjJiOTMyYQ==" | ./metricbeat keystore add CLOUD_ID --stdin
----

Finally configure Metricbeat to read our Prometheus metrics. Start with a
basic `metricbeat.yaml`

[source,yaml]
----
metricbeat.config.modules:
  path: ${path.config}/modules.d/*.yml
  reload.enabled: false

name: javalin-metrics-shipper

cloud.id: ${CLOUD_ID}
output.elasticsearch:
  api_key: ${ES_API_KEY}

processors:
  - add_host_metadata: ~
  - add_cloud_metadata: ~
  - add_docker_metadata: ~
  - add_kubernetes_metadata: ~
----

As Metricbeat supports dozens of modules, which in turn are different
ways of gathering metrics (the same applies to Filebeat with different
types of log files and formats), the Prometheus module needs to be enabled

[source,bash]
----
./metricbeat modules enable prometheus
----

The Prometheus endpoint to poll needs to be added in
`./modules.d/prometheus.yml`:

[source,yaml]
----
- module: prometheus
  period: 10s
  hosts: ["localhost:7000"]
  metrics_path: /metrics
  username: "metrics"
  password: "secret"
  use_types: true
  rate_counters: true
----

In order to improve security, you should add the username and the
password to the keystore and refer it here as well.

Now start Metricbeat and verify that Prometheus events are flowing into
Elasticsearch.

[source,console]
----
GET metricbeat-7.8.0/_search?filter_path=**.prometheus,hits.total
{
  "query": {
    "term": {
      "event.module": "prometheus"
    }
  }
}
----

==== Visualize number of logging messages

As this is custom data from our javalin app, there is no pre-defined
dashboard for such custom data.

First let’s check for the number of logging messages per log level.

[source,console]
----
GET metricbeat-7.8.0/_search
{
  "query": {
    "exists": {
      "field": "prometheus.log4j2_events_total.counter"
    }
  }
}
----

Visualize the number of log messages over time, split by the
log level. Since the Elastic Stack 7.7, there is a new way of creating a
visualization called `Lens`

Click on `Kibana > Visualize` and select `Create Visualization`.

Create a line chart for now. The basic idea is to have a
https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-metrics-max-aggregation.html[max
aggregation] on the y-axis on the `prometheus.log4j2_events_total.rate`
field, where as the x-axis is split by date using a
https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-bucket-datehistogram-aggregation.html[date_histogram
aggregation] on the `@timestamp` field. There is one more split within
each date histogram bucket, split by log level, using a
https://www.elastic.co/guide/en/elasticsearch/reference/7.8/search-aggregations-bucket-terms-aggregation.html[terms
aggregation] on the `prometheus.labels.level`, which contains the log
level. Also, increase the size of the log level to six to display
every log level.

The final result should look like this:

image:./images/monitor-java-app-metrics-kibana-create-visualization-log-rate.png[Date Histogram of
the log rate per log level]

==== Visualize open files over time

The second visualization is be a check for the number of open
files in our application.

As no one can remember all the field names, let’s take a look at the metrics
output again first.

[source,bash]
----
curl -s localhost:7000/metrics -u metrics:secret | grep ^process
process_files_max_files 10240.0
process_cpu_usage 1.8120711232436825E-4
process_uptime_seconds 72903.726
process_start_time_seconds 1.594048883317E9
process_files_open_files 61.0
----

The `process_files_open_files` metric looks like a good start. In the best
case this should be a rather static value, that rarely changes. If you run
an application which stores data within the JVM or opens and closes network
sockets, this usually goes up a lot and hopefully also down again depending
on the load. With a web application, this is rather static. For the sake of
the argument, let’s figure out, why there are actually 60 files open on our
tiny little web app!

Run `jps` first, that will contain your App in the process list

[source,bash]
----
$ jps
14224 Jps
82437 Launcher
82438 App
40895
----

Then you use `lsof` on that process

[source,bash]
----
$ lsof -p 82438
----

You will actually see more output than just all the files being opened,
as a file is also a TCP connection happening right now.

So, let’s add a small endpoint to increase the number of open files by
having long running HTTP connections (each connection is also considered
an open file as it requires a file descriptor) and then run `wrk`
against it.

[source,java]
----
final Executor executor = CompletableFuture.delayedExecutor(20, TimeUnit.SECONDS);
app.get("/wait", ctx -> {
    CompletableFuture<String> future = CompletableFuture.supplyAsync(() -> "done", executor);
    ctx.result(future);
});
----

Every future gets delayed by 20 seconds, which means that a single HTTP
request stays open for 20 seconds.

Let’s run a `wrk` workload

[source,bash]
----
wrk -c 100 -t 20 -d 5m http://localhost:7000/wait
----

Results show, that barely sent twenty requests were sent, which makes sense
given the processing time.

Now let’s build a visualization using
https://www.elastic.co/guide/en/kibana/current/lens.html[Lens], a
relatively new tool in Kibana which makes it easier to build
visualizations.

image:./images/monitor-java-app-metrics-kibana-create-visualization-open-files.png[Lens
visualization]

The first thing you need to do is to select the `metricbeat-*` index
pattern at the top left below the `Add filter`, as this will likely use
`filebeat-*` as the default. The x-axis uses the `@timestamp` field -
which in turn will create a `date_histogram` aggregation again. The
y-axis should not be the document count, as that one will always be
stable, but the maximum value of the documents in the buckets. You can
just click on the right on field name of the y-axis and select `Max` and
this should give you a similar visualization than shown, with a peak
where you ran the `wrk` command above.

==== Predefined metric dashboards

Before closing this introductory chapter I’d like to stress a point: In
many use-cases there is no need for you to create a dedicated
dashboard, unless you are reading custom metrics like the one from our
Javalin application.

Let’s take a look at a couple of predefined dashboards, that you get out
of the box. Click on `Kibana > Dashboard` and search for `metricbeat',
so you can open the `[Metricbeat System] Overview ECS` dashboard.

image:./images/monitor-java-app-metrics-metricbeat-system-dashboard.png[Metricbeat System Overview
Dashboard]

As you can see this features a nice overview of a development machine.
Based on the load you can see the start of a workday. Also
Zoom and Slack are the biggest CPU consumers along with Docker and
IntelliJ eating their fair share of memory.

==== Metrics UI

There is one last app to show in the Observability section of Kibana,
and that is the `Metrics UI`. An overview over all your shippers sending
data to Elasticsearch. If you click on it, you will only see data from a
single shipper, so it does not look super spectacular, but the moment
you are running several services and the ability to group this per
Kubernetes pod or host will allow you to easily spot hosts with elevated
CPU or memory consumption. If you click on `Metrics Explorer`, you can
start exploring your data for certain hosts or the CPU usage across your
nodes.

Let’s do two examples

image:./images/monitor-java-app-metrics-ui-prometheus-file-handles.png[Metrics UI with open
file handles]

This list of the currently open files vs. the maximum allowed file handles,
which are a bit more than 10k on my notebook. You can see, even with the
`wrk` test of open held connections there was no significant change of open
file handles. This would require some proper real time workload.

image:./images/monitor-java-app-metrics-ui-prometheus-event-counter.png[Metrics UI Log
Counter]

This UI uses an area chart on the total events counter that is emitted
by the javalin app. It’s rising steadily because there is a component
polling an endpoint that in turn produces another log message. The
steeper peek was due to sending more requests. But where is the sudden
drop-off coming from? Simple: A JVM restart. As those metrics are not
persisted, they will be reset on a JVM restart. With that in mind, it’s
often better to log the `rate` instead of the `counter` field.

That concludes the dive into metrics, and next up is APM.








=== APM

The third piece of Observability is Application Performance Management.
The ability to get exact information which part of your code is slow. An
APM setup consists of an APM server which accepts the data (and is
already running within our Elastic Cloud setup) and an agent delivering
the data to the server.

The agent has two tasks. First, instrumenting the java application in
order to extract application performance information and second the
ability to send that data to the APM server.

Java has a very good infrastructure to support this particular use-case
compared to other languages. There is a dedicated interface for so
called java agents to hook up. These agents have the ability to change
the execution of a java program before it starts up, as an agent can
manipulate the bytecode before it is loaded by the JVM. Using this you
can implement security mechanisms by disallowing certain actions or
method calls or you can add information how long a single method is
executing or how often a single method is called. The latter is what an
APM agent is doing. Instead of changing the behavior of the Java program
it basically observes its execution.

The https://www.elastic.co/guide/en/apm/agent/java/current/[Elastic APM
agent] uses a well known and used library called https://bytebuddy.net/#/[Byte
Buddy] to do this instrumentation.

One of the advantages of this whole Java Agent mechanism is the ability
to change an application without requiring to recompile it. This way
even an organization that differentiates between software packaging and
operations could add the agent to their applications, that are provided
by the developers (this is not a recommendation of such a practice!).

One of the core ideas of APM is the ability to follow the flow of a user
session across your whole stack - regardless if you are having dozens of
microservices or a monolith answering your user requests. This implies
the ability to tag a request across your whole stack.

This is how the architecture looks like. Your applications are running
APM agents, which report into Elastic Cloud.

image:./images/monitor-java-app-apm-architecture-cloud.png[APM architecture using Elastic
Cloud]

In order to fully caption user activity, you need to start in the
browser of the user using RUM (real user monitoring) down to your
application, which sends a SQL query to your database.

==== Data Model

Despite a heavily fragmented APM landscape the terminology roughly is
the same everywhere. The two most important terms are *Spans* and
*Transactions*.

A transaction encapsulates a series of spans, which in turn contain
information about the execution of a piece of code. Let’s take a look at
this screenshot from the Kibana APM UI.

image:./images/monitor-java-app-apm-transaction.png[A transaction with spans]

This is a Spring Boot application. The
`UserProfileController.showProfile()` is called, which is marked as the
transaction. There are two spans within. First, a request is sent to
Elasticsearch using the Elasticsearch REST client and after the the
response is rendered using Thymeleaf. The request to Elasticsearch is
actually faster than the rendering in this case.

The Java APM agent has the ability to instrument certain frameworks
automatically. Spring and Spring Boot are supported really well and the
above data was created by just adding the agent to the Spring Boot
application without doing any configuration.

In addition, there are more agents than just the Java one. There are
agents for Go, .NET, Node, Python, Ruby and the browser (RUM). Agents
keep getting added so you may want to check the
https://www.elastic.co/guide/en/apm/agent/index.html[current
documentation].

==== Adding the APM agent to our code

You have two options to add Java agent instrumentation to your
application.

===== Agent

First, you can add the agent via a parameter, when calling the `java`
binary. This way it does not interfere with the packaging of the
application. This mechanism instruments the application when starting
up.

First download the agent, you can check
https://search.maven.org/search?q=g:co.elastic.apm%20AND%20a:elastic-apm-agent[for
the most recent version]

[source,bash]
----
wget https://repo1.maven.org/maven2/co/elastic/apm/elastic-apm-agent/1.17.0/elastic-apm-agent-1.17.0.jar
----

The agent needs to be specified on startup as well as configuration
parameters where to send the APM data. Before starting the java application,
let’s get an API key for our APM server running in Elastic Cloud.

When you check your deployment in Elastic Cloud and click on `APM` on
the left, you will see the `APM Server Secret Token`, which you can use.
Also you can copy the APM endpoint URL from there.

image:./images/monitor-java-app-elastic-cloud-apm-token.png[Elastic Cloud APM Token]

[source,bash]
----
java -javaagent:/path/to/elastic-apm-agent-1.17.0.jar\
  -Delastic.apm.service_name=javalin-app \
  -Delastic.apm.application_packages=de.spinscale.javalin \
  -Delastic.apm.server_urls=$APM_ENDPOINT_URL \
  -Delastic.apm.secret_token=PqWTHGtHZS2i0ZuBol \
  -jar build/libs/javalin-app-all.jar
----

You could now go ahead and open up the APM UI and you should see the
data flowing in.

===== Automatic attachment

If you do not want to change the start up options of your application,
the standalone agent allows you to attach to running JVMs on a host.

This requires you to download the standalone jar. You can find the link
on the
https://www.elastic.co/guide/en/apm/agent/java/current/setup-attach-cli.html[official
docs].

In order to list your locally running java application, you can run

....
java -jar /path/to/apm-agent-attach-1.17.0-standalone.jar --list
....

As I usually run more than a single java app on my system, I specify the
application to attach to. Also, make sure, that you have stopped your
javalin application with the agent already attached and just start a
regular javalin app without the agent configured in order to attach.

[source,bash]
----
java -jar /tmp/apm-agent-attach-1.17.0-standalone.jar --pid 30730 \
  --config service_name=javalin-app \
  --config application_packages=de.spinscale.javalin \
  --config server_urls=$APM_ENDPOINT_URL \
  --config secret_token=PqWTHGtHZS2i0ZuBol
----

This above message will return something like this:

[source,text]
----
2020-07-10 15:04:48.144  INFO Attaching the Elastic APM agent to 30730
2020-07-10 15:04:49.649  INFO Done
----

So now the agent was attached to a running application with a special
configuration.

While both of the first two possibilities work, you can also use the third
one: using the APM agent as a direct dependency. This will allow to write
custom spans and transactions within our application.

===== Programmatic setup

So, let’s add the java agent dependency

[source,gradle]
----
dependencies {
  ... 
  implementation 'co.elastic.apm:apm-agent-attach:1.17.0'
  ...
}
----

Now, instrument the application right at the start in our `main()` method.

[source,java]
----
public static void main(String[] args) {
    ElasticApmAttacher.attach();
    ...
}
----

We did not configure any endpoint or API tokens yet. While the
https://www.elastic.co/guide/en/apm/agent/java/current/setup-attach-api.html#setup-attach-api-configuration[documentation]
recommends to use the `src/main/resources/elasticapm.properties` file, I
personally prefer the use of environment variables, as this prevents
either committing API tokens to your source or merging another
repository. Mechanisms like https://www.vaultproject.io/[vault] allow
you to manage your secrets in such a way.

For our local deployment I usually use something like
https://direnv.net/[direnv] for local setup. `direnv` is an extension
for your local shell, that loads/unloads environment variables when you
enter a directory, like your application. `direnv` can do quite a bit
more like loading the right node/ruby version or adding directory to
your $PATH variable.

In order to enable `direnv` you need to create a `.envrc` file with this
content:

[source,text]
----
dotenv
----

This tells `direnv` to load the contents of the `.env` file as
environment variables. The `.env` file should look like this

[source,bash]
----
ELASTIC_APM_SERVICE_NAME=javalin-app
ELASTIC_APM_SERVER_URLS=https://APM_ENDPOINT_URL
ELASTIC_APM_SECRET_TOKEN=PqWTHGtHZS2i0ZuBol
----

If you are not comfortable with putting sensitive data in that `.env`
file, you can use tools like https://github.com/sorah/envchain[envchain]
or call arbitrary commands in the `.envrc` file like accessing vault.

That said, you can now run the java application like you did before

[source,bash]
----
java -jar build/libs/javalin-app-all.jar
----

If you want to run this in your IDE, you can either set the environment
variables manually or search for a plugin that support `.env` files.

Wait a few minutes and let’s finally take a look at the APM UI.

image:./images/monitor-java-app-apm-ui-javalin-app.png[Javalin App APM UI]

As you can see, this is quite the difference to the Spring Boot
application shown earlier. The different endpoints are not listed, we
can see the requests per minute though including errors.

The only transaction comes from a single servlet, which is not too helpful.
Let’s try to fix this by introducing custom programmatic transactions.

==== Custom Transactions

First, add another dependency

[source,gradle]
----
dependencies {
  ...
  implementation 'co.elastic.apm:apm-agent-attach:1.17.0'
  implementation 'co.elastic.apm:apm-agent-api:1.17.0'
  ...
}
----

Next, let’s fix the name of the transactions to include the HTTP method
and the request path

[source,java]
----
app.before(ctx -> ElasticApm.currentTransaction()
  .setName(ctx.method() + " " + ctx.req.getPathInfo()));
----

Restart your app and see data flowing in, test a few different
endpoints, especially the one, that throws exceptions and also one, that
triggers a 404.

image:./images/monitor-java-app-apm-ui-javalin-with-transaction-names.png[APM UI with
correct transaction names]

This looks much better, having differences between endpoints.

Add another endpoint to actually see the power of transactions, which polls
another HTTP service. You may have heard of https://wttr.in/[wttr.in], a
service to poll weather information from. Time to implement a proxy
HTTP method that forwards the request to that endpoint.

In order to do that, a HTTP client is needed. Let’s use
https://hc.apache.org/httpcomponents-client-4.5.x/quickstart.html[Apache
HTTP client], one of the most common HTTP clients out there.

[source,gradle]
----
implementation 'org.apache.httpcomponents:fluent-hc:4.5.12'
----

This is our new endpoint

[source,java]
----
app.get("/weather/:city", ctx -> {
  String city = ctx.pathParam("city");
  ctx.result(Request.Get("https://wttr.in/" + city + "?format=3").execute()
      .returnContent().asBytes())
    .contentType("text/plain; charset=utf-8");
});
----

Now you can go ahead and curl `http://localhost:7000/weather/Munich` and
see a one-line response about the current weather. Let’s check the APM
UI.

In the overview you can see now, that most time is spent in the HTTP
client, which is not too surprising.

image:./images/monitor-java-app-apm-ui-javalin-wttr-1.png[Overview]

Our transactions for the `/weather/Munich` now contains a span, that
shows how much time is spent for retrieving the weather data. Because
the HTTP client is instrumented automatically there is no need to do
anything!

image:./images/monitor-java-app-apm-ui-javalin-wttr-2.png[Transaction with span]

This poses a problem however! If the `city` parameter if that URL is of high
cardinality, this will result in a high amount of URLs mentioned instead of
the generic endpoint. The solution to this is to fix the transaction naming to this:

[source,java]
----
// better transaction names for each URL
app.before(ctx -> ElasticApm.currentTransaction().setName(ctx.method() + " " + ctx.matchedPath()));
----

==== Method tracing via configuration

Instead of writing code to trace methods, you can also configure the
agent to do this. Let’s do something fun and try to figure out, if
logging is a bottleneck for our application and trace the request logger
statements added earlier.

The agent has the ability to
https://www.elastic.co/guide/en/apm/agent/java/current/config-core.html#config-trace-methods[trace
methods] based on their signature.

The interface to monitor would be the `io.javalin.http.RequestLogger`
interface with the `handle` method. So let’s try
`io.javalin.http.RequestLogger#handle` to identify the method to log
and put this in your `.envrc`.

....
ELASTIC_APM_TRACE_METHODS="de.spinscale.javalin.Log4j2RequestLogger#handle"
....

Create a dedicated logger class as well to match the above trace method.

[source,java]
----
package de.spinscale.javalin;

import io.javalin.http.Context;
import io.javalin.http.RequestLogger;
import org.jetbrains.annotations.NotNull;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

public class Log4j2RequestLogger implements RequestLogger  {

    private final Logger logger = LoggerFactory.getLogger(Log4j2RequestLogger.class);

    @Override
    public void handle(@NotNull Context ctx, @NotNull Float executionTimeMs) throws Exception {
        String userAgent = ctx.userAgent() != null ? ctx.userAgent() : "-";
        logger.info("{} {} {} {} \"{}\" {}",
                ctx.method(), ctx.req.getPathInfo(), ctx.res.getStatus(),
                ctx.req.getRemoteHost(), userAgent, executionTimeMs.longValue());
    }
}
----

And fix the call in our `App` class.

[source,java]
----
config.requestLogger(new Log4j2RequestLogger());
----

Restart your app, and see how much time your logging takes.

image:./images/monitor-java-app-apm-ui-logging-trace.png[Logging caller trace]

The request logger takes roughly 400 microseconds. The whole request takes
about 1.3 milliseconds. Roughly a third of our requests processing goes into
logging. If you are on the quest for a faster service, you may want to
rethink logging. However this logging happens, after the result is written
to the client, so while the total processing time increases with logging,
responding back to the client does not (closing the connection however might
be). Also note, that these tests were conducted without proper warm up, I
assume that after proper JVM warm up you will have *much* faster processing of
requests.

==== Method tracing via profiling inferred spans

Once you have a bigger application with more code paths than our sample
app, you can try to enable the
https://www.elastic.co/guide/en/apm/agent/java/current/config-profiling.html#config-profiling-inferred-spans-enabled[automatic
profiling of inferred spans] by setting

[source,bash]
----
ELASTIC_APM_PROFILING_INFERRED_SPANS_ENABLED=true
----

This mechanism uses the
https://github.com/jvm-profiling-tools/async-profiler[async profiler] to
create spans without you having to instrument anything allowing you to
find bottlenecks faster.

==== Log correlation

As already introduced in Java ECS logging, go one step further and ease the
https://www.elastic.co/guide/en/apm/agent/java/current/config-logging.html#config-enable-log-correlation[correlation
of logs] by adding the transaction ids to our logs.

This can be done via an agent configuration

[source,bash]
----
ELASTIC_APM_ENABLE_LOG_CORRELATION=true
----

After adding this, you can check the generated log files that are sent
to Elasticsearch via Filebeat. An entry now looks like this

[source,json]
----
{
  "@timestamp": "2020-07-13T12:03:22.491Z",
  "log.level": "INFO",
  "message": "GET / 200 0:0:0:0:0:0:0:1 \"curl/7.64.1\" 0",
  "service.name": "my-javalin-app",
  "event.dataset": "my-javalin-app.log",
  "process.thread.name": "qtp34871826-36",
  "log.logger": "de.spinscale.javalin.Log4j2RequestLogger",
  "trace.id": "ed735860ec0cd3ee3bdf80ed7ea47afb",
  "transaction.id": "8af7dff698937dc5"
}
----

having the `trace.id` and `transaction.id` added, in case of an error
you will get an `error.id` field.

==== But wait… there’s more.

We have not covered the
https://www.elastic.co/guide/en/apm/agent/java/current/opentracing-bridge.html[Elastic
APM OpenTracing bridge] or dove into the
https://www.elastic.co/guide/en/apm/agent/java/current/metrics.html[additional
metrics] the agent provides, which allows us to take a look at things
like garbage collection or memory footprint of our application.






=== Uptime

There is some good basic monitoring capabilities in our app so far. We index
logs (with traces), we index metrics and we even can look in our app to
figure out single performance bottlenecks thanks to APM. However there is
still one weak spot. Everything done so far was within the application. All
the users however are reaching the application from the internet. They come
from places all over the world.

How about checking, if our users have the same experience that our APM data
is suggesting us?  Imagine having a lagging load balancer fronting your app,
that costs you an additional 50ms per request. That would be devastating. Or
TLS negotiation being very expensive. Or an internet provider loses a
undersea cable and latency to a certain region across the globe starts to
explode. Even though none of those external events are your fault, you will
still be impacted by this and should try to mitigate those - which means you
need to know about them first.

Enter https://www.elastic.co/uptime-monitoring[Uptime].

Uptime allows to not only a monitor a service being up or down, but also
graph latencies over time, get notified about expiring TLS certificates (For
sure, no one of you ever had an outage because of an expired TLS cert,
right?).

==== Setup

Let’s first download heartbeat (the polling component) and configure it
to check for our application.

After downloading and unpacking we have to set up the cloud id and the
password one more time.

We need to create another `API_KEY` as elastic admin user in Kibana.

[source,console]
----
POST /_security/api_key
{
  "name": "heartbeat_javalin-app",
  "role_descriptors": {
    "metricbeat_writer": {
      "cluster": ["monitor", "read_ilm"],
      "index": [
        {
          "names": ["heartbeat-*"],
          "privileges": ["view_index_metadata", "create_doc"]
        }
      ]
    }
  }
}
----

Let’s setup the heartbeat keystore and run the setup

[source,bash]
----
./heartbeat keystore create
echo -n "observability-javalin-app:ZXUtY2VudHJhbC0xLmF3cy5jbG91ZC5lcy5pbyQ4NDU5M2I1YmQzYTY0N2NhYjA2MWQ3NTJhZWFhNWEzYyQzYmQwMWE2OTQ2MmQ0N2ExYjdhYTkwMzI0YjJiOTMyYQ==" | ./heartbeat keystore add CLOUD_ID --stdin
echo -n "SCdUSHMB1JmLUFPLgWAY:R3PQzBWW3faJT01wxXD6uw" | ./heartbeat keystore add ES_API_KEY --stdin

./heartbeat setup -e -E 'cloud.id=${CLOUD_ID}' -E 'cloud.auth=elastic:YOUR_SUPER_SECRET_PASS'
----

Add some services to monitor - which of course will not be
super spectacular, as this primarily monitors localhost based services.

[source,yaml]
----
name: heartbeat-shipper

cloud.id: ${CLOUD_ID}
output.elasticsearch:
  api_key: ${ES_API_KEY}

heartbeat.monitors:
  - type: http
    id: javalin-http-app
    name: "Javalin Web Application"
    urls: ["http://localhost:7000"]
    check.response.status: [200]
    schedule: '@every 15s'

  - type: http
    id: httpbin-get
    name: "httpbin GET"
    urls: ["https://httpbin.org/get"]
    check.response.status: [200]
    schedule: '@every 15s'

  - type: tcp
    id: javalin-tcp
    name: "TCP Port 7000"
    hosts: ["localhost:7000"]
    schedule: '@every 15s'

processors:
  - add_observer_metadata:
      geo:
        name: europe-munich
        location: "48.138791, 11.583030"
----

Now start heartbeat and wait a couple of minutes to get some data. You
can find the Uptime UI via `Observability > Uptime`. The overview looks
like this

image:./images/monitor-java-app-uptime-overview.png[Uptime Overview]

You can see the list of monitors and a global overview. Let’s see the
details for one of those alerts, click on `Javalin Web Application`.

A few interesting tidbits: First one can see the execution for the last
scheduled checks, but the duration for each check might be more interesting.
You can see if the latency for one of your checks is going up. The second
interesting part is the world map at the top. By specifying in the
configuration where the check originated, which was in Munich in Europe in
this case. By configuring several heartbeats running across the world, you
can compare latencies and figure out, in which data center you need run your
app, to be next to your users.

The duration of the monitor is in the low milliseconds, as it is really
fast. Check the monitor for the `httpbin.org` endpoint and you will see a
much higher duration. In this case it is about 400ms for each request. This
is not too surprising, because the endpoint is not nearby and you need to
initiate a TLS connection for every request, which is costly.

While most of this functionality does not look super awesome and is
rather a nice UI on top, we will cover the anomaly detection later,
allowing us to automatically detect latency increases.

That said, do not underestimate the importance of this kind of
monitoring. Also, consider this just the beginning as the next step is
to have synthetics that monitor the correct behavior of your
application, for example to ensure that your checkout process works all
the time.
