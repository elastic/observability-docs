[[categorize-logs]]
= Categorize log entries

Application log events are often unstructured and contain variable data. Many
log messages are the same or very similar, so classifying them can reduce
millions of log lines into just a few categories.

Within the {logs-app}, the *Categories* page enables you to identify patterns in
your log events quickly. Instead of manually identifying similar logs, the logs 
categorization view lists log events that have been grouped based on their 
messages and formats so that you can take action quicker.

[[create-log-categories]]
== Create log categories

Create a {ml} job to categorize log messages automatically. {ml-cap} observes 
the static parts of the message, cluster similar messages, and classify them 
into message categories.

[role="screenshot"]
image::images/log-create-categorization-job.jpg[Configure log categorization job]

1. Select *Categories*, and you are prompted to use {ml}, which carries out the 
   log rate categorizations.
2. Choose a time range for the {ml} analysis. By default, the {ml} job analizes 
   log messages no lder than four weeks and continues indefinitely.
3. Add the indices that contain the logs you want to analyze.
4. Click *Create ML job*. The job is created and it starts run. It takes a few 
   minutes for the {ml} robots to collect the necessary data. After the job 
   processed the data, you can view the results.

[[analyze-log-categories]]
== Analyze log categories

The *Categories* page lists all the log categories from the selected indices. 
You can filter the categories by indices. Below, you can view the categories 
from the `elastic.agent` log along with the maximum anomaly score on the right.

[role="screenshot"]
image::images/log-categories.png[Log categories]

The highlighted log category has a very high anomaly score, and looks like this:

[source,console-result]
----------------------------------
⁕type⁕server⁕timestamp⁕level⁕component⁕o.e.m.j.JvmGcMonitorService⁕cluster.name
⁕elasticsearch⁕node.name⁕message⁕gc⁕overhead⁕spent⁕collecting⁕in⁕the⁕last⁕cluster.uuid⁕
jx_Bw17eT_OQshT2IrDuzg⁕node.id⁕
----------------------------------

Here is an example of a log line in this category:

[source,console-result]
----------------------------------
[elasticsearch.server][INFO] {"type": "server", "timestamp": "2020-01-31T22:51:03,633Z",
"level": "INFO", "component": "o.e.m.j.JvmGcMonitorService", "cluster.name": "elasticsearch",
"node.name": "elasticsearch-master-0", "message": "[gc][411617] overhead, spent [316ms]
collecting in the last [1s]", "cluster.uuid": "jx_Bw17eT_OQshT2IrDuzg", "node.id": "JwIW6wI2T6WsgpoaUB3eaA" }
----------------------------------

The anomaly indicates an unexpected slow down of garbage collection runs, which 
could indicate low JVM memory problems impacting {es} performance.

Suppose you were already alerted about low JVM memory through the monitoring 
setup. In that case, logs categorization makes it easier to find the root cause 
by indicating the above category of `stopped garbage collection`.
