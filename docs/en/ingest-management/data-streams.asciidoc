[[data-streams]]
= Data streams

{agent} uses data streams to store time series data across multiple indices
while giving you a single named resource for requests.
Data streams are well-suited for logs, metrics, traces, and other continuously generated data.
They offer a host of benefits over other indexing strategies:

* *Reduced number of fields per index*: Indices only need to store a specific subset of your
data–meaning no more indices with hundreds of thousands of fields.
This leads to better space efficiency and faster queries.
As an added bonus, only relevant fields are shown in Discover.

* *More granular data control*: For example, filesystem, load, cpu, network, and process metrics are sent
to different indices–each potentially with its own rollover, retention, and security permissions.

* *Flexible*: Use the custom namespace component to divide and organize data in a way that
makes sense to your use case or company.

* *Fewer ingest permissions required*: Data ingestion only requires permissions to append data.

[discrete]
[[data-streams-naming-scheme]]
== Data stream naming scheme

{agent} uses the Elastic data stream naming scheme to name data streams.
The naming scheme splits data into different streams based on the following components:

`type`::
A generic `type` describing the data, such as `logs`, `metrics`, `traces`, or `synthetics`.
// Corresponds to the `data_stream.type` field.

`dataset`::
The `dataset` is defined by the integration and describes the ingested data and its structure for each index.
For example, you might have a dataset for process metrics with a field describing whether the process is running or not,
and another dataset for disk I/O metrics with a field describing the number of bytes read.

`namespace`::
A user-configurable arbitrary grouping, such as an environment (`dev`, `prod`, or `qa`),
a team, or a strategic business unit.
A `namespace` can be up to 100 bytes in length (multibyte characters will count toward this limit faster).
Using a namespace makes it easier to search the data from a given source by using index patterns, or to give users permissions to data by assigning an index pattern to user roles.
// Corresponds to the `data_stream.dataset` field.

The naming scheme separates each components with a `-` character:

[source,text]
--
<type>-<dataset>-<namespace>
--

For example, if you've set up the Nginx integration with a namespace of `prod`,
{agent} uses the `logs` type, `nginx.access` dataset, and `prod` namespace to store data in the following data stream:

[source,text]
--
logs-nginx.access-prod
--

Alternatively, if you use the APM integration with a namespace of `dev`,
{agent} stores data in the following data stream:

[source,text]
--
traces-apm-dev
--

All data streams, and the pre-built dashboards that they ship with,
are viewable on the Fleet Data Streams page:

[role="screenshot"]
image::images/kibana-fleet-datastreams.png[Data streams page]

TIP: If you're familiar with the concept of indices, you can think of each data stream as a separate index in {es}.
Under the hood though, things are a bit more complex.
All of the juicy details are available in {ref}/data-streams.html[{es} Data streams].

[discrete]
[[data-streams-index-pattern]]
== Index patterns

When searching your data in {kib}, you can use an {kibana-ref}/index-patterns.html[index pattern]
to search across all or some of your data streams.

[discrete]
[[data-streams-index-templates]]
== Index templates

An index template is a way to tell {es} how to configure an index when it is created.
For data streams, the index template configures the stream's backing indices as they are created.

{es} provides the following built-in, ECS based templates: `logs-*-*`, `metrics-*-*`, and `synthetics-*-*`.
{agent} integrations can also provide dataset-specific index templates, like `logs-nginx.access-*`.
These templates are loaded when the integration is installed, and are used to configure the integration's data streams.

[discrete]
[[data-streams-ilm]]
== Configure an index lifecycle management (ILM) policy

Use the {ref}/getting-started-index-lifecycle-management.html[index lifecycle
management] (ILM) feature in {es} to manage your {agent} data stream indices as they age.
For example, create a new index after a certain period of time,
or delete stale indices to enforce data retention standards.

{agent} uses ILM policies built-in to {es} to manage backing indices for its data streams.
See the {ref}/example-using-index-lifecycle-policy.html[Customize built-in ILM policies] tutorial
to learn how to customize these policies based on your performance, resilience, and retention requirements.

To instead create a new ILM policy, in {kib},
go to **Stack Management** > **Index Lifecycle Policies**. Click **Create policy**.
Define data tiers for your data, and any associated actions,
like a rollover, freeze, or shrink.
See {ref}/set-up-lifecycle-policy.html[configure a lifecycle policy] for more information.

[discrete]
[[data-streams-ilm-tutorial]]
=== Tutorial: Customize data retention for integrations

Each integration may have one or many associated data streams, each with an associated ILM policy.
By default, these data streams use an ILM policy that matches their data type, for example,
`metrics-system.logs-*` data streams use the metrics ILM policy as defined in the `metrics-system.logs` index template.
This tutorial shows you how to override this default config and apply your own ILM policy to an integrations’ data stream.

Scenario

You have {agent}s collecting metrics in 2 environments, for which you use the namespaces "development" and "production". In the production namespace you want to apply the in-built 90-days-default ILM policy to the `system.network` data stream, so that it is deleted after 90 days.

Prerequisites

To complete this tutorial, you'll need:
An {es} cluster set up for the ILM policy you wish to use
A host with Elastic Agent installed and configured to send logs to your {es} cluster.

[discrete]
[[data-streams-ilm-one]]
==== Step 1: View data streams

First you may wish to view the associated data streams and index templates with a given integration.

First you can use the filters in the data streams view in Fleet to see which data streams the system package uses in production.
// ss

You can see the `system.network` data stream.
To view further information about this data stream you can go to **Stack Management** > **Index Management** > **Data Streams** tab and search for the data stream,
note that the data stream uses the data stream naming scheme and starts with it's type, `metrics-` in this case.
// ss

Clicking the data stream allows you to see the index template used by the data stream `metrics-system.network-production`,
click the index template to view the template details.
// ss

[discrete]
[[data-streams-ilm-two]]
==== Step 2: Create a component template

In order for your changes to continue to be applied in future versions,
you must put any custom index settings you wish to apply into a component template.
The component template must follow the data stream naming scheme,
as well as starting with “.” and end with `@custom: .<type>-<dataset>-<namespace>@custom>`.
For example, if you are setting custom index settings for `system.network` in production,
the component template name would be ``.metrics-system.network-production@custom`.

. Navigate to **Stack Management** > **Index Management** > **Component Templates**
. Create a component template
.. Set the name as described above
.. Set the ILM policy name under index setting `lifecycle.name` key `("lifecycle" : { "name" : "90-days-default"})`
.. Create the template
// ss

[discrete]
[[data-streams-ilm-three]]
==== Step 3: Clone and modify the existing template

Now that you have created a component template,
we can create an index template to apply the settings to the data stream in the desired namespace.
The easiest way of doing this is to duplicate the existing index template which the integration uses across all namespaces.

. Navigate to **Stack Management** > **Index Management** > **Index Templates** (tab)
. Find the index template you wish to clone,
this will have the type and dataset in the name but not the namespace, `metrics-system.network` in this case.
. In order to set a custom ILM policy for the production namespace,
we must clone the index template and use an index pattern which includes the namespace,
to clone the index template click **manage** > **clone**
// ss

WARNING: When duplicating the template it's important to keep the managed properties or you may have problems upgrading in the future.

. We must now change some settings for our new index template
.. set the name to `metrics-system.network-production`
.. set the index pattern to `metrics-system.network-production*``
.. set the priority to 250
.. Under component templates, specify add the component template you created earlier.
To ensure your namespace specific settings are applied over other custom settings,
the template should be added below the existing `@custom` template.
// ss

. Create the index template

[discrete]
[[data-streams-ilm-four]]
==== Step 4: Rollover the data stream (optional)

We can now see that the data stream is using our new index template and ilm_policy by showing it in dev tools

[source,bash]
----
GET /_data_stream/metrics-system.network-production
----

The ILM policy will only take effect for new indices,
therefore we must either wait for a rollover (after 30 days or once the index size reaches 50GB usually by default)
or force a rollover using the rollover API

[source,bash]
----
POST /metrics-system.network-production/_rollover/
----
