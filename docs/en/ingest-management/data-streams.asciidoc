[[data-streams]]
= Data streams

{agent} uses data streams to store time series data across multiple indices
while giving you a single named resource for requests.
Data streams are well-suited for logs, metrics, traces, and other continuously generated data.
They offer a host of benefits over other indexing strategies:

* *Reduced number of fields per index*: Indices only need to store a specific subset of your
data–meaning no more indices with hundreds of thousands of fields.
This leads to better space efficiency and faster queries.
As an added bonus, only relevant fields are shown in Discover.

* *More granular data control*: For example, file system, load, CPU, network, and process metrics are sent
to different indices–each potentially with its own rollover, retention, and security permissions.

* *Flexible*: Use the custom namespace component to divide and organize data in a way that
makes sense to your use case or company.

* *Fewer ingest permissions required*: Data ingestion only requires permissions to append data.

[discrete]
[[data-streams-naming-scheme]]
== Data stream naming scheme

{agent} uses the Elastic data stream naming scheme to name data streams.
The naming scheme splits data into different streams based on the following components:

`type`::
A generic `type` describing the data, such as `logs`, `metrics`, `traces`, or `synthetics`.
// Corresponds to the `data_stream.type` field.

`dataset`::
The `dataset` is defined by the integration and describes the ingested data and its structure for each index.
For example, you might have a dataset for process metrics with a field describing whether the process is running or not,
and another dataset for disk I/O metrics with a field describing the number of bytes read.

`namespace`::
A user-configurable arbitrary grouping, such as an environment (`dev`, `prod`, or `qa`),
a team, or a strategic business unit.
A `namespace` can be up to 100 bytes in length (multibyte characters will count toward this limit faster).
Using a namespace makes it easier to search data from a given source by using a matching pattern.
You can also use matching patterns to give users access to data when creating user roles.
// Corresponds to the `data_stream.dataset` field.

The naming scheme separates each components with a `-` character:

[source,text]
--
<type>-<dataset>-<namespace>
--

For example, if you've set up the Nginx integration with a namespace of `prod`,
{agent} uses the `logs` type, `nginx.access` dataset, and `prod` namespace to store data in the following data stream:

[source,text]
--
logs-nginx.access-prod
--

Alternatively, if you use the APM integration with a namespace of `dev`,
{agent} stores data in the following data stream:

[source,text]
--
traces-apm-dev
--

All data streams, and the pre-built dashboards that they ship with,
are viewable on the {fleet} Data Streams page:

[role="screenshot"]
image::images/kibana-fleet-datastreams.png[Data streams page]

TIP: If you're familiar with the concept of indices, you can think of each data stream as a separate index in {es}.
Under the hood though, things are a bit more complex.
All of the juicy details are available in {ref}/data-streams.html[{es} Data streams].

[discrete]
[[data-streams-data-view]]
== {data-sources-cap}

When searching your data in {kib}, you can use a {kibana-ref}/data-views.html[{data-source}]
to search across all or some of your data streams.

[discrete]
[[data-streams-index-templates]]
== Index templates

An index template is a way to tell {es} how to configure an index when it is created.
For data streams, the index template configures the stream's backing indices as they are created.

{es} provides the following built-in, ECS based templates: `logs-*-*`, `metrics-*-*`, and `synthetics-*-*`.
{agent} integrations can also provide dataset-specific index templates, like `logs-nginx.access-*`.
These templates are loaded when the integration is installed, and are used to configure the integration's data streams.

[discrete]
[[data-streams-ilm]]
== {ilm} ({ilm-init})

Use the {ref}/index-lifecycle-management.html[index lifecycle
management] ({ilm-init}) feature in {es} to manage your {agent} data stream indices as they age.
For example, create a new index after a certain period of time,
or delete stale indices to enforce data retention standards.

Installed integrations may have one or many associated data streams--each with an associated {ilm-init} policy.
By default, these data streams use an {ilm-init} policy that matches their data type.
For example, the data stream `metrics-system.logs-*`,
uses the metrics {ilm-init} policy as defined in the `metrics-system.logs` index template.

Want to customize your index lifecycle management? See <<data-streams-ilm-tutorial>>.

[discrete]
[[data-streams-pipelines]]
== Ingest pipelines

{agent} integration data streams ship with a default {ref}/ingest.html[ingest pipeline]
that preprocesses and enriches data before indexing.
The default pipeline should not be directly edited as changes can easily break the functionality of the integration.

Starting in version 8.4, all default ingest pipelines call a non-existent and non-versioned "`@custom`" ingest pipeline.
If left uncreated, this pipeline has no effect on your data. However, if added to a data stream and customized,
this pipeline can be used for custom data processing, adding fields, sanitizing data, and more.

The full name of the `@custom` pipeline follows the following pattern: `<type>-<dataset>@custom`.
The `@custom` pipeline can directly contain processors or you can use the
pipeline processor to call other pipelines that can be shared across multiple data streams or integrations.
The `@custom` pipeline will persist across all version upgrades.

See <<data-streams-pipeline-tutorial>> to get started.

[[data-streams-ilm-tutorial]]
== Tutorial: Customize data retention policies

This tutorial explains how to apply a custom {ilm-init} policy to an integration's data stream.

**Scenario:** You have {agent}s collecting system metrics with the System integration in two environments--one with the namespace `development`, and one with `production`.

**Goal:** Customize the {ilm-init} policy for the `system.network` data stream in the `production` namespace.
Specifically, apply the built-in `90-days-default` {ilm-init} policy so that data is deleted after 90 days.

[discrete]
[[data-streams-ilm-one]]
=== Step 1: View data streams

The **Data Streams** view in {kib} shows you the data streams,
index templates, and {ilm-init} policies associated with a given integration.

. Navigate to **{stack-manage-app}** > **Index Management** > **Data Streams**.
. Search for `system` to see all data streams associated with the System integration.
. Select the `metrics-system.network-{namespace}` data stream to view its associated index template and {ilm-init} policy.
As you can see, the data stream follows the <<data-streams-naming-scheme>> and starts with its type, `metrics-`.
+
[role="screenshot"]
image::images/data-stream-info.png[Data streams info]

[discrete]
[[data-streams-ilm-two]]
=== Step 2: Create a component template

For your changes to continue to be applied in future versions,
you must put all custom index settings into a component template.
The component template must follow the data stream naming scheme,
and end with `@custom`:

[source,text]
----
<type>-<dataset>-<namespace>@custom
----

For example, to create custom index settings for the `system.network` data stream with a namespace of `production`,
the component template name would be:

[source,text]
----
metrics-system.network-production@custom
----

. Navigate to **{stack-manage-app}** > **Index Management** > **Component Templates**
. Click **Create component template**.
. Use the template above to set the name--in this case, `metrics-system.network-production@custom`. Click **Next**.
. Under **Index settings**, set the {ilm-init} policy name under the `lifecycle.name` key:
+
[source,json]
----
{
  "lifecycle": {
    "name": "90-days-default"
  }
}
----
. Continue to **Review** and ensure your request looks similar to the image below.
If it does, click **Create component template**.
+
[role="screenshot"]
image::images/create-component-template.png[Create component template]

[discrete]
[[data-streams-ilm-three]]
=== Step 3: Clone and modify the existing index template

Now that you've created a component template,
you need to create an index template to apply the changes to the correct data stream.
The easiest way to do this is to duplicate and modify the integration's existing index template.

WARNING: When duplicating the index template, do not change or remove any managed properties. This may result in problems when upgrading.

. Navigate to **{stack-manage-app}** > **Index Management** > **Index Templates**.
. Find the index template you want to clone. The index template will have the `<type>` and `<dataset>` in its name,
but not the `<namespace>`. In this case, it's `metrics-system.network`.
. Select **Actions** > **Clone**.
. Set the name of the new index template to `metrics-system.network-production`.
. Change the index pattern to include a namespace--in this case, `metrics-system.network-production*`.
This ensures the previously created component template is only applied to the `production` namespace.
. Set the priority to `250`. This ensures that the new index template takes precedence over other index templates that match the index pattern.
. Under **Component templates**, search for and add the component template created in the previous step.
To ensure your namespace-specific settings are applied over other custom settings,
the new template should be added below the existing `@custom` template.
. Create the index template.

[role="screenshot"]
image::images/create-index-template.png[Create index template]

[discrete]
[[data-streams-ilm-four]]
=== Step 4: Roll over the data stream (optional)

To confirm that the data stream is now using the new index template and {ilm-init} policy,
you can either repeat <<data-streams-ilm-one,step one>>, or navigate to **{dev-tools-app}** and run the following:

[source,bash]
----
GET /_data_stream/metrics-system.network-production <1>
----
<1> The name of the data stream we've been hacking on

The result should include the following:

[source,json]
----
{
  "data_streams" : [
    {
      ...
      "template" : "metrics-system.network-production", <1>
      "ilm_policy" : "90-days-default", <2>
      ...
    }
  ]
}
----
<1> The name of the custom index template created in step three
<2> The name of the {ilm-init} policy applied to the new component template in step two

New {ilm-init} policies only take effect when new indices are created,
so you either must wait for a rollover to occur (usually after 30 days or when the index size reaches 50 GB),
or force a rollover using the {ref}/indices-rollover-index.html[{es} rollover API]:

[source,bash]
----
POST /metrics-system.network-production/_rollover/
----

[[data-streams-pipeline-tutorial]]
== Tutorial: Transform data with custom ingest pipelines

This tutorial explains how to add a custom ingest pipeline to an Elastic Integration.
Custom pipelines can be used to add custom data processing,
like adding fields, obfuscate sensitive information, and more.

**Scenario:** You have {agent}s collecting system metrics with the System integration.

**Goal:** Add a custom ingest pipeline that adds a new field to each {es} document before it is indexed.

[discrete]
[[data-streams-pipeline-one]]
=== Step 1: Create a custom ingest pipeline

Create a custom ingest pipeline that will be called by the default integration pipeline.
In this tutorial, we'll create a pipeline that adds a new field to our documents.

. In {kib}, navigate to **Stack Management** -> **Ingest Pipelines** -> **Create pipeline** -> **New pipeline**.

. Name your pipeline. We'll call this one, `add_field`.

. Select **Add a processor**. Fill out the following information:
+
** Processor: "Set"
** Field: `test`
** Value: `true`
+
The {ref}/set-processor.html[Set processor] sets a document field and associates it with the specified value.

. Click **Add**.

. Click **Create pipeline**.

[discrete]
[[data-streams-pipeline-two]]
=== Step 2: Apply your ingest pipeline

Add a custom pipeline to an integration by calling it from the default ingest pipeline.
The custom pipeline will run after the default pipeline but before the final pipeline.

[discrete]
==== Edit integration

Add a custom pipeline to an integration from the **Edit integration** workflow.
The integration must already be configured and installed before a custom pipeline can be added.
To enter this workflow, do the following:

. Navigate to **{fleet}**
. Select the relevant {agent} policy
. Search for the integration you want to edit
. Select **Actions** -> **Edit integration**

[discrete]
==== Select a data stream

Most integrations write to multiple data streams.
You'll need to add the custom pipeline to each data stream individually.

. Find the first data stream you wish to edit and select **Change defaults**.
For this tutorial, find the data stream configuration titled, **Collect metrics from System instances**.

. Scroll to **System CPU metrics** and under **Advanced options** select **Add custom pipeline**.
+
This will take you to the **Create pipeline** workflow in **Stack management**.

[discrete]
==== Add the pipeline

Add the pipeline you created in step one.

. Select **Add a processor**. Fill out the following information:
+
** Processor: "Pipeline"
** Pipeline name: "add_field"
** Value: `true`

. Click **Create pipeline** to return to the **Edit integration** page.

[discrete]
==== Roll over the data stream (optional)

For pipeline changes to take effect immediately, you must roll over the data stream.
If you do not, the changes will not take effect until the next scheduled roll over.
Select **Apply now and rollover**.

After the data stream rolls over, note the name of the custom ingest pipeline.
In this tutorial, it's `metrics-system.cpu@custom`.
The name follows the pattern `<type>-<dataset>@custom`:

* type: `metrics`
* dataset: `system.cpu`
* Custom ingest pipeline designation: `@custom`

[discrete]
==== Repeat

Add the custom ingest pipeline to any other data streams you wish to update.

[discrete]
[[data-streams-pipeline-three]]
=== Step 3: Test the ingest pipeline (optional)

Allow time for new data to be ingested before testing your pipeline.
In a new window, open {kib} and navigate to **{kib} Dev tools**.

Use an {ref}/query-dsl-exists-query.html[exists query] to ensure that the
new field, "test" is being applied to documents.

[source,console]
----
GET metrics-system.cpu-default/_search <1>
{
  "query": {
    "exists": {
      "field": "test" <2>
    }
  }
}
----
<1> The data stream to search. In this tutorial, we've edited the `metrics-system.cpu` type and dataset.
`default` is the default namespace.
Combining all three of these gives us a data stream name of `metrics-system.cpu-default`.
<2> The name of the field set in step one.

If your custom pipeline is working correctly, this query will return at least one document.

[discrete]
[[data-streams-pipeline-four]]
=== Step 4: Add custom mappings

Now that a new field is being set in your {es} documents, you'll want to assign a new mapping for that field.
Use the `@custom` component template to apply custom mappings to an integration data stream.

In the **Edit integration** workflow, do the following:

. Under **Advanced options** select the pencil icon to edit the `@custom` component template.

. Define the new field for your indexed documents. Select **Add field** and add the following information:
+
* Field name: `test`
* Field type: `Boolean`

. Click **Add field**.

. Click **Review** to fast-forward to the review step and click **Save component template** to return to the **Edit integration** workflow.

. For changes to take effect immediately, select **Apply now and rollover**.

[discrete]
[[data-streams-pipeline-five]]
=== Step 5: Test the custom mappings (optional)

Allow time for new data to be ingested before testing your mappings.
In a new window, open {kib} and navigate to **{kib} Dev tools**.

Use the {ref}/indices-get-field-mapping.html[Get field mapping API] to ensure that the
custom mapping has been applied.

[source,console]
----
GET metrics-system.cpu-default/_mapping/field/test <1>
----
<1> The data stream to search. In this tutorial, we've edited the `metrics-system.cpu` type and dataset.
`default` is the default namespace.
Combining all three of these gives us a data stream name of `metrics-system.cpu-default`.

The result should include `type: "boolean"` for the specified field.

[source,json]
----
".ds-metrics-system.cpu-default-2022.08.10-000002": {
  "mappings": {
    "test": {
      "full_name": "test",
      "mapping": {
        "test": {
          "type": "boolean"
        }
      }
    }
  }
}
----
