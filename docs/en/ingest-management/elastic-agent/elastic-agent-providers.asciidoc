[discrete]
[[providers]]
== Providers

Providers supply the key-value pairs that are used for variable substitution
and conditionals. Each provider's keys are automatically prefixed with the name
of the provider in the context of the {agent}.

For example, a provider named `foo` provides
`{"key1": "value1", "key2": "value2"}`, the key-value pairs are placed in
`{"foo" : {"key1": "value1", "key2": "value2"}}`. To reference the keys, use `{{foo.key1}}` and `{{foo.key2}}`.

[discrete]
=== Provider configuration

The provider configuration is specified under the top-level `providers`
key in the `elastic-agent.yml` configuration. All registered
providers are enabled by default. If a provider cannot connect, no mappings are produced.

The following example shows two providers (`local` and `local_dynamic`) that
supply custom keys:

[source,yaml]
----
providers:
  local:
    vars:
      foo: bar
  local_dynamic:
    vars:
      - item: key1
      - item: key2
----

Explicitly disable a provider by setting `enabled: false`. All providers
are prefixed without name collisions. The name of the provider is in the key in the configuration.

[source,yaml]
----
providers:
  docker:
    enabled: false
----

{agent} supports two broad types of providers: <<context-providers,context>> and
<<dynamic-providers,dynamic>>.

[discrete]
[[context-providers]]
=== Context providers

Context providers give the current context of the running {agent}, for
example, agent information (ID, version), host information (hostname, IP
addresses), and environment information (environment variables).

They can only provide a single key-value mapping. Think of them as singletons;
an update of a key-value mapping results in a re-evaluation of the entire
configuration. These providers are normally very static, but not
required. A value can change which results in re-evaluation. 

Context providers use the Elastic Common Schema (ECS) naming to ensure consistency and understanding throughout documentation and projects. 

{agent} supports the following context providers:

[discrete]
[[local-provider]]
==== Local

Provides custom keys to use as variables. For example:

[source,yaml]
----
providers:
  local:
    vars:
      foo: bar
----

[discrete]
[[agent-provider]]
==== Agent provider

Provides information about the {agent}. The available keys are:

|===
|Key |Type |Description

|`agent.id`
|`string`
|Current agent ID

|`agent.version`
|`object`
|Current agent version information object

|`agent.version.version`
|`string`
|Current agent version

|`agent.version.commit`
|`string`
|Version commit

|`agent.version.build_time`
|`date`
|Version build time

|`agent.version.snapshot`
|`boolean`
|Version is snapshot build
|===


[discrete]
[[host-provider]]
==== Host provider

Provides information about the current host. The available keys are:

|===
|Key |Type |Description

|`host.name`
|`string`
|Host name

|`host.platform`
|`string`
|Host platform

|`host.architecture`
|`string`
|Host architecture

|`host.ip[]`
|`[]string`
|Host IP addresses

|`host.mac[]`
|`[]string`
|Host MAC addresses
|===

[discrete]
[[env-provider]]
==== Env Provider

Provides access to the environment variables as key-value pairs.

For example, set the variable `foo`:

[source,shell]
----
foo=bar elastic-agent run
----

The environment variable can be referenced as `${env.foo}`.

[discrete]
[[kubernetes_secrets-provider]]
==== Kubernetes Secrets Provider

Provides access to the Kubernetes Secrets API.

The provider needs a `kubeconfig` file to establish connection to the Kubernetes API.
It can automatically reach the API if it's run in an inCluster environment ({agent} runs as pod).

[source,yaml]
----
providers.kubernetes_secrets:
  #kube_config: /Users/elastic-agent/.kube/config
----

Reference the Kubernetes Secrets variable as `${kubernetes_secrets.default.somesecret.value}`,
where `default` is the namespace of the Secret, `somesecret` is the name of the Secret and `value` the field
of the Secret to access.

If you run agent on Kubernetes, the proper rule in the `ClusterRole` is required to privide access to the {agent} pod in the Secrets API:

[source,yaml]
----
- apiGroups: [""]
  resources:
    - secrets
  verbs: ["get"]
----

CAUTION: The above rule will give permission to {agent} pod to access Kubernetes Secrets API.
Anyone who has access to the {agent} pod (`kubectl exec` for example) will also have
access to the Kubernetes Secrets API. This allows access to a specific secret, regardless of the namespace that it belongs to.
This option should be carefully considered.

[discrete]
[[kubernetes_leaderelection-provider]]
==== Kubernetes LeaderElection Provider

Provides the option to enable leaderelection between a set of {agent}s
running on Kubernetes. Only one {agent} at a time will be the holder of the leader
lock and based on this, configurations can be enabled with the condition
that the {agent} holds the leadership. This is useful in cases where the {agent} between a set of {agent}s collects cluster wide metrics for the Kubernetes cluster, such as the `kube-state-metrics` endpoint.

Provider needs a `kubeconfig` file to establish a connection to Kubernetes API.
It can automatically reach the API if it's running in an inCluster environment ({agent} runs as Pod).

[source,yaml]
----
providers.kubernetes_leaderelection:
  #kube_config: /Users/elastic-agent/.kube/config
  #leader_lease: agent-k8s-leader-lock
----

`kube_config`:: (Optional) Use the given config file as configuration for the Kubernetes
client. If kube_config is not set, KUBECONFIG environment variable will be
checked and will fall back to InCluster if it's not present.
`leader_lease`:: (Optional) Specify the name of the leader lease.
This is set to `elastic-agent-cluster-leader` by default.

The available key is:

|===
|Key |Type |Description

|`kubernetes_leaderelection.leader`
|`bool`
|The value of the leadership flag. This is set to `true` when the {agent} is the current leader, and is set to `false` otherwise.

|===

[discrete]
===== Enabling confgiurations only when on leadership

Use conditions based on the `kubernetes_leaderelection.leader` key to leverage the leaderelection provider and enable specific inputs only when the {agent} holds the leadership lock.
The below example enables the `state_container`
metricset only when the leadership lock is acquired:

[source,yaml]
----
- data_stream:
    dataset: kubernetes.state_container
    type: metrics
  metricsets:
    - state_container
  add_metadata: true
  hosts:
    - 'kube-state-metrics:8080'
  period: 10s
  condition: ${kubernetes_leaderelection.leader} == true
----

[discrete]
[[dynamic-providers]]
=== Dynamic Providers

Dynamic providers give an array of multiple key-value mappings. Each
key-value mapping is combined with the previous context provider's key and value
mapping which provides a new unique mapping that is used to generate a
configuration.

[discrete]
[[local-dynamic-provider]]
==== Local dynamic provider

Define multiple key-value pairs to generate multiple configurations.

For example, the following {agent} policy defines a local dynamic provider that
defines three values for `item`:

[source,yaml]
----
inputs:
 - type: logfile
   streams:
     - paths: "/var/${local_dynamic.my_var}/app.log"

providers:
  local_dynamic:
    items:
      - vars:
          my_var: key1
      - vars:
          my_var: key2
      - vars:
          my_var: key3
----

The configuration generated by this policy looks like:

[source,yaml]
----
inputs:
 - type: logfile
   streams:
     - paths: "/var/key1/app.log"
 - type: logfile
   streams:
     - paths: "/var/key2/app.log"
 - type: logfile
   streams:
   - paths: "/var/key3/app.log"
----

[discrete]
[[docker-provider]]
==== Docker Provider

Provides inventory information from Docker. The available keys are:


|===
|Key |Type |Description

|`docker.id`
|`string`
|ID of the container

|`docker.cmd`
|`string`
|Arg path of container

|`docker.name`
|`string`
|Name of the container

|`docker.image`
|`string`
|Image of the container

|`docker.labels`
|`string`
|Labels of the container

|`docker.ports`
|`string`
|Ports of the container

|`docker.paths`
|`object`
|Object of paths for the container

|`docker.paths.log`
|`string`
|Log path of the container
|===

For example, the Docker provider provides the following inventory:

[source,json]
----
[
    {
       "id": "1",
       "mapping:": {"id": "1", "paths": {"log": "/var/log/containers/1.log"}},
       "processors": {"add_fields": {"container.name": "my-container"}}
    },
    {
        "id": "2",
        "mapping": {"id": "2", "paths": {"log": "/var/log/containers/2.log"}},
        "processors": {"add_fields": {"container.name": "other-container"}}
    }
]
----

{agent} automatically prefixes the result with `docker`:


[source,json]
---
[
    {"docker": {"id": "1", "paths": {"log": "/var/log/containers/1.log"}}},
    {"docker": {"id": "2", "paths": {"log": "/var/log/containers/2.log"}},
]
---

To set the log path dynamically in the configuration, use a variable in the
{agent} policy to return path information from the provider:

[source,yaml]
----
inputs:
  - type: logfile
    path: "${docker.paths.log}"
----

The policy generated by this configuration looks like:

[source,yaml]
----
inputs:
  - type: logfile
    path: "/var/log/containers/1.log"
    processors:
      - add_fields:
          container.name: my-container
  - type: logfile
    path: "/var/log/containers/2.log"
    processors:
      - add_fields:
          container.name: other-container
----

[discrete]
[[kubernetes-provider]]
==== Kubernetes Provider

Provides inventory information from Kubernetes. The available keys are:


|===
|Key |Type |Description

|`kubernetes.namespace`
|`string`
|Namespace of the Pod

|`kubernetes.pod.name`
|`string`
|Name of the Pod

|`kubernetes.pod.uuid`
|`string`
|UUID of the Pod

|`kubernetes.pod.ip`
|`string`
|IP of the Pod

|`kubernetes.pod.labels`
|`object`
|Object of labels of the Pod

|`kubernetes.container.name`
|`string`
|Name of the container

|`kubernetes.container.runtime`
|`string`
|Runtime of the container

|`kubernetes.container.id`
|`string`
|ID of the container

|`kubernetes.container.image`
|`string`
|Image of the container
|===

Fox example, if the Kubernetes provider provides the following inventory:

[source,json]
----
[
    {
       "id": "1",
       "mapping:": {"namespace": "kube-system", "pod": {"name": "kube-controllermanger"}},
       "processors": {"add_fields": {"container.name": "my-container"}}
    },
    {
        "id": "2",
        "mapping:": {"namespace": "kube-system", "pod": {"name": "kube-scheduler"}},
        "processors": {"add_fields": {"kuberentes.namespace": "kube-system", "kubernetes.pod": {"name": "kube-scheduler"}}
    }
]
----

{agent} automatically prefixes the result with `kuberentes`:


[source,json]
----
[
    {"kubernetes": {"id": "1", "namespace": "kube-system", "pod": {"name": "kube-controllermanger"}},
    {"kubernetes": {"id": "2", "namespace": "kube-system", "pod": {"name": "kube-scheduler"}},
]
----

[discrete]
===== Provider configuration

[source,yaml]
----
providers.kubernetes:
  node: ${NODE_NAME}
  scope: node
  #kube_config: /Users/elastic-agent/.kube/config
  #sync_period: 600
  #cleanup_timeout: 60
----

`node`:: (Optional) Specify the node to scope {agent} to in case it
cannot be accurately detected when running {agent} in host network
mode.
`cleanup_timeout`:: (Optional) Specify the time of inactivity before stopping the
running configuration for a container. This is `60s` by default.
`sync_period`:: (Optional) Specify the timeout for listing historical resources.
`kube_config`:: (Optional) Use the given config file as configuration for Kubernetes
client. If kube_config is not set, the KUBECONFIG environment variable will be
checked and will fall back to InCluster if not present.
`scope`:: (Optional) Specify the level for autodiscover. `scope` can
either take `node` or `cluster` as values. `node` scope allows discovery of resources in
the specified node. `cluster` scope allows cluster wide discovery. Only `pod` and `node` resources
can be discovered at node scope.

[discrete]
===== Autodiscover target Pods

To set the target host dynamically only for a targeted Pod based on its labels, use a variable in the
{agent} policy to return path information from the provider:

[source,yaml]
----
- data_stream:
      dataset: kubernetes.scheduler
      type: metrics
  metricsets:
    - scheduler
  hosts:
    - '${kubernetes.pod.ip}:10251'
  period: 10s
  condition: ${kubernetes.pod.labels.component} == 'kube-scheduler'
----

The policy generated by this configuration looks like:

[source,yaml]
----
- hosts:
  - 172.18.0.4:10251
  metricsets:
  - scheduler
  module: kubernetes
  period: 10s
  processors:
  - add_fields:
    fields:
      namespace: kube-system
      pod:
        ip: 172.18.0.4
        labels:
          component: kube-scheduler
          tier: control-plane
        name: kube-scheduler-kind-control-plane
        uid: 6da04645-04b4-4cb2-b203-2ad58abc6cdf
    target: kubernetes
----

To set the log path of Pods dynamically in the configuration, use a variable in the
{agent} policy to return path information from the provider:

[source,yaml]
----
streams:
  - data_stream:
      dataset: generic
    symlinks: true
    paths:
      - /var/log/containers/*${kubernetes.container.id}.log
----

The policy generated by this configuration looks like:

[source,yaml]
----
- paths:
  - /var/log/containers/*c957652eca53594ce496c7b237d19f05be339ebfe281b99ce1c0a0401e48ce3a.log
  processors:
  - add_fields:
    fields:
      container:
        id: c957652eca53594ce496c7b237d19f05be339ebfe281b99ce1c0a0401e48ce3a
        image: k8s.gcr.io/kube-apiserver:v1.18.2
        name: kube-apiserver
        runtime: containerd
      namespace: kube-system
      pod:
        ip: 172.18.0.4
        labels:
          component: kube-apiserver
          tier: control-plane
        name: kube-apiserver-kind-control-plane
        uid: f8743f90-50a4-4ef8-9fe9-78c245eb8bf3
    target: kubernetes
  symlinks: true
----
