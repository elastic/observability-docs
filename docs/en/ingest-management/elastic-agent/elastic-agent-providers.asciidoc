[discrete]
[[providers]]
== Providers

Providers supply the key/values pairs that are used for variable substitution
and conditionals. Each provider's keys are automatically prefixed with the name
of the provider in the context of the {agent}.

For example, if a provider named `foo` provides
`{"key1": "value1", "key2": "value2"}`, the key/value pairs are placed in
`{"foo" : {"key1": "value1", "key2": "value2"}}`. To reference the keys, you
would use `{{foo.key1}}` and `{{foo.key2}}`.

[discrete]
=== Provider configuration

The provider configuration is specified under the top-level `providers`
key in the `elastic-agent.yml` configuration. By default, all registered
providers are enabled. If a provider cannot connect, it produces no mappings.

The following example shows two providers, `local` and `local_dynamic`, that
supply custom keys:

[source,yaml]
----
providers:
  local:
    vars:
      foo: bar
  local_dynamic:
    vars:
      - item: key1
      - item: key2
----

To explicitly disable a provider, set `enabled: false`. Because all providers
are prefixed and have no name collisions, the name of the provider is the key in
the configuration.

[source,yaml]
----
providers:
  docker:
    enabled: false
----

{agent} supports two broad types of providers: <<context-providers,context>> and
<<dynamic-providers,dynamic>>.

[discrete]
[[context-providers]]
=== Context providers

Context providers provide the current context of the running {agent}, for
example, agent information (id, version), host information (hostname, IP
addresses), and environment information (environment variables).

They can only provide a single key/value mapping. Think of them as singletons;
an update of a key/value mapping will result in a re-evaluation of the entire
configuration. These providers are normally very static, but that's not
required. It is possible for a value to change resulting in re-evaluation.

Context providers use ECS naming when possible to ensure that documentation and
understanding across projects is the same.

{agent} supports the following context providers:

[discrete]
[[local-provider]]
==== Local

Provides custom keys to use as variables. For example:

[source,yaml]
----
providers:
  local:
    vars:
      foo: bar
----

[discrete]
[[agent-provider]]
==== Agent provider

Provides information about the {agent}. The available keys are:

|===
|Key |Type |Description

|`agent.id`
|`string`
|Current agent ID

|`agent.version`
|`object`
|Current agent version information object

|`agent.version.version`
|`string`
|Current agent version

|`agent.version.commit`
|`string`
|Version commit

|`agent.version.build_time`
|`date`
|Version build time

|`agent.version.snapshot`
|`boolean`
|Version is snapshot build
|===


[discrete]
[[host-provider]]
==== Host provider

Provides information about the current host. The available keys are:

|===
|Key |Type |Description

|`host.name`
|`string`
|Host name

|`host.platform`
|`string`
|Host platform

|`host.architecture`
|`string`
|Host architecture

|`host.ip[]`
|`[]string`
|Host IP addresses

|`host.mac[]`
|`[]string`
|Host MAC addresses
|===

[discrete]
[[env-provider]]
==== Env Provider

Provides access to the environment variables as key/values.

For example, if you set the variable foo:

[source,shell]
----
foo=bar elastic-agent run
----

You can reference the environment variable as `${env.foo}`.

[discrete]
[[kubernetes_secrets-provider]]
==== Kubernetes Secrets Provider

Provides access to the Kubernetes Secrets API.

Provider needs a `kubeconfig` file so as to establish connection to Kubernetes API,
or it can automatically reach the API if it runs in an inCluster environment (Agent runs as Pod).

[source,yaml]
----
providers.kubernetes_secrets:
  #kube_config: /Users/elastic-agent/.kube/config
----

You can reference the Kubernetes Secrets variable as `${kubernetes_secrets.default.somesecret.value}`,
where `default` is the namespace of the Secret, `somesecret` is the name of the Secret and `value` the field
of the Secret to access.

If you run Agent on Kubernetes the proper rule in the `ClusterRole` is required so as Agent Pod to have access
to Secrets API:

[source,yaml]
----
- apiGroups: [""]
  resources:
    - secrets
  verbs: ["get"]
----

CAUTION: The above rule will give permission to Agent Pod to access Kubernetes Secrets API.
This means that anyone who have access to Agent Pod (`kubectl exec` for example) will be able to
access Kubernetes Secrets API and get a specific secret no matter which namespace it belongs to.
In this, this option should be carefully considered.

[discrete]
[[kubernetes_leaderelection-provider]]
==== Kubernetes LeaderElection Provider

Provides the option to enable leaderelection between a set of Agents
running on Kubernetes. Only one Agent at a time will be the holder of the leader
lock and based on this, configurations can be enabled with the condition
that the Agent holds the leadership. This can be useful in cases where we want
only Agent between a set of Agents to collect cluster wide metrics for the
Kubernetes cluster like from `kube-state-metrics` endpoint.

Provider needs a `kubeconfig` file so as to establish connection to Kubernetes API,
or it can automatically reach the API if it runs in an inCluster environment (Agent runs as Pod).

[source,yaml]
----
providers.kubernetes_leaderelection:
  #kube_config: /Users/elastic-agent/.kube/config
  #leader_lease: agent-k8s-leader-lock
----

`kube_config`:: (Optional) Use given config file as configuration for Kubernetes
client. If kube_config is not set, KUBECONFIG environment variable will be
checked and if not present it will fall back to InCluster.
`leader_lease`:: (Optional) Specify the name of the leader lease.
By default it is `elastic-agent-cluster-leader`.

The available key is:

|===
|Key |Type |Description

|`kubernetes_leaderelection.leader`
|`bool`
|The value of the leadership flags. True when the Agents holds the leader lock.

|===

==== Enabling confgiurations only when on leadership

In order to leverage leaderelection provider and enable
specific inputs only when Agent holds the leadership lock, users
can use conditions based on `kubernetes_leaderelection.leader` key.
Below we provide an example that will enable `state_container`
metricset only when the leadership lock is acquired:

[source,yaml]
----
- data_stream:
    dataset: kubernetes.state_container
    type: metrics
  metricsets:
    - state_container
  add_metadata: true
  hosts:
    - 'kube-state-metrics:8080'
  period: 10s
  condition: ${kubernetes_leaderelection.leader} == true
----

[discrete]
[[dynamic-providers]]
=== Dynamic Providers

Dynamic providers provide an array of multiple key/value mappings. Each
key/value mapping is combined with the previous context provider's key/value
mapping to provide a new unique key/value mapping that is used to generate a
configuration.

[discrete]
[[local-dynamic-provider]]
==== Local dynamic provider

Allows you to define multiple key/values to generate multiple configurations.

For example, the following agent policy defines a local dynamic provider that
defines 3 values for `item`:

[source,yaml]
----
inputs:
 - type: logfile
   streams:
     - paths: "/var/${local_dynamic.my_var}/app.log"

providers:
  local_dynamic:
    items:
      - vars:
          my_var: key1
      - vars:
          my_var: key2
      - vars:
          my_var: key3
----

The configuration generated by this policy looks like:

[source,yaml]
----
inputs:
 - type: logfile
   streams:
     - paths: "/var/key1/app.log"
 - type: logfile
   streams:
     - paths: "/var/key2/app.log"
 - type: logfile
   streams:
   - paths: "/var/key3/app.log"
----

[discrete]
[[docker-provider]]
==== Docker Provider

Provides inventory information from Docker. The available keys are:


|===
|Key |Type |Description

|`docker.id`
|`string`
|ID of the container

|`docker.cmd`
|`string`
|Arg path of container

|`docker.name`
|`string`
|Name of the container

|`docker.image`
|`string`
|Image of the container

|`docker.labels`
|`string`
|Labels of the container

|`docker.ports`
|`string`
|Ports of the container

|`docker.paths`
|`object`
|Object of paths for the container

|`docker.paths.log`
|`string`
|Log path of the container
|===

Imagine that the Docker provider provides the following inventory:

[source,json]
----
[
    {
       "id": "1",
       "mapping:": {"id": "1", "paths": {"log": "/var/log/containers/1.log"}},
       "processors": {"add_fields": {"container.name": "my-container"}}
    },
    {
        "id": "2",
        "mapping": {"id": "2", "paths": {"log": "/var/log/containers/2.log"}},
        "processors": {"add_fields": {"container.name": "other-container"}}
    }
]
----

{agent} automatically prefixes the result with `docker`:


[source,json]
---
[
    {"docker": {"id": "1", "paths": {"log": "/var/log/containers/1.log"}}},
    {"docker": {"id": "2", "paths": {"log": "/var/log/containers/2.log"}},
]
---

To set the log path dynamically in the configuration, use a variable in the
{agent} policy to return path information from the provider:

[source,yaml]
----
inputs:
  - type: logfile
    path: "${docker.paths.log}"
----

The policy generated by this configuration looks like:

[source,yaml]
----
inputs:
  - type: logfile
    path: "/var/log/containers/1.log"
    processors:
      - add_fields:
          container.name: my-container
  - type: logfile
    path: "/var/log/containers/2.log"
    processors:
      - add_fields:
          container.name: other-container
----

[discrete]
[[kubernetes-provider]]
==== Kubernetes Provider

Provides inventory information from Kubernetes. The available keys are:


|===
|Key |Type |Description

|`kubernetes.namespace`
|`string`
|Namespace of the Pod

|`kubernetes.pod.name`
|`string`
|Name of the Pod

|`kubernetes.pod.uuid`
|`string`
|UUID of the Pod

|`kubernetes.pod.ip`
|`string`
|IP of the Pod

|`kubernetes.pod.labels`
|`object`
|Object of labels of the Pod

|`kubernetes.container.name`
|`string`
|Name of the container

|`kubernetes.container.runtime`
|`string`
|Runtime of the container

|`kubernetes.container.id`
|`string`
|ID of the container

|`kubernetes.container.image`
|`string`
|Image of the container
|===

Imagine that the Kubernetes provider provides the following inventory:

[source,json]
----
[
    {
       "id": "1",
       "mapping:": {"namespace": "kube-system", "pod": {"name": "kube-controllermanger"}},
       "processors": {"add_fields": {"container.name": "my-container"}}
    },
    {
        "id": "2",
        "mapping:": {"namespace": "kube-system", "pod": {"name": "kube-scheduler"}},
        "processors": {"add_fields": {"kuberentes.namespace": "kube-system", "kubernetes.pod": {"name": "kube-scheduler"}}
    }
]
----

{agent} automatically prefixes the result with `kuberentes`:


[source,json]
----
[
    {"kubernetes": {"id": "1", "namespace": "kube-system", "pod": {"name": "kube-controllermanger"}},
    {"kubernetes": {"id": "2", "namespace": "kube-system", "pod": {"name": "kube-scheduler"}},
]
----

[discrete]
===== Provider configuration

[source,yaml]
----
providers.kubernetes:
  node: ${NODE_NAME}
  scope: node
  #kube_config: /Users/elastic-agent/.kube/config
  #sync_period: 600
  #cleanup_timeout: 60
----

`node`:: (Optional) Specify the node to scope {agent} to in case it
cannot be accurately detected, as when running {agent} in host network
mode.
`cleanup_timeout`:: (Optional) Specify the time of inactivity before stopping the
running configuration for a container, 60s by default.
`sync_period`:: (Optional) Specify timeout for listing historical resources.
`kube_config`:: (Optional) Use given config file as configuration for Kubernetes
client. If kube_config is not set, KUBECONFIG environment variable will be
checked and if not present it will fall back to InCluster.
`scope`:: (Optional) Specify at what level autodiscover needs to be done at. `scope` can
either take `node` or `cluster` as values. `node` scope allows discovery of resources in
the specified node. `cluster` scope allows cluster wide discovery. Only `pod` and `node` resources
can be discovered at node scope.

[discrete]
===== Autodiscover target Pods

To set the target host dynamically only for a targeted Pod based on its labels, use a variable in the
{agent} policy to return path information from the provider:

[source,yaml]
----
- data_stream:
      dataset: kubernetes.scheduler
      type: metrics
  metricsets:
    - scheduler
  hosts:
    - '${kubernetes.pod.ip}:10251'
  period: 10s
  condition: ${kubernetes.pod.labels.component} == 'kube-scheduler'
----

The policy generated by this configuration looks like:

[source,yaml]
----
- hosts:
  - 172.18.0.4:10251
  metricsets:
  - scheduler
  module: kubernetes
  period: 10s
  processors:
  - add_fields:
    fields:
      namespace: kube-system
      pod:
        ip: 172.18.0.4
        labels:
          component: kube-scheduler
          tier: control-plane
        name: kube-scheduler-kind-control-plane
        uid: 6da04645-04b4-4cb2-b203-2ad58abc6cdf
    target: kubernetes
----

To set the log path of Pods dynamically in the configuration, use a variable in the
{agent} policy to return path information from the provider:

[source,yaml]
----
streams:
  - data_stream:
      dataset: generic
    symlinks: true
    paths:
      - /var/log/containers/*${kubernetes.container.id}.log
----

The policy generated by this configuration looks like:

[source,yaml]
----
- paths:
  - /var/log/containers/*c957652eca53594ce496c7b237d19f05be339ebfe281b99ce1c0a0401e48ce3a.log
  processors:
  - add_fields:
    fields:
      container:
        id: c957652eca53594ce496c7b237d19f05be339ebfe281b99ce1c0a0401e48ce3a
        image: k8s.gcr.io/kube-apiserver:v1.18.2
        name: kube-apiserver
        runtime: containerd
      namespace: kube-system
      pod:
        ip: 172.18.0.4
        labels:
          component: kube-apiserver
          tier: control-plane
        name: kube-apiserver-kind-control-plane
        uid: f8743f90-50a4-4ef8-9fe9-78c245eb8bf3
    target: kubernetes
  symlinks: true
----
