[discrete]
[[providers]]
== Providers

Providers supply the key/values pairs that are used for variable substitution
and conditionals. Each provider's keys are automatically prefixed with the name
of the provider in the context of the {agent}.

For example, if a provider named `foo` provides
`{"key1": "value1", "key2": "value2"}`, the key/value pairs are placed in
`{"foo" : {"key1": "value1", "key2": "value2"}}`. To reference the keys, you
would use `{{foo.key1}}` and `{{foo.key2}}`.

[discrete]
=== Provider configuration

The provider configuration is specified under the top-level `providers`
key in the `elastic-agent.yml` configuration. By default, all registered
providers are enabled. If a provider cannot connect, it produces no mappings.

The following example shows two providers, `local` and `local_dynamic`, that
supply custom keys:

[source,yaml]
----
providers:
  local:
    vars:
      foo: bar
  local_dynamic:
    vars:
      - item: key1
      - item: key2
----

To explicitly disable a provider, set `enabled: false`. Because all providers
are prefixed and have no name collisions, the name of the provider is the key in
the configuration.

[source,yaml]
----
providers:
  docker:
    enabled: false
----

{agent} supports two broad types of providers: <<context-providers,context>> and
<<dynamic-providers,dynamic>>.

[discrete]
[[context-providers]]
=== Context providers

Context providers provide the current context of the running {agent}, for
example, agent information (id, version), host information (hostname, IP
addresses), and environment information (environment variables).

They can only provide a single key/value mapping. Think of them as singletons;
an update of a key/value mapping will result in a re-evaluation of the entire
configuration. These providers are normally very static, but that's not
required. It is possible for a value to change resulting in re-evaluation.

Context providers use ECS naming when possible to ensure that documentation and
understanding across projects is the same.

{agent} supports the following context providers:

[discrete]
[[local-provider]]
==== Local

Provides custom keys to use as variables. For example:

[source,yaml]
----
providers:
  local:
    vars:
      foo: bar
----

[discrete]
[[agent-provider]]
==== Agent provider

Provides information about the {agent}. The available keys are:

|===
|Key |Type |Description

|`agent.id`
|`string`
|Current agent ID

|`agent.version`
|`object`
|Current agent version information object

|`agent.version.version`
|`string`
|Current agent version

|`agent.version.commit`
|`string`
|Version commit

|`agent.version.build_time`
|`date`
|Version build time

|`agent.version.snapshot`
|`boolean`
|Version is snapshot build
|===


[discrete]
[[host-provider]]
==== Host provider

Provides information about the current host. The available keys are:

|===
|Key |Type |Description

|`host.name`
|`string`
|Host name

|`host.platform`
|`string`
|Host platform

|`host.architecture`
|`string`
|Host architecture

|`host.ip[]`
|`[]string`
|Host IP addresses

|`host.mac[]`
|`[]string`
|Host MAC addresses
|===

[discrete]
[[env-provider]]
==== Env Provider

Provides access to the environment variables as key/values.

For example, if you set the variable foo:

[source,shell]
----
foo=bar elastic-agent run
----

You can reference the environment variable as `${env.foo}`.

[discrete]
[[kubernetes_secrets-provider]]
==== Kubernetes Secrets Provider

Provides access to the Kubernetes Secrets API.

Provider needs a `kubeconfig` file so as to establish connection to Kubernetes API,
or it can automatically reach the API if it runs in an inCluster environment (Agent runs as Pod).

[source,yaml]
----
providers.kubernetes_secrets:
  #kube_config: /Users/elastic-agent/.kube/config
----

You can reference the Kubernetes Secrets variable as `${kubernetes_secrets.default.somesecret.value}`,
where `default` is the namespace of the Secret, `somesecret` is the name of the Secret and `value` the field
of the Secret to access.

If you run Agent on Kubernetes the proper rule in the `ClusterRole` is required so as Agent Pod to have access
to Secrets API:

[source,yaml]
----
- apiGroups: [""]
  resources:
    - secrets
  verbs: ["get"]
----

CAUTION: The above rule will give permission to Agent Pod to access Kubernetes Secrets API.
This means that anyone who have access to Agent Pod (`kubectl exec` for example) will be able to
access Kubernetes Secrets API and get a specific secret no matter which namespace it belongs to.
In this, this option should be carefully considered.

[discrete]
[[dynamic-providers]]
=== Dynamic Providers

Dynamic providers provide an array of multiple key/value mappings. Each
key/value mapping is combined with the previous context provider's key/value
mapping to provide a new unique key/value mapping that is used to generate a
configuration.

[discrete]
[[local-dynamic-provider]]
==== Local dynamic provider

Allows you to define multiple key/values to generate multiple configurations.

For example, the following agent policy defines a local dynamic provider that
defines 3 values for `item`:

[source,yaml]
----
inputs:
 - type: logfile
   streams:
     - paths: "/var/${local_dynamic.my_var}/app.log"

providers:
  local_dynamic:
    items:
      - vars:
          my_var: key1
      - vars:
          my_var: key2
      - vars:
          my_var: key3
----

The configuration generated by this policy looks like:

[source,yaml]
----
inputs:
 - type: logfile
   streams:
     - paths: "/var/key1/app.log"
 - type: logfile
   streams:
     - paths: "/var/key2/app.log"
 - type: logfile
   streams:
   - paths: "/var/key3/app.log"
----

[discrete]
[[docker-provider]]
==== Docker Provider

Provides inventory information from Docker. The available keys are:


|===
|Key |Type |Description

|`docker.id`
|`string`
|ID of the container

|`docker.cmd`
|`string`
|Arg path of container

|`docker.name`
|`string`
|Name of the container

|`docker.image`
|`string`
|Image of the container

|`docker.labels`
|`string`
|Labels of the container

|`docker.ports`
|`string`
|Ports of the container

|`docker.paths`
|`object`
|Object of paths for the container

|`docker.paths.log`
|`string`
|Log path of the container
|===

Imagine that the Docker provider provides the following inventory:

[source,json]
----
[
    {
       "id": "1",
       "mapping:": {"id": "1", "paths": {"log": "/var/log/containers/1.log"}},
       "processors": {"add_fields": {"container.name": "my-container"}}
    },
    {
        "id": "2",
        "mapping": {"id": "2", "paths": {"log": "/var/log/containers/2.log"}},
        "processors": {"add_fields": {"container.name": "other-container"}}
    }
]
----

{agent} automatically prefixes the result with `docker`:


[source,json]
---
[
    {"docker": {"id": "1", "paths": {"log": "/var/log/containers/1.log"}}},
    {"docker": {"id": "2", "paths": {"log": "/var/log/containers/2.log"}},
]
---

To set the log path dynamically in the configuration, use a variable in the
{agent} policy to return path information from the provider:

[source,yaml]
----
inputs:
  - type: logfile
    path: "${docker.paths.log}"
----

The policy generated by this configuration looks like:

[source,yaml]
----
inputs:
  - type: logfile
    path: "/var/log/containers/1.log"
    processors:
      - add_fields:
          container.name: my-container
  - type: logfile
    path: "/var/log/containers/2.log"
    processors:
      - add_fields:
          container.name: other-container
----

[discrete]
[[kubernetes-provider]]
==== Kubernetes Provider

Provides inventory information from Kubernetes. The available keys are:


|===
|Key |Type |Description

|`kubernetes.namespace`
|`string`
|Namespace of the Pod

|`kubernetes.pod.name`
|`string`
|Name of the Pod

|`kubernetes.pod.uuid`
|`string`
|UUID of the Pod

|`kubernetes.pod.ip`
|`string`
|IP of the Pod

|`kubernetes.pod.labels`
|`object`
|Object of labels of the Pod

|`kubernetes.container.name`
|`string`
|Name of the container

|`kubernetes.container.runtime`
|`string`
|Runtime of the container

|`kubernetes.container.id`
|`string`
|ID of the container

|`kubernetes.container.image`
|`string`
|Image of the container
|===

Imagine that the Kubernetes provider provides the following inventory:

[source,json]
----
[
    {
       "id": "1",
       "mapping:": {"namespace": "kube-system", "pod": {"name": "kube-controllermanger"}},
       "processors": {"add_fields": {"container.name": "my-container"}}
    },
    {
        "id": "2",
        "mapping:": {"namespace": "kube-system", "pod": {"name": "kube-scheduler"}},
        "processors": {"add_fields": {"kuberentes.namespace": "kube-system", "kubernetes.pod": {"name": "kube-scheduler"}}
    }
]
----

{agent} automatically prefixes the result with `kuberentes`:


[source,json]
----
[
    {"kubernetes": {"id": "1", "namespace": "kube-system", "pod": {"name": "kube-controllermanger"}},
    {"kubernetes": {"id": "2", "namespace": "kube-system", "pod": {"name": "kube-scheduler"}},
]
----

[discrete]
===== Provider configuration

[source,yaml]
----
providers.kubernetes:
  node: ${NODE_NAME}
  scope: node
  #kube_config: /Users/elastic-agent/.kube/config
  #sync_period: 600
  #cleanup_timeout: 60
----

`node`:: (Optional) Specify the node to scope {agent} to in case it
cannot be accurately detected, as when running {agent} in host network
mode.
`cleanup_timeout`:: (Optional) Specify the time of inactivity before stopping the
running configuration for a container, 60s by default.
`sync_period`:: (Optional) Specify timeout for listing historical resources.
`kube_config`:: (Optional) Use given config file as configuration for Kubernetes
client. If kube_config is not set, KUBECONFIG environment variable will be
checked and if not present it will fall back to InCluster.
`scope`:: (Optional) Specify at what level autodiscover needs to be done at. `scope` can
either take `node` or `cluster` as values. `node` scope allows discovery of resources in
the specified node. `cluster` scope allows cluster wide discovery. Only `pod` and `node` resources
can be discovered at node scope.

[discrete]
===== Autodiscover target Pods

To set the target host dynamically only for a targeted Pod based on its labels, use a variable in the
{agent} policy to return path information from the provider:

[source,yaml]
----
- data_stream:
      dataset: kubernetes.scheduler
      type: metrics
  metricsets:
    - scheduler
  hosts:
    - '${kubernetes.pod.ip}:10251'
  period: 10s
  condition: ${kubernetes.pod.labels.component} == 'kube-scheduler'
----

The policy generated by this configuration looks like:

[source,yaml]
----
- hosts:
  - 172.18.0.4:10251
  metricsets:
  - scheduler
  module: kubernetes
  period: 10s
  processors:
  - add_fields:
    fields:
      namespace: kube-system
      pod:
        ip: 172.18.0.4
        labels:
          component: kube-scheduler
          tier: control-plane
        name: kube-scheduler-kind-control-plane
        uid: 6da04645-04b4-4cb2-b203-2ad58abc6cdf
    target: kubernetes
----

To set the log path of Pods dynamically in the configuration, use a variable in the
{agent} policy to return path information from the provider:

[source,yaml]
----
streams:
  - data_stream:
      dataset: generic
    symlinks: true
    paths:
      - /var/log/containers/*${kubernetes.container.id}.log
----

The policy generated by this configuration looks like:

[source,yaml]
----
- paths:
  - /var/log/containers/*c957652eca53594ce496c7b237d19f05be339ebfe281b99ce1c0a0401e48ce3a.log
  processors:
  - add_fields:
    fields:
      container:
        id: c957652eca53594ce496c7b237d19f05be339ebfe281b99ce1c0a0401e48ce3a
        image: k8s.gcr.io/kube-apiserver:v1.18.2
        name: kube-apiserver
        runtime: containerd
      namespace: kube-system
      pod:
        ip: 172.18.0.4
        labels:
          component: kube-apiserver
          tier: control-plane
        name: kube-apiserver-kind-control-plane
        uid: f8743f90-50a4-4ef8-9fe9-78c245eb8bf3
    target: kubernetes
  symlinks: true
----
