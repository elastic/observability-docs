[[running-on-kubernetes-standalone]]
= Run {agent} standalone on Kubernetes

Use {agent} https://www.docker.elastic.co/r/beats/elastic-agent[Docker images] on Kubernetes to
retrieve cluster metrics.

TIP: Running {ecloud} on Kubernetes? Refer to {eck-ref}/k8s-elastic-agent.html[Run {elastic-agent} on ECK].


[discrete]
== Kubernetes deploy manifests


Before deploying the {agent}, you need to first deploy `kube-state-metrics`, to get the metrics about the state of the objects on the cluster (see the
https://github.com/kubernetes/kube-state-metrics#kubernetes-deployment[Kubernetes
deployment] docs). You can do that by running these two command lines:

["source", "sh", subs="attributes"]
------------------------------------------------
gh repo clone kubernetes/kube-state-metrics
kubectl apply -k kube-state-metrics
------------------------------------------------

To deploy the {Agent}, you first need to download the manifest file:

["source", "sh", subs="attributes"]
------------------------------------------------
curl -L -O https://raw.githubusercontent.com/elastic/elastic-agent/{branch}/deploy/kubernetes/elastic-agent-standalone-kubernetes.yaml
------------------------------------------------

This manifest includes the Kubernetes integration to collect Kubernetes metrics, System integration to collect system level metrics and logs from nodes, and the Pod's log collection using <<kubernetes-provider,dynamic inputs and Kubernetes provider>>. Notice that everything is deployed under the `kube-system` namespace by default. To change the namespace, modify the manifest file.

The {agent} is deployed as a https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/[DaemonSet]
to ensure that there is a running instance on each node of the cluster. These instances are used to retrieve most metrics from the host, such as system metrics, Docker stats, and metrics from all the services running on top of Kubernetes. These metrics are accessed through the deployed `kube-state-metrics`.

Moreover, one of the Pods in the DaemonSet will constantly hold a _leader lock_ which makes it responsible for
handling cluster-wide monitoring. You can find more information about leader election configuration options at <<kubernetes_leaderelection-provider, leader election provider>>. The leader pod will retrieve metrics from all metricsets with the `state_` prefix and the metricset `event`. We make sure that these metrics are retrieved from the leader pod by applying the following <<elastic-agent-kubernetes-autodiscovery, condition>> in the manifest, before declaring the data streams with these metricsets:

[source,yaml]
------------------------------------------------
...
inputs:
  - name: kubernetes-cluster-metrics
    condition: ${kubernetes_leaderelection.leader} == true
    type: kubernetes/metrics
    # metricsets with the state_ prefix and the metricset event
...
------------------------------------------------




[discrete]
== Settings

Set the {es} settings before deploying the manifest:

[source,yaml]
------------------------------------------------
- name: ES_USERNAME
  value: "elastic"
- name: ES_PASSWORD
  value: "passpassMyStr0ngP@ss"
- name: ES_HOST
  value: "https://somesuperhostiduuid.europe-west1.gcp.cloud.es.io:443"
------------------------------------------------

// Begin collapsed section
[%collapsible]
.Configuration details
====
****

[cols="2*<a"]
|===
| Setting | Description

include::configuration/env/shared-env.asciidoc[tag=es-username]

include::configuration/env/shared-env.asciidoc[tag=es-password]

include::configuration/env/shared-env.asciidoc[tag=es-host]

|===

Refer to <<agent-environment-variables>> for all available options.

****
====

[discrete]
=== Run {agent} on control plane nodes

Kubernetes control plane nodes can use https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/[taints]
to limit the workloads that can run on them. The manifest for standalone {agent} defines
tolerations to run on these nodes. Agents running on control plane nodes collect metrics from the control plane
components (scheduler, controller manager) of Kubernetes.
To disable {agent} from running on control plane nodes, remove the following part of the DaemonSet spec:

[source,yaml]
------------------------------------------------
spec:
  # Tolerations are needed to run Elastic Agent on Kubernetes control-plane nodes.
  # Agents running on control-plane nodes collect metrics from the control plane components (scheduler, controller manager) of Kubernetes
  tolerations:
    - key: node-role.kubernetes.io/control-plane
      effect: NoSchedule
    - key: node-role.kubernetes.io/master
      effect: NoSchedule
------------------------------------------------

Both these two tolerations do the same, but `node-role.kubernetes.io/master` is deprecated as of Kubernetes version v1.25 (see https://kubernetes.io/docs/reference/labels-annotations-taints/#node-role-kubernetes-io-master-taint[here]).


[discrete]
== Deploy
To deploy to Kubernetes, run:

["source", "sh", subs="attributes"]
------------------------------------------------
kubectl create -f elastic-agent-standalone-kubernetes.yaml
------------------------------------------------

To check the status, run:

["source", "sh", subs="attributes"]
------------------------------------------------
$ kubectl -n kube-system get pods -l app=elastic-agent
NAME                            READY   STATUS    RESTARTS   AGE
elastic-agent-4665d             1/1     Running   0          81m
elastic-agent-9f466c4b5-l8cm8   1/1     Running   0          81m
elastic-agent-fj2z9             1/1     Running   0          81m
elastic-agent-hs4pb             1/1     Running   0          81m
------------------------------------------------

NOTE: You might need to adjust https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/[resource limits] of the elastic-agent container
in the `elastic-agent-standalone-kubernetes.yaml` manifest. Container resource usage depends on the number of data streams
and the environment size.

[discrete]
== Red Hat OpenShift configuration

If you are using Red Hat OpenShift, you need to specify additional settings in the manifest file and enable the container to run as privileged.

. In the manifest file, modify the `agent-node-datastreams` ConfigMap and adjust inputs:
+
--
* `kubernetes-cluster-metrics` input:
** If `https` is used to access `kube-state-metrics`, add the following settings to all `kubernetes.state_*` datasets:
+
[source,yaml]
------------------------------------------------
  bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
  ssl.certificate_authorities:
    - /var/run/secrets/kubernetes.io/serviceaccount/service-ca.crt
------------------------------------------------
* `kubernetes-node-metrics` input:
** Change the `kubernetes.controllermanager` data stream condition to:
+
[source,yaml]
------------------------------------------------
condition: ${kubernetes.labels.app} == 'kube-controller-manager'
------------------------------------------------
** Change the `kubernetes.scheduler` data stream condition to:
+
[source,yaml]
------------------------------------------------
condition: ${kubernetes.labels.app} == 'openshift-kube-scheduler'
------------------------------------------------
** The `kubernetes.proxy` data stream configuration should look like:
+
[source,yaml]
------------------------------------------------
- data_stream:
    dataset: kubernetes.proxy
    type: metrics
  metricsets:
    - proxy
  hosts:
    - 'localhost:29101'
  period: 10s
------------------------------------------------
** Add the following settings to all data streams that connect to `https://${env.NODE_NAME}:10250`:
+
[source,yaml]
------------------------------------------------
  bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
  ssl.certificate_authorities:
    - /path/to/ca-bundle.crt
------------------------------------------------
NOTE: `ca-bundle.crt` can be any CA bundle that contains the issuer of the certificate used in the Kubelet API.
According to each specific installation of OpenShift this can be found either in `secrets` or in `configmaps`.
In some installations it can be available as part of the service account secret, in
`/var/run/secrets/kubernetes.io/serviceaccount/service-ca.crt`.
When using the https://github.com/openshift/installer/blob/master/docs/user/gcp/install.md[OpenShift installer]
for GCP, mount the following `configmap` in the elastic-agent pod and use `ca-bundle.crt`
in `ssl.certificate_authorities`:
+
[source,shell]
-----
Name:         kubelet-serving-ca
Namespace:    openshift-kube-apiserver
Labels:       <none>
Annotations:  <none>

Data
====
ca-bundle.crt:
-----
--
. Grant the `elastic-agent` service account access to the privileged SCC:
+
[source,shell]
-----
oc adm policy add-scc-to-user privileged system:serviceaccount:kube-system:elastic-agent
-----
+
This command enables the container to be privileged as an administrator for
OpenShift.

. If the namespace where elastic-agent is running has the `"openshift.io/node-selector"` annotation set, elastic-agent
might not run on all nodes. In this case consider overriding the node selector for the namespace to allow scheduling
on any node:
+
[source,shell]
----
oc patch namespace kube-system -p \
'{"metadata": {"annotations": {"openshift.io/node-selector": ""}}}'
----
+
This command sets the node selector for the project to an empty string.


[discrete]
== Autodiscover targeted Pods

Refer to <<elastic-agent-kubernetes-autodiscovery>> for more information.


[discrete]
== Deploying {agent} to collect cluster-level metrics in large clusters

The size and the number of nodes in a Kubernetes cluster can be fairly large at times,
and in such cases the Pod that will be collecting cluster level metrics might face performance
issues due to resources limitations. In this case users might consider to avoid using the
leader election strategy and instead run a dedicated, standalone {agent} instance using
a Deployment in addition to the DaemonSet.

[discrete]
== Deploying {agent} to managed Kubernetes environment

On managed Kubernetes solutions, such as AKS, GKE or EKS, {agent} has no access to collect metrics from the https://kubernetes.io/docs/concepts/overview/components/#control-plane-components[Kubernetes control plane]
components, like `kube-scheduler` and `kube-controller-manager`, that are scheduled on Kubernetes  control plane nodes. Audit logs are available only on Kubernetes control plane nodes as well, and hence cannot be collected by {agent}.

NOTE: You might need to adjust https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/[resource limits] of the elastic-agent container
in the `elastic-agent-standalone-kubernetes.yaml` manifest. Container resource usage depends on the number of data streams
and the environment size.
