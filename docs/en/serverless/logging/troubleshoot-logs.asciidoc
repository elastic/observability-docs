[[observability-troubleshoot-logs]]
= Troubleshoot logs

// :description: Find solutions to errors you might encounter while onboarding your logs.
// :keywords: serverless, observability, troubleshooting

Use this page to find possible solutions for errors your encountering with your logs. This troubleshooting page is broken into the following sections:

- <<logs-onboarding-troubleshooting>>
- <<logs-common-mapping-troubleshooting, Mapping and pipeline issues>>

[discrete]
[[logs-onboarding-troubleshooting]]
== Common onboarding issues

This section provides possible solutions for errors you might encounter while onboarding your logs.

[discrete]
[[observability-troubleshoot-logs-user-does-not-have-permissions-to-create-api-key]]
=== User does not have permissions to create API key

When adding a new data using the guided instructions in your project (**Add data** → **Collect and analyze logs** → **Stream log files**),
if you don't have the required privileges to create an API key, you'll see the following error message:

[NOTE]
====
You need permission to manage API keys
====

[discrete]
[[observability-troubleshoot-logs-solution]]
==== Solution

You need to either:

* Ask an administrator to update your user role to at least **Developer** by going to the user icon on the header bar and opening **Organization** → **Members**. Read more about user roles in <<general-assign-user-roles>>. After your use role is updated, restart the onboarding flow.
* Get an API key from an administrator and manually add the API to the {agent} configuration. See <<observability-stream-log-files-step-3-configure-the-agent,Configure the {agent}>> for more on manually updating the configuration and adding the API key.

// Not sure if these are different in serverless...

////
/* ## Failed to create API key

If you don't have the privileges to create `savedObjects` in a project, you'll see the following error message:

```plaintext
Failed to create API key

Something went wrong: Unable to create observability-onboarding-state
```

### Solution

You need an administrator to give you the `Saved Objects Management` {kib} privilege to generate the required `observability-onboarding-state` flow state.
Once you have the necessary privileges, restart the onboarding flow. */
////

[discrete]
[[observability-troubleshoot-logs-observability-project-not-accessible-from-host]]
=== Observability project not accessible from host

If your Observability project is not accessible from the host, you'll see the following error message after pasting the **Install the {agent}** instructions into the host:

[source,plaintext]
----
Failed to connect to {host} port {port} after 0 ms: Connection refused
----

[discrete]
[[observability-troubleshoot-logs-solution-1]]
==== Solution

The host needs access to your project. Port `443` must be open and the project's {es} endpoint must be reachable. You can locate your project's endpoint by clicking the help icon (image:images/icons/help.svg[Help icon]) and selecting **Endpoints**. Run the following command, replacing the URL with your endpoint, and you should get an authentication error with more details on resolving your issue:

[source,shell]
----
curl https://your-endpoint.elastic.cloud
----

[discrete]
[[observability-troubleshoot-logs-download-agent-failed]]
=== Download {agent} failed

If the host was able to download the installation script but cannot connect to the public artifact repository, you'll see the following error message:

[source,plaintext]
----
Download Elastic Agent

Failed to download Elastic Agent, see script for error.
----

[discrete]
[[observability-troubleshoot-logs-solutions]]
==== Solutions

* If the combination of the {agent} version and operating system architecture is not available, you'll see the following error message:
+
[source,plaintext]
----
The requested URL returned error: 404
----
+
To fix this, update the {agent} version in the installation instructions to a known version of the {agent}.
* If the {agent} was fully downloaded previously, you'll see the following error message:
+
[source,plaintext]
----
Error: cannot perform installation as Elastic Agent is already running from this directory
----
+
To fix this, delete previous downloads and restart the onboarding.
* You're an Elastic Cloud Enterprise user without access to the Elastic downloads page.

[discrete]
[[observability-troubleshoot-logs-install-agent-failed]]
=== Install {agent} failed

If an {agent} already exists on your host, you'll see the following error message:

[source,plaintext]
----
Install Elastic Agent

Failed to install Elastic Agent, see script for error.
----

[discrete]
[[observability-troubleshoot-logs-solution-2]]
==== Solution

You can uninstall the current {agent} using the `elastic-agent uninstall` command, and run the script again.

[WARNING]
====
Uninstalling the current {agent} removes the entire current setup, including the existing configuration.
====

[discrete]
[[observability-troubleshoot-logs-waiting-for-logs-to-be-shipped-step-never-completes]]
=== Waiting for Logs to be shipped... step never completes

If the **Waiting for Logs to be shipped...** step never completes, logs are not being shipped to your Observability project, and there is most likely an issue with your {agent} configuration.

[discrete]
[[observability-troubleshoot-logs-solution-3]]
==== Solution

Inspect the {agent} logs for errors. See the {fleet-guide}/debug-standalone-agents.html#inspect-standalone-agent-logs[Debug standalone {agent}s] documentation for more on finding errors in {agent} logs.

[discrete]
[[logs-common-mapping-troubleshooting]]
== Mapping and pipeline issues

This section provides possible solutions for mapping and pipeline issues you might encounter with your logs.

[discrete]
[[logs-mapping-troubleshooting-keyword-limit]]
=== Keyword fields are too long

The `keyword` field limit is 32,766 bytes. When indexing a document, if your `keyword` field length exceeds this limit, you'll see an error similar to the following:

[source, plaintext]
----
max_bytes_length_exceeded_exception: bytes can be at most 32766 in length
----

[discrete]
[[logs-mapping-troubleshooting-keyword-limit-solution]]
==== Solution

Avoid this error using one of the following options:

*Stop indexing the field:* If you don't need the `keyword` field for aggregation or search, set `"index":false` in the index template to stop indexing the field.

*Convert the `keyword` field to a `text` field:* To continue indexing the field while avoiding length limits, you can convert the `keyword` field to a `text` field.

NOTE: Aggregations on this field would no longer be supported, but the contents would be searchable.

To convert the `keyword` field to a `text` field:

. Create a new index with the `text` field data type.
. Reindex from the `_source` field of the source index using the {ref}/docs-reindex.html[`_reindex` API].

[discrete]
[[logs-mapping-troubleshooting-date-mismatch]]
=== Date format mismatch

If the format of the `date` field in your document doesn't match the format set in your index template, you'll see an error similar to the following:

[source, plaintext]
----
failed to parse field [date] of type [date] in document with id 'KGcZb3cBqhj6kAxank_x'.
----

[discrete]
[[logs-mapping-troubleshooting-date-solution]]
==== Solution

Add the format of the mismatched date to your index template.
Multiple formats can be specified by separating them with `||` as a separator.
Each format will be tried in turn until a matching format is found.
For example:

[source,console,id=date-format-example]
----
PUT my-index-000001
{
  "mappings": {
    "properties": {
      "date": {
        "type":   "date",
        "format": "yyyy-MM-dd HH:mm:ss||yyyy-MM-dd||epoch_millis"
      }
    }
  }
}
----

Refer to the {ref}/date.html[`date` field type] docs for more information.

[discrete]
[[logs-mapping-troubleshooting-grok-mismatch]]
=== Grok or dissect pattern mismatch

If the pattern in your grok or dissect processor doesn't match the format of your document, you'll see an error similar to the following:

[source, plaintext]
----
Provided Grok patterns do not match field value...
----

[discrete]
[[logs-mapping-troubleshooting-grok-solution]]
==== Solution

Make sure your {ref}/grok-processor.html[grok] or {ref}/dissect-processor.html[dissect] processor pattern matches your log document format.

You can build and debug grok patterns in {kib} using the {kibana-ref}/xpack-grokdebugger.html[Grok Debugger]. Find the *Grok Debugger* by navigating to the *Developer tools* page using the
navigation menu or the global search field.

From here, you can enter sample data representative of the log document you're trying to ingest and the Grok pattern you want to apply to the data.

If you don't see any *Structured Data* when you simulate the grok pattern, iterate on the pattern until you find the error.