[[build-a-new-integration]]
= Build an integration

Ready to monitor, ingest, and visualize something? Let's get started.

* <<build-overview>>
* <<build-spin-stack>>
* <<build-create-package>>
* <<add-a-data-stream>>
* <<edit-ingest-pipeline>>
* <<add-a-mapping>>
* <<create-dashboards>>
* <<testing-and-validation>>
* <<finishing-touches>>

// Quick start isn't ready yet
// TIP: Familiar with the {stack} and just want a quick way to get started?
// See <<quick-start>>.

[[build-overview]]
== Overview and prerequisites

Before building an integration, you should have an understanding of the following:

* {stack} concepts, like data streams, ingest pipelines, and mappings
* The <<package-spec>>

In addition, you must have <<elastic-package>> installed on your machine.

[[build-spin-stack]]
== Spin up the {stack}

The <<elastic-package,`elastic-package`>> tool provides a quick way to spin up the {stack}.
The following command deploys {es}, {kib}, and the {package-registry}:

[source,terminal]
----
elastic-package stack up -v -d
----

To view a list of the available options for this command, run:

[source,terminal]
----
elastic-package stack up -h
----

When complete, go to http://localhost:5601 and log in with the username `elastic` and the password `changeme`.

[TIP]
====
Development time over? Tear down the {stack} with:

[source,terminal]
----
elastic-package stack down
----
====

[[build-create-package]]
== Create a new package

Rather than copying the source of an existing package, we recommend using the `elastic-package create` command to build a new package. Running this command ensures that your integration follows the latest recommendations for the package format.

Use the `elastic-package` TUI wizard to bootstrap a new package:

[source,terminal]
----
elastic-package create package
----

The wizard walks you through the creation of the package, including setting a package name, version, category, etc.
When the wizard completes, you'll have a basic package complete with a sample manifest,
changelog, documentation, and screenshot.

[NOTE]
====
It may not do anything yet, but your integration can be built and loaded into your locally running package registry from this step onwards.
Jump to <<build-it>> at any point in this documentation to take your integration for a test run.
====

[[add-a-data-stream]]
== Add a data stream

A data stream is a logical sub-division of an integration package,
dealing with a specific observable aspect of the service or product being observed. For example,
the https://github.com/elastic/integrations/tree/main/packages/apache[Apache integration] has three data streams,
each represented by a separate folder of assets in the `data_stream` directory:

[source,text]
----
apache
└───data_stream
│   └───access
│   └───error
│   └───status
----

****
**Data streams** allow you to store time series data across multiple indices while giving you a single named resource for requests.

A data stream defines multiple {es} assets, like index templates, ingest pipelines, and field definitions.
These assets are loaded into {es} when a user installs an integration using the {fleet} UI in {kib}.

A data stream also defines a policy template.
Policy templates include variables that allow users to configure the data stream using the {fleet} UI in {kib}.
Then, the {agent} interprets the resulting policy to collect relevant information from the product or service being observed.

See {fleet-guide}/data-streams.html[data streams] for more information.
****

Bootstrap a new data stream using the TUI wizard.
In the directory of your package, run:

[source,terminal]
----
elastic-package create data-stream
----

Follow the prompts to name, title, and select your data stream type.
Then, run this command each time you add a new data stream to your integration.

// This needs work
Next, manually adjust the data stream:

* define required variables
* define used fields
* define ingest pipeline definitions (if necessary)
* update the {agent}'s stream configuration

[[edit-ingest-pipeline]]
== Edit ingest pipelines

In most instances, before you ingest data into the {stack}, the data needs to be manipulated.
For example, you should parse your logs into structured data before ingestion.
To do so, integrations use **ingest pipelines**.

****
**Ingest pipelines** let you perform common transformations on your data before indexing. For example, you can use pipelines to remove fields, extract values from text, and enrich your data.

A pipeline consists of a series of configurable tasks called processors. Each processor runs sequentially, making specific changes to incoming documents. After the processors have run, {es} adds the transformed documents to your data stream or index.

Learn more in the {ref}/ingest.html[ingest pipeline reference].
****

Ingest pipelines are defined in the `elasticsearch/ingest_pipeline` directory.
They only apply to the parent data stream within which they live.

For example, the https://github.com/elastic/integrations/tree/main/packages/apache[Apache integration]:

[source,text]
----
apache
└───data_stream
│   └───access
│   │   └───elasticsearch/ingest_pipeline
│   │          default.yml <1>
│   └───error
│   └───status
----
<1> The ingest pipeline definition for the access logs data stream of the apache integration

An ingest pipeline definition requires a description and an array of processors.
Here's a snippet of the access logs ingest pipeline:

[source,yaml]
----
description: "Pipeline for parsing Apache HTTP Server access logs."
processors:
- set:
    field: event.ingested
    value: '{{_ingest.timestamp}}'
- rename:
    field: message
    target_field: event.original
- remove:
    field: apache.access.time
    ignore_failure: true
----

Open each `elasticsearch/ingest_pipeline/default.yml` file created for each data stream.
Edit each ingest pipeline to match your needs.

The {ref}/processors.html[processor reference] provides a list of all available processors and their configurations.

[[add-a-mapping]]
== Edit field mappings

Ingest pipelines create fields in an {es} index, but don't define the fields themselves.
Instead, each field requires a defined data type or mapping.

****
**Mapping** is the process of defining how a document, and the fields it contains, are stored and indexed.
Each document is a collection of fields, each having its own data type. When mapping your data, create a mapping definition containing a list of fields pertinent to the document. A mapping definition also includes metadata fields, like the _source field, which customize how the associated metadata of a document is handled.

To learn more, see {ref}/mapping.html[mapping].
****

Mappings are defined in the `fields` directory.
Like ingest pipelines, mappings only apply to the parent data stream.
The apache integration has four different field definitions:

[source,text]
----
apache
└───data_stream
│   └───access
│   │   └───elasticsearch/ingest_pipeline
│   │   │      default.yml
│   │   └───fields
│   │          agent.yml <1>
│   │          base-fields.yml <2>
│   │          ecs.yml <3>
│   │          fields.yml <4>
│   └───error
│   └───status
----
<1> ??
<2> `base-fields.yml` never changes and is required for all integrations
<3> Defines the relevant ECS fields
<4> Custom apache access log fields ??


// Need more on mapping

// Maybe something on ECS too??

[[create-dashboards]]
== Create and export dashboards

// https://github.com/elastic/integrations/issues/269

Visualizing integration data in a meaningful way is an important aspect of an integration.

When creating a new integration, it's important to add dashboards.

To get started, create a new dashboard, or customize an existing one.
You can use `elastic-package` to boot up the service stack.
Navigate to the package you want to create dashboards for, and run:

[source,terminal]
----
elastic-package service
----

When you're done making changes, you can use `elastic-package` to export the dashboards and their dependencies to the package source.

[discrete]
== Dashboard planning

Many integrations cover more than one component of a target system.
For example, the RabitMQ module provides several metricsets covering connection, exchange, node, queue.
It makes sense to break this information down into several interconnected dashboards.
The default one is an overview of a target system, and the others provide deep-dives into the various parts of the target system.
The content of the Overview dashboard should be cherry-picked from all datasets and individually compiled for every such integration.

[discrete]
=== Metrics

Always check the type of a metric and ensure that the correct transformation is applied where applicable.
For example, in most cases for cumulative counters, it makes sense to use the rate function.

// relevant blog post: https://www.elastic.co/blog/visualizing-observability-with-kibana-event-rates-and-rate-of-change-in-tsvb

[discrete]
=== Visualization type

For new visualizations, we recommend using Lens first.
If what you're trying to achieve cannot be accomplished with the current capabilities of Lens, try TSVB.

// add links

[discrete]
=== Filters

When building a dashboard, always consider adding a filter dropdown. Why?
In most cases, the integrations monitor multiple instances of a target system,
so we need to provide a way to switch between them.

To build a filter dropdown, use the Controls visualization.
Here's an example of a host name dropdown that you can add to the System dashboard:

// screenshot omitted for now

// screenshot omitted for now

// screenshot omitted for now

[discrete]
=== Navigation

If an integration has several dashboards, ensure that you can easily navigate all of them.
To build dashboard navigation, use the Markdown visualization type.

For example, the System dashboard provides the following navigation:

// screenshot omitted for now

Source:

[source,text]
----
[System Overview](#/dashboard/system-Metrics-system-overview-ecs)  | [Host Overview](#/dashboard/system-79ffd6e0-faa0-11e6-947f-177f697178b8-ecs) |
[Containers overview](#/dashboard/system-CPU-slash-Memory-per-container-ecs)
----

While this can work, it doesn't highlight the selected dashboard.
Unfortunately the Markdown control is not optimized for navigation,
which makes it cumbersome to build navigation with highlighted links because each link should be highlighted separately.
This means that the navigation control you're building has to be cloned as many times as there are dashboard to ensure proper link highlighting. E.g.

[source,text]
----
**[System Overview](#/dashboard/system-Metrics-system-overview-ecs)**  | [Host Overview](#/dashboard/system-79ffd6e0-faa0-11e6-947f-177f697178b8-ecs) |
[Containers overview](#/dashboard/system-CPU-slash-Memory-per-container-ecs)

[System Overview](#/dashboard/system-Metrics-system-overview-ecs)  | **[Host Overview](#/dashboard/system-79ffd6e0-faa0-11e6-947f-177f697178b8-ecs)** |
[Containers overview](#/dashboard/system-CPU-slash-Memory-per-container-ecs)

[System Overview](#/dashboard/system-Metrics-system-overview-ecs)  | [Host Overview](#/dashboard/system-79ffd6e0-faa0-11e6-947f-177f697178b8-ecs) |
**[Containers overview](#/dashboard/system-CPU-slash-Memory-per-container-ecs)**
----

[discrete]
=== Target system name

Currently we don't make it a rule to show on a dashboard what system it's designed to monitor. The only way to see it is through the dashboard name.

// screenshot omitted for now

When using multiple dashboards on bigger screens, it makes it hard to distinguish between the dashboards. You can improve this by using the Markdown control to display the target system the dashboard is used for.

[discrete]
=== Naming

When building dashboards, use the following naming convention.

[discrete]
==== Visualizations

[source,text]
----
<NAME> [<Metrics | Logs> <PACKAGE NAME>]
----

Examples:

* Memory Usage Gauge [Metrics System]
* New groups [Logs System]

Rename all visualizations added to a dashboard only to show the <NAME> part.

// screenshot omitted for now

[discrete]
==== Dashboards

[source,text]
----
[<Metrics | Logs> <PACKAGE NAME>] <Name>
----

Examples:

* [Metrics System] Host overview
* [Metrics MongoDB] Overview

[discrete]
=== Screenshots

Letter casing is important for screenshot descriptions.
Descriptions are shown in the {kib} UI, so try and keep them clean and consistent.

These descriptions are visualized in the {kib} UI. It would be better experience to have them clean and consistent.

* Bad candidate: filebeat running on ec2 machine
* Good candidates: {filebeat} running on AWS EC2 machine

[discrete]
== Exporting

// move to new page

// add https://www.elastic.co/guide/en/beats/devguide/current/export-dashboards.html

[source,terminal]
----
elastic-package export
----

[[build-it]]
== Build

To format, lint, and build your integration, in that order, run:

[source,terminal]
----
elastic-package check
----

Problems and potential solutions will display in the console.
Fix them and rerun the command.
Alternatively,
skip formatting and linting with the `build` command:

[source,terminal]
----
elastic-package build
----

With the package built, run the following command from inside of the integration directory to recycle the package-registry docker container.
This refreshes the {fleet} UI, allowing it to pick up the new integration in {kib}.

[source,terminal]
----
elastic-package stack up --services package-registry
----

[[testing-and-validation]]
== Testing and validation

. Build the package you'd like to verify (e.g. `apache`):
+
[source,terminal]
----
cd apache
elastic-package build
----

. Start the testing environment:
+
Run from inside the Integrations repository:
+
[source,terminal]
----
elastic-package stack up -d -v
----
+
The command above will boot up the {stack} ({es}, {kib}, and {package-registry}) using Docker containers.
It rebuilds the {package-registry} Docker image using packages built in step 1. and boots up the {package-registry}.
+
To reload the already deployed {package-registry}, use the following command:
+
[source,terminal]
----
elastic-package stack up -v -d --services package-registry
----

. Verify that your integration is available in the correct version. For example, MySQL: http://localhost:8080/search?package=mysql (use `experimental=true` parameter if the package is in experimental version.
Alternatively set `release` to `beta` or higher in your package's `manifest.yml`, if appropriate.)
+
[source,json]
----
[
  {
    "description": "MySQL Integration",
    "download": "/epr/mysql/mysql-0.0.1.tar.gz",
    "icons": [
      {
        "src": "/package/mysql/0.0.1/img/logo_mysql.svg",
        "title": "logo mysql",
        "size": "32x32",
        "type": "image/svg+xml"
      }
    ],
    "name": "mysql",
    "path": "/package/mysql/0.0.1",
    "title": "MySQL",
    "type": "integration",
    "version": "0.0.1"
  }
]
----
+
The `elastic-package stack` provides an enrolled instance of the {agent}. Use that one instead of a local application
if you can run the service (you're integrating with) in the Docker network and you don't need to rebuild the Elastic-Agent
or it's subprocesses (e.g. {filebeat} or {metricbeat}). The service Docker image can be used for [system
testing](https://github.com/elastic/elastic-package/blob/main/docs/howto/system_testing.md). If you prefer to use a local
instance of the {agent}, proceed with steps 4 and 5:

. (Optional) Download the https://www.elastic.co/downloads/elastic-agent[{agent}].

// lint ignore fleet ingest-manager
. (Optional) Enroll the {agent} and start it:
+
Use the "Enroll new agent" option in the {kib} UI (Ingest Manager -> Fleet -> Create user and enable Fleet) and run a similar command:
+
[source,terminal]
----
./elastic-agent enroll http://localhost:5601/rel cFhNVlZIRUIxYjhmbFhqNTBoS2o6OUhMWkF4SFJRZmFNZTh3QmtvR1cxZw==
./elastic-agent run
----
+
The `elastic-agent` starts two other processes: `metricbeat` and `filebeat`.

. Run the product you're integrating with (e.g. a docker image with MySQL).

. Install package.
+
Click out the configuration in the {kib} UI, deploy it and wait for the agent to pick out the updated configuration.

. Navigate with {kib} UI to freshly installed dashboards, verify the metrics/logs flow.

=== Use test runners

`elastic-package` provides different types of test runners.
See <<testing>> to learn about the various methods for testing packages.

The `test` subcommand requires a reference to the live {stack}. You can define service endpoints using environment variables.
If you're using the {stack} created with `elastic-package`, you can use export endpoints with `elastic-package stack shellinit`:

[source,terminal]
----
$ eval "$(elastic-package stack shellinit)"
----

To preview environment variables:

[source,terminal]
----
$ elastic-package stack shellinit
export ELASTIC_PACKAGE_ELASTICSEARCH_HOST=http://127.0.0.1:9200
export ELASTIC_PACKAGE_ELASTICSEARCH_USERNAME=elastic
export ELASTIC_PACKAGE_ELASTICSEARCH_PASSWORD=changeme
export ELASTIC_PACKAGE_KIBANA_HOST=http://127.0.0.1:5601
----

[[finishing-touches]]
== Finishing touches

// https://github.com/elastic/integrations/blob/main/docs/fine_tune_integration.md

=== Words

Tips for manifest files:

* Descriptions of configuration options should be as short as possible.
+
Remember to keep only the meaningful information about the configuration option.
+
** Good candidates: references to the product configuration, accepted string values, explanation.
** Bad candidates: Collect metrics from A, B, C, D,... X, Y, Z datasets.

* Descriptions should be human readable.
+
Try to rephrase sentences like: Collect foo_Bar3 metrics, into Collect Foo Bar metrics.

* Descriptions should be easy to understand.
+
Simplify sentences, don't provide information about the input if not required.
+
** Bad candidate: Collect application logs (log input)
** Good candidates: Collect application logs, Collect standard logs for the application

=== Add an icon

The integration icons are displayed in different places in {kib}, hence it's better to define custom icons to make the UI easier to navigate.

=== Add screenshots

The {kib} Integration Manager shows screenshots related to the integration. Screenshots include {kib} dashboards visualizing the metric and log data.

=== Create a readme file

The README template is used to render the final README file, including exported fields. The template should be placed in the package/<integration-name>/_dev/build/docs/README.md. If the directory doesn't exist, please create it.

To see how to use template functions, for example {{fields "data-stream-name"}}, review the MySQL docs template. If the same data stream name is used in both metrics and logs, please add -metrics and -logs in the template. For example, elb is a data stream for log and also a data stream for metrics. In README.md template, {{fields "elb_logs"}} and {{fields "elb_metrics"}} are used to separate them.

=== Review artifacts

=== Define variable properties

The variable properties customize visualization of configuration options in the {kib} UI. Make sure they're defined in all manifest files.

[source,yaml]
----
vars:
  - name: paths
    required: true <1>
    show_user: true <2>
    title: Access log paths <3>
    description: Paths to the nginx access log file. <4>
    type: text <5>
    multi: true <6>
    default:
      - /var/log/nginx/access.log*
----
<1> option is required
<2> don't hide the configuration option (collapsed menu)
<3> human readable variable name
<4> variable description (may contain some details)
<5> field type (according to the reference: text, password, bool, integer)
<6> the field has multiple values.

// === Add sample events

// text
