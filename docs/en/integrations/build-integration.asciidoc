[[build-a-new-integration]]
= Build an integration

Ready to monitor, ingest, and visualize something? Let's get started.

* <<build-overview>>
* <<build-spin-stack>>
* <<build-create-package>>
* <<add-a-data-stream>>
* <<define-deployment-modes>>
* <<edit-ingest-pipeline>>
* <<add-a-mapping>>
* <<create-dashboards>>
* <<testing-and-validation>>
* <<finishing-touches>>
* <<tips-for-building>>

TIP: Familiar with the {stack} and just want a quick way to get started?
See <<quick-start>>.

[[build-overview]]
== Overview and prerequisites

Before building an integration, you should have an understanding of the following:

* {stack} concepts, like data streams, ingest pipelines, and mappings
* The <<package-spec>>

In addition, you must have <<elastic-package,`elastic-package`>> installed on your machine.
Using `elastic-package` is recommended for integration maintainers as it provides crucial utilities and scripts for building out integrations.

[[build-spin-stack]]
== Spin up the {stack}

The <<elastic-package,`elastic-package`>> tool provides a quick way to spin up the {stack}.
The following command deploys {es}, {kib}, and the {package-registry}:

[source,terminal]
----
elastic-package stack up -v -d
----

To view a list of the available options for this command, run:

[source,terminal]
----
elastic-package stack up -h
----

When complete, go to http://localhost:5601 and log in with the username `elastic` and the password `changeme`.

[TIP]
====
Development time over? Tear down the {stack} with:

[source,terminal]
----
elastic-package stack down
----
====

[[build-create-package]]
== Create a new package

Rather than copying the source of an existing package, we recommend using the `elastic-package create` command to build a new package. Running this command ensures that your integration follows the latest recommendations for the package format.

Use the `elastic-package` TUI wizard to bootstrap a new package:

[source,terminal]
----
elastic-package create package
----

The wizard walks you through the creation of the package, including setting a package name, version, category, etc.
When the wizard completes, you'll have a basic package complete with a sample manifest,
changelog, documentation, and screenshot.

[NOTE]
====
It may not do anything yet, but your integration can be built and loaded into your locally running package registry from this step forward.
Jump to <<build-it>> at any point in this documentation to take your integration for a test run.
====

[[add-a-data-stream]]
== Add a data stream

A data stream is a logical sub-division of an integration package,
dealing with a specific observable aspect of the service or product being observed. For example,
the https://github.com/elastic/integrations/tree/main/packages/apache[Apache integration] has three data streams,
each represented by a separate folder of assets in the `data_stream` directory:

[source,text]
----
apache
└───data_stream
│   └───access
│   └───error
│   └───status
----

****
**Data streams** allow you to store time series data across multiple indices while giving you a single named resource for requests.

A data stream defines multiple {es} assets, like index templates, ingest pipelines, and field definitions.
These assets are loaded into {es} when a user installs an integration using the {fleet} UI in {kib}.

A data stream also defines a policy template.
Policy templates include variables that allow users to configure the data stream using the {fleet} UI in {kib}.
Then, the {agent} interprets the resulting policy to collect relevant information from the product or service being observed.
Policy templates can also define an integration's supported <<deployment_modes>>.

See {fleet-guide}/data-streams.html[data streams] for more information.
****

Bootstrap a new data stream using the TUI wizard.
In the directory of your package, run:

[source,terminal]
----
elastic-package create data-stream
----

Follow the prompts to name, title, and select your data stream type.
Then, run this command each time you add a new data stream to your integration.

// This needs work
Next, manually adjust the data stream:

* define required variables
* define used fields
* define ingest pipeline definitions (if necessary)
* update the {agent}'s stream configuration

[[define-deployment-modes]]
== Define deployment modes

Some integrations can be deployed on fully managed agents.
These integrations are known as "agentless" integrations.
Define the deployment mode of an integration with the <<deployment_modes>> property and display/hide variables
in different deployment modes with the <<hide_in_deployment_modes>> property.

[discrete]
[[deployment_modes]]
=== `deployment_modes`

Policy templates can indicate which deployment modes they support.
Use the `deployment_modes` property in the policy template schema to define the supported deployment modes.
Options are `default` and `agentless`. A policy template can support both modes.

Example policy template declaration:

[source,yaml]
----
format_version: 3.2.0
name: aws
title: AWS
version: 2.13.1
...
policy_templates:
  - name: billing
    title: AWS Billing
    description: Collect billing metrics with Elastic Agent
    deployment_modes: <1>
      default:
        enabled: false <2>
      agentless:
        enabled: true <3>
    data_streams:
      - billing
    ...
----
<1> Defines the supported deployment modes
<2> Disables agent deployment support
<3> Enables agentless deployment support

[discrete]
[[hide_in_deployment_modes]]
=== `hide_in_deployment_modes`

Variables can be hidden in certain deployment modes.
Use the `hide_in_deployment_modes` property to opt variables in or out of being displayed in default or agentless mode.
This property works at any manifest level.

Example variable declaration:

[source,yaml]
----
streams:
  - input: filestream
    vars:
      - name: paths
        type: text
        title: Paths
        multi: true
        required: true
        show_user: true
        default:
          - /var/log/my-package/*.log
      - name: agentless_only
        type: text
        title: Agentless only variable
        multi: false
        required: false
        show_user: true
        hide_in_deployment_modes: <1>
          - default
     - name: hidden_in_agentless
       type: text
       title: Hidden in agentless variable
       multi: false
       required: false
       show_user: true
       hide_in_deployment_modes: <2>
         - agentless
----
<1> Disables visibility of the variable in agent deployment mode
<2> Disables visibility of the variable in agentless deployment mode

For more information on variable property definitions, refer to <<define-variable-properties>>.

[discrete]
[[agentless-capabilities]]
=== Agentless capabilities

The capabilities feature protects agentless deployments from allowing undesired inputs to run.
A static `capabilities.yml` file defines these allowed and disallowed inputs and is passed to deployed agents.
To determine which capabilities are currently allowed on Agentless, refer to https://github.com/elastic/agentless-controller/blob/main/controllers/config/capabilities.yml[`capabilities.yml`].

[[edit-ingest-pipeline]]
== Edit ingest pipelines

In most instances, before you ingest data into the {stack}, the data needs to be manipulated.
For example, you should parse your logs into structured data before ingestion.
To do so, integrations use **ingest pipelines**.

****
**Ingest pipelines** let you perform common transformations on your data before indexing. For example, you can use pipelines to remove fields, extract values from text, and enrich your data.

A pipeline consists of a series of configurable tasks called processors. Each processor runs sequentially, making specific changes to incoming documents. After the processors have run, {es} adds the transformed documents to your data stream or index.

Learn more in the {ref}/ingest.html[ingest pipeline reference].
****

Ingest pipelines are defined in the `elasticsearch/ingest_pipeline` directory.
They only apply to the parent data stream within which they live. For our example, this would be the `apache.access` dataset.

For example, the https://github.com/elastic/integrations/tree/main/packages/apache[Apache integration]:

[source,text]
----
apache
└───data_stream
│   └───access
│   │   └───elasticsearch/ingest_pipeline
│   │          default.yml <1>
│   └───error
│   └───status
----
<1> The ingest pipeline definition for the access logs data stream of the Apache integration

An ingest pipeline definition requires a description and an array of processors.
Here's a snippet of the access logs ingest pipeline:

[source,yaml]
----
description: "Pipeline for parsing Apache HTTP Server access logs."
processors:
- set:
    field: event.ingested
    value: '{{_ingest.timestamp}}'
- rename:
    field: message
    target_field: event.original
- remove:
    field: apache.access.time
    ignore_failure: true
----

Open each `elasticsearch/ingest_pipeline/default.yml` file created for each data stream.
Edit each ingest pipeline to match your needs.

The {ref}/processors.html[processor reference] provides a list of all available processors and their configurations.

[[add-a-mapping]]
== Edit field mappings

Ingest pipelines create fields in an {es} index, but don't define the fields themselves.
Instead, each field requires a defined data type or mapping.

****
**Mapping** is the process of defining how a document, and the fields it contains, are stored and indexed.
Each document is a collection of fields, each having its own data type. When mapping your data, create a mapping definition containing a list of fields pertinent to the document. A mapping definition also includes metadata fields, like the _source field, which customize how the associated metadata of a document is handled.

To learn more, see {ref}/mapping.html[mapping].
****

In the integration, the `fields` directory serves as the blueprint used to create component templates for the integration. The content from all files in this directory will be unified when the integration is built, so the mappings need to be unique per data stream dataset.

Like ingest pipelines, mappings only apply to the data stream dataset, for our example the `apache.access` dataset.
+
NOTE: The names of these files are conventions, any file name with a `.yml` extension will work.

Integrations have had significant enhancements in how ECS fields are defined. Below is a guide on which approach to use, based on the version of Elastic your integration will support.
+
. ECS mappings component template (>=8.13.0)
Integrations *only* supporting version 8.13.0 and up, can use the https://github.com/elastic/elasticsearch/blob/c2a3ec42632b0339387121efdef13f52c6c66848/x-pack/plugin/core/template-resources/src/main/resources/ecs%40mappings.json[ecs@mappings] component template installed by Fleet.
This makes explicitly declaring ECS fields unnecessary; the `ecs@mappings` component template in Elasticsearch will automatically detect and configure them.
However, should ECS fields be explicitly defined, they will overwrite the dynamic mapping provided by the `ecs@mappings` component template.
They can also be imported with an `external` declaration, as seen in the example below.
+
. Dynamic mappings imports (<8.13.0 & >=8.13.0)
Integrations supporting the Elastic stack below version 8.13.0 can still dynamically import ECS field mappings by defining `import_mappings: true` in the ECS section of the `_dev/build/build.yml` file in the root of the package directory.
This introduces a https://github.com/elastic/elastic-package/blob/f439b96a74c27c5adfc3e7810ad584204bfaf85d/internal/builder/_static/ecs_mappings.yaml[dynamic mapping] with most of the ECS definitions.
Using this method means that, just like the previous approach, ECS fields don't need to be defined in your integration, they are dynamically integrated into the package at build time.
Explicitly defined ECS fields can be used and will also overwrite this mechanism.

An example of the aformentioned `build.yml` file for this method:
+
[source,yaml]
----
dependencies:
  ecs:
    reference: git@v8.6.0
    import_mappings: true
----
+
. Explicit ECS mappings
As mentioned in the previous two approaches, ECS mappings can still be set explicitly and will overwrite the dynamic mappings.
This can be done in two ways:
- Using an `external: ecs` reference to import the definition of a specific field.
- Literally defining the ECS field.

The `external: ecs` definition instructs the `elastic-package` command line tool to refer to an external ECS reference to resolve specific fields. By default it looks at the https://raw.githubusercontent.com/elastic/ecs/v8.6.0/generated/ecs/ecs_nested.yml[ECS reference] file hosted on Github.
This external reference file is determined by a Git reference found in the `_dev/build/build.yml` file, in the root of the package directory.
The `build.yml` file set up for external references:
+
[source,yaml]
----
dependencies:
  ecs:
    reference: git@v8.6.0
----

Literal definition a ECS field:
[source,yaml]
----
- name: cloud.acount.id
  level: extended
  type: keyword
  ignore_above: 1024
  description: 'The cloud account or organ....'
  example: 43434343
----

. Local ECS reference file (air-gapped setup)
By changing the Git reference in in `_dev/build/build.yml` to the path of the downloaded https://raw.githubusercontent.com/elastic/ecs/v8.6.0/generated/ecs/ecs_nested.yml[ECS reference] file, it is possible for the `elastic-package` command line tool to look for this file locally. Note that the path should be the full path to the reference file.
Doing this, our `build.yml` file looks like:
+
----
dependencies:
  ecs:
    reference: file:///home/user/integrations/packages/apache/ecs_nested.yml
----

The `access` data stream dataset of the Apache integration has four different field definitions:
+
NOTE: The `apache` integration below has not yet been updated to use the dynamic ECS field definition and uses `external` references to define ECS fields in `ecs.yml`.
+
[source,text]
----
apache
└───data_stream
│   └───access
│   │   └───elasticsearch/ingest_pipeline
│   │   │      default.yml
│   │   └───fields
│   │          agent.yml
│   │          base-fields.yml
│   │          ecs.yml
│   │          fields.yml
│   └───error
│   │   └───elasticsearch/ingest_pipeline
│   │   │      default.yml
│   │   └───fields
│   │          agent.yml
│   │          base-fields.yml
│   │          ecs.yml
│   │          fields.yml
│   └───status
----

=== agent.yml
The `agent.yml` file defines fields used by default processors.
Examples: `cloud.account.id`, `container.id`, `input.type`

=== base-fields.yml
In this file, the `data_stream` subfields `type`, `dataset` and `namespace` are defined as type `constant_keyword`, the values for these fields are added by the integration.
The `event.module` and `event.dataset` fields are defined with a fixed value specific for this integration:
- `event.module: apache`
- `event.dataset: apache.access`
Field `@timestamp` is defined here as type `date`.

=== fields.yml
Here we define fields that we need in our integration and are not found in the ECS.
The example below defines field `apache.access.ssl.protocol` in the Apache integration.
+
[source,yaml]
----
- name: apache.access
  type: group
  fields:
    - name: ssl.protocol
      type: keyword
      description: |
        SSL protocol version.
----

// Maybe something on ECS too??

Learn more about fields in the https://www.elastic.co/guide/en/integrations-developer/current/general-guidelines.html#_document_all_fields[general guidelines].

[[create-dashboards]]
== Create and export dashboards

// https://github.com/elastic/integrations/issues/269

Visualizing integration data in a meaningful way is an important aspect of an integration.

When creating a new integration, it's important to add dashboards.

To get started, create a new dashboard, or customize an existing one.
You can use `elastic-package` to boot up the service stack.
Navigate to the package you want to create dashboards for, and run:

[source,terminal]
----
elastic-package service
----

When you're done making changes, you can use `elastic-package` to export the dashboards and their dependencies to the package source.

[discrete]
== Dashboard planning

Many integrations cover more than one component of a target system.
For example, the RabbitMQ module provides several metricsets covering connection, exchange, node, queue.
It makes sense to break this information down into several interconnected dashboards.
The default one is an overview of a target system, and the others provide deep-dives into the various parts of the target system.
The content of the Overview dashboard should be cherry-picked from all datasets and individually compiled for every such integration.

[discrete]
=== Metrics

Always check the type of a metric and ensure that the correct transformation is applied where applicable.
For example, in most cases for cumulative counters, it makes sense to use the rate function.

// relevant blog post: https://www.elastic.co/blog/visualizing-observability-with-kibana-event-rates-and-rate-of-change-in-tsvb

[discrete]
=== Visualization type

For new visualizations, we recommend using Lens first.
If what you're trying to achieve cannot be accomplished with the current capabilities of Lens, try TSVB.

// add links

[discrete]
=== Filters

When building a dashboard, always consider adding a filter dropdown. Why?
In most cases, the integrations monitor multiple instances of a target system,
so we need to provide a way to switch between them.

To build a filter dropdown, use the Controls visualization.
Here's an example of a host name dropdown that you can add to the System dashboard:

// screenshot omitted for now

// screenshot omitted for now

// screenshot omitted for now

[discrete]
=== Navigation

If an integration has several dashboards, ensure that you can easily navigate all of them.
To build dashboard navigation, use the Markdown visualization type.

For example, the System dashboard provides the following navigation:

// screenshot omitted for now

Source:

[source,text]
----
[System Overview](#/dashboard/system-Metrics-system-overview-ecs)  | [Host Overview](#/dashboard/system-79ffd6e0-faa0-11e6-947f-177f697178b8-ecs) |
[Containers overview](#/dashboard/system-CPU-slash-Memory-per-container-ecs)
----

While this can work, it doesn't highlight the selected dashboard.
Unfortunately the Markdown control is not optimized for navigation,
which makes it cumbersome to build navigation with highlighted links because each link should be highlighted separately.
This means that the navigation control you're building has to be cloned as many times as there are dashboard to ensure proper link highlighting. E.g.

[source,text]
----
**[System Overview](#/dashboard/system-Metrics-system-overview-ecs)**  | [Host Overview](#/dashboard/system-79ffd6e0-faa0-11e6-947f-177f697178b8-ecs) |
[Containers overview](#/dashboard/system-CPU-slash-Memory-per-container-ecs)

[System Overview](#/dashboard/system-Metrics-system-overview-ecs)  | **[Host Overview](#/dashboard/system-79ffd6e0-faa0-11e6-947f-177f697178b8-ecs)** |
[Containers overview](#/dashboard/system-CPU-slash-Memory-per-container-ecs)

[System Overview](#/dashboard/system-Metrics-system-overview-ecs)  | [Host Overview](#/dashboard/system-79ffd6e0-faa0-11e6-947f-177f697178b8-ecs) |
**[Containers overview](#/dashboard/system-CPU-slash-Memory-per-container-ecs)**
----

[discrete]
=== Target system name

Currently we don't make it a rule to show on a dashboard what system it's designed to monitor. The only way to see it is through the dashboard name.

// screenshot omitted for now

When using multiple dashboards on bigger screens, it makes it hard to distinguish between the dashboards. You can improve this by using the Markdown control to display the target system the dashboard is used for.

[discrete]
=== Naming

When building dashboards, use the following naming convention.

[discrete]
==== Visualizations

[source,text]
----
<NAME> [<Metrics | Logs> <PACKAGE NAME>]
----

Examples:

* Memory Usage Gauge [Metrics System]
* New groups [Logs System]

Rename all visualizations added to a dashboard only to show the <NAME> part.

// screenshot omitted for now

[discrete]
==== Dashboards

[source,text]
----
[<Metrics | Logs> <PACKAGE NAME>] <Name>
----

Examples:

* [Metrics System] Host overview
* [Metrics MongoDB] Overview

[discrete]
=== Screenshots

Letter casing is important for screenshot descriptions.
Descriptions are shown in the {kib} UI, so try and keep them clean and consistent.

These descriptions are visualized in the {kib} UI. It would be better experience to have them clean and consistent.

// lint ignore ec2
* Bad candidate: filebeat running on ec2 machine
* Good candidates: {filebeat} running on AWS EC2 machine

[discrete]
== Exporting

// move to new page

// add https://www.elastic.co/guide/en/beats/devguide/current/export-dashboards.html

[source,terminal]
----
elastic-package export
----

[[build-it]]
== Build

To format, lint, and build your integration, in that order, run:

[source,terminal]
----
elastic-package check
----

Problems and potential solutions will display in the console.
Fix them and rerun the command.
Alternatively,
skip formatting and linting with the `build` command:

[source,terminal]
----
elastic-package build
----

With the package built, run the following command from inside of the integration directory to recycle the package-registry docker container.
This refreshes the {fleet} UI, allowing it to pick up the new integration in {kib}.

[source,terminal]
----
elastic-package stack up --services package-registry
----

[[testing-and-validation]]
== Testing and validation

. Build the package you'd like to verify (e.g. `apache`):
+
[source,terminal]
----
cd apache
elastic-package build
----

. Start the testing environment:
+
Run from inside the Integrations repository:
+
[source,terminal]
----
elastic-package stack up -d -v
----
+
The command above will boot up the {stack} ({es}, {kib}, and {package-registry}) using Docker containers.
It rebuilds the {package-registry} Docker image using packages built in step 1. and boots up the {package-registry}.
+
To reload the already deployed {package-registry}, use the following command:
+
[source,terminal]
----
elastic-package stack up -v -d --services package-registry
----

. Verify that your integration is available in the correct version. For example, MySQL: http://localhost:8080/search?package=mysql (use `experimental=true` parameter if the package is in experimental version.
Alternatively set `release` to `beta` or higher in your package's `manifest.yml`, if appropriate.)
+
[source,json]
----
[
  {
    "description": "MySQL Integration",
    "download": "/epr/mysql/mysql-0.0.1.tar.gz",
    "icons": [
      {
        "src": "/package/mysql/0.0.1/img/logo_mysql.svg",
        "title": "logo mysql",
        "size": "32x32",
        "type": "image/svg+xml"
      }
    ],
    "name": "mysql",
    "path": "/package/mysql/0.0.1",
    "title": "MySQL",
    "type": "integration",
    "version": "0.0.1"
  }
]
----
+
The `elastic-package stack` provides an enrolled instance of the {agent}. Use that one instead of a local application
if you can run the service (you're integrating with) in the Docker network and you don't need to rebuild the Elastic-Agent
or it's subprocesses (e.g. {filebeat} or {metricbeat}). The service Docker image can be used for <<system-testing,system testing]. If you prefer to use a local instance of the {agent}, proceed with steps 4 and 5:

. (Optional) Download the https://www.elastic.co/downloads/elastic-agent[{agent}].
// lint ignore fleet ingest-manager
. (Optional) Enroll the {agent} and start it:
+
Use the "Enroll new agent" option in the {kib} UI (Ingest Manager -> Fleet -> Create user and enable Fleet) and run a similar command:
+
[source,terminal]
----
./elastic-agent enroll http://localhost:5601/rel cFhNVlZIRUIxYjhmbFhqNTBoS2o6OUhMWkF4SFJRZmFNZTh3QmtvR1cxZw==
./elastic-agent run
----
+
The `elastic-agent` starts two other processes: `metricbeat` and `filebeat`.

. Run the product you're integrating with (e.g. a docker image with MySQL).

. Install package.
+
Click out the configuration in the {kib} UI, deploy it and wait for the agent to pick out the updated configuration.

. Navigate with {kib} UI to freshly installed dashboards, verify the metrics/logs flow.

=== Use test runners

`elastic-package` provides different types of test runners.
See <<testing>> to learn about the various methods for testing packages.

The `test` subcommand requires a reference to the live {stack}. You can define service endpoints using environment variables.
If you're using the {stack} created with `elastic-package`, you can use export endpoints with `elastic-package stack shellinit`:

[source,terminal]
----
$ eval "$(elastic-package stack shellinit)"
----

To preview environment variables:

[source,terminal]
----
$ elastic-package stack shellinit
export ELASTIC_PACKAGE_ELASTICSEARCH_HOST=http://127.0.0.1:9200
export ELASTIC_PACKAGE_ELASTICSEARCH_USERNAME=elastic
export ELASTIC_PACKAGE_ELASTICSEARCH_PASSWORD=changeme
export ELASTIC_PACKAGE_KIBANA_HOST=http://127.0.0.1:5601
----

=== Review test coverage

The `elastic-package` tool can calculate test coverage for packages and export coverage reports in the link:https://cobertura.github.io/cobertura/[Cobertura] format.
Coverage reports contain information about present or missing pipelines, and system and static tests, so they help in identifying untested integrations. For pipeline tests, it features detailed source-code coverage reports
highlighting the ingest processors that are covered during testing.

The CI job runner collects coverage data and stores them together with build artifacts. The Cobertura plugin (*Coverage Report* tab) uses this data to visualize test coverage grouped by package, data stream, and test type.

// DK: This link doesn't work for me so I've commented it out. If anyone knows what the link should be, please re-add.
//See link:https://fleet-ci.elastic.co/job/Ingest-manager/job/integrations/job/main/cobertura/[test coverage report for the *main* branch].

=== Cobertura format vs. package domain language

As the Cobertura report format refers to packages, classes, methods, and such, unfortunately it doesn't map easily onto the packages domain. We have decided to make a few assumptions for the Cobertura classification:

* **Package**: `integration``
* **File**: `data stream``
* **Class**: test type (`pipeline tests`, `system tests`, etc.)
* **Method**: "OK" if there are any tests present.

For pipeline tests, which include actual source-code coverage, the mapping is different:

* **Package**: `integration.data_stream`
* **File**: Path to ingest pipeline file
* **Class**: Ingest pipeline name
* **Method**: Ingest processor

[[finishing-touches]]
== Finishing touches

// https://github.com/elastic/integrations/blob/main/docs/fine_tune_integration.md

=== Words

Tips for manifest files:

* Descriptions of configuration options should be as short as possible.
+
Remember to keep only the meaningful information about the configuration option.
+
** Good candidates: references to the product configuration, accepted string values, explanation.
** Bad candidates: Collect metrics from A, B, C, D,... X, Y, Z datasets.

// lint disable foo_bar3
* Descriptions should be human readable.
+
Try to rephrase sentences like: Collect foo_Bar3 metrics, into Collect Foo Bar metrics.
// lint enable foo_bar3

* Descriptions should be easy to understand.
+
Simplify sentences, don't provide information about the input if not required.
+
** Bad candidate: Collect application logs (log input)
** Good candidates: Collect application logs, Collect standard logs for the application

=== Add an icon

The integration icons are displayed in different places in {kib}, hence it's better to define custom icons to make the UI easier to navigate.

=== Add screenshots

The {kib} Integration Manager shows screenshots related to the integration. Screenshots include {kib} dashboards visualizing the metric and log data.

=== Create a README file

The README template is used to render the final README file, including exported fields. The template should be placed in the `package/<integration-name>/_dev/build/docs/README.md`. If the directory doesn't exist, please create it.

To see how to use template functions, for example {{fields "data-stream-name"}}, review the MySQL docs template. If the same data stream name is used in both metrics and logs, please add -metrics and -logs in the template. For example, ELB is a data stream for log and also a data stream for metrics. In README.md template, {{fields "elb_logs"}} and {{fields "elb_metrics"}} are used to separate them.

=== Review artifacts

[[define-variable-properties]]
=== Define variable properties

The variable properties customize visualization of configuration options in the {kib} UI. Make sure they're defined in all manifest files.

[source,yaml]
----
vars:
  - name: paths
    required: true <1>
    show_user: true <2>
    title: Access log paths <3>
    description: Paths to the apache access log file. <4>
    type: text <5>
    multi: true <6>
    hide_in_deployment_modes: <7>
      - agentless
    default:
      - /var/log/httpd/access.log*
----
<1> option is required
<2> don't hide the configuration option (collapsed menu)
<3> human readable variable name
<4> variable description (may contain some details)
<5> field type (according to the reference: text, password, bool, integer)
<6> the field has multiple values
<7> hides the variable in agentless mode (see <<hide_in_deployment_modes>> for more information)

// === Add sample events

[[tips-for-building]]
== Tips for building integrations

The section offers a set of tips for developers to improve integrations that they're working on. It combines hints, guidelines,
recommendations and tricks. Please consider this section as a live document that may evolve in the future, depending
on the business or technical requirements for the entire platform (Elastic Package Registry, Elastic Agent and Kibana).

=== elastic-package

https://github.com/elastic/elastic-package[elastic-package] is a command line tool, written in Go, used for developing Elastic packages. It can help you lint,
format, test and build your packages. This is the official builder tool to develop Integrations. See the
https://github.com/elastic/elastic-package#getting-started[Getting started] section to ramp up quickly and review its features.

If you need the revision of elastic-package in the correct version (the same one as the CI uses), which is defined in `go.mod`, use the following command
(in the Integrations repository):

[,bash]
----
$ go build github.com/elastic/elastic-package
$ ./elastic-package help
----

=== New integrations

==== Manifest files

. Set the initial version to `0.1.0`.
+
Tagging the integration with a lower version, like `0.0.1`, means that it's at very early stage and most likely
it doesn't work at all. It might be partially developed.

. Select one or two categories for the integration.
+
The list of available categories is present in the Package Registry source: https://github.com/elastic/package-registry/blob/1dd3e7c4956f7e34809bb87acae50b2a63cd7ad0/packages/package.go#L29-L55

. Make sure that the version condition for Kibana is set to `+^7.10.0+` and not `>=7.10.0`. Otherwise the package is also in 8.0.0 but we do not know today if it will actually be compatible with >= 8.0.0.
+
[,yaml]
----
conditions:
  kibana.version: '^7.10.0'
----

. Set the proper package owner (either Github team or personal account)
+
Good candidates for a team: `elastic/integrations`, `elastic/security-service-integrations`
+
Update the `.github/CODEOWNERS` file accordingly.

=== All integrations

==== Development

. When you're developing integrations and you'd like to propagate your changes to the package registry, first rebuild the package:
+
[,bash]
----
$ cd packages/apache
$ elastic-package build
----
+
Then, rebuild and redeploy the Package Registry:
+
_It's important to execute the following command in the Integrations repository._
+
[,bash]
----
$ elastic-package stack up -v -d --services package-registry
----
+
Explanation: it's much faster to rebuild and restart the container with the Package Registry, than work with
mounted volumes.

==== Code reviewers

. Ping "Team:Integrations".
+
Use the team label to notify relevant team members about the incoming pull request.

===== Manifest files

. Descriptions of configuration options should be as short as possible.
+
Remember to keep only the meaningful information about the configuration option.
+
Good candidates: references to the product configuration, accepted string values, explanation.
+
Bad candidates: _Collect metrics from A, B, C, D,... X, Y, Z datasets._

. Descriptions should be human readable.
+
Try to rephrase sentences like: _Collect foo_Bar3 metrics_, into _Collect Foo Bar metrics_.

. Description should be easy to understand.
+
Simplify sentences, don't provide information about the input if not required.
+
Bad candidate: _Collect application logs (log input)_
+
Good candidates: _Collect application logs_, _Collect standard logs for the application_

. Letter casing is important for screenshot descriptions.
+
These descriptions are visualized in the Kibana UI. It would be better experience to have them clean and consistent.
+
Bad candidate: _filebeat running on ec2 machine_
+
Good candidates: _Filebeat running on AWS EC2 machine_

. If package relies on some feature or a field, available only in a specific stack or beats version, `kibana.version` condition should be adjusted accordingly in the package's `manifest.yml`:
+
[,yaml]
----
conditions:
   kibana.version: '^8.7.0'
----
+
NOTE: The package version with such condition as above will be only available in Kibana version >=8.7.0
+
NOTE: Changing dashboards and visualizations using an unreleased version of Kibana might be unsafe since the Kibana Team might make changes to the Kibana code and potentially the data models. There is no guarantee that your changes won't be broken by the time new Kibana version is released.

===== CI

. Run `elastic-package check` and `elastic-package test` locally.
+
If you want to verify if your integration works as intended, you can execute the same steps as CI:
+
[,bash]
----
$ cd packages/apache
$ elastic-package check -v
$ elastic-package test -v
----
+
Keep in mind that the `elastic-package test` command requires a live cluster running and exported environment variables.
The environment variables can be set with `eval "$(elastic-package stack shellinit)"`.

===== Fields

. Remove empty fields files.
+
If you notice that fields file (e.g. `package-fields.yml`) doesn't contain any field definitions or it defines root only,
feel free to remove it.
+
Bad candidate:
+
[,yaml]
----
- name: mypackage.mydataset
  type: group
----
