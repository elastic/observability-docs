[[integrations-tsds-synthetic-source]]
= Working with new indexing features

These pages include details for incorporating new indexing features into your integrations, such as time series data stream (TSDS), `doc-value-only` fields, and synthetic source.

* <<developer-tsds-guidelines>>
* <<testing-new-indexing-features>>

[[developer-tsds-guidelines]]
== TSDS guidelines

This page describes how to enable TSDS functionality in your integration packages. Full details about TSDS can be found in {ref}/tsds.html[Time series data stream] in the {es} documentation. 

In this document you can find:

* <<integrations-dev-tsds-background>>
* <<integrations-dev-tsds-migrating>>
* <<integrations-dev-tsds-testing>>
* <<integrations-dev-tsds-best-practices>>
* <<integrations-dev-tsds-troubleshooting>>

[discrete]
[[integrations-dev-tsds-background]]
=== Background

A time series is a sequence of observations for a specific entity. TSDS enables the column-oriented functionality in elasticsearch by co-locating the data and optimizing the storage and aggregations to take advantage of such co-allocation.

Integrations are one of the biggest sources of input data to Elasticsearch. Enabling TSDS on integration packages can be achieved by minimal changes made in the `fields.yml` and `manifest.yml` files of a package.

[discrete]
[[integrations-dev-tsds-migrating]]
=== Steps for enabling TSDS for a metrics dataset

IMPORTANT: Datastreams having type `logs` are excluded from TSDS migration.

[discrete]
=== Step 1: Set the dimension fields

Each field belonging to the set of fields that uniquely identify a document is a dimension. For more details, refer to {ref}/tsds.html#time-series-dimension[Dimensions].

To set a field as a dimension simply add `dimension: true` to its mapping:

[source,yaml]
----
- name: ApiId
  type: keyword
  dimension: true
----

[NOTE] 
==== 
A field having type {ref}/flattened.html[flattened] cannot be selected as a dimension field. If the field that you are choosing as a dimension is too long or is of type flattened, consider hashing the value of this field and using the result as a dimension. {ref}/fingerprint-processor.html[Fingerprint processor] can be used for this purpose.

You can find an example in link:https://github.com/elastic/integrations/blob/8a57d6ba96d391afc33da20c80ec51280d22f009/packages/oracle/data_stream/performance/elasticsearch/ingest_pipeline/default.yml#LL127C4-L131C29[Oracle Integration TSDS Enablement Example]
====

Important considerations:

* There is a limit on how many dimension fields a datastream can have. By default, this value is `21`. You can adjust this restriction by altering the `index.mapping.dimension_fields.limit`:
+
[source,yaml]
----
elasticsearch:
  index_template:
    settings:
      index.mapping.dimension_fields.limit: 32 # Defaults to 21
----
+
* Dimension keys have a hard limit of 512b. Documents are rejected if this limit is reached.
* Dimension values have a hard limit of 1024b. Documents are rejected if this limit is reached.

[discrete]
==== ECS fields

There are fields that are part of every package, and they are potential candidates for becoming dimension fields:

* `host.name`
* `service.address`
* `agent.id`
* `container.id`

For products that are capable of running both on-premise and in a public cloud environment (by being deployed on public cloud virtual machines), it is recommended to annotate the ECS fields listed below as dimension fields:

* `host.name`
* `service.address`
* `container.id`
* `cloud.account.id`
* `cloud.provider`
* `cloud.region`
* `cloud.availability_zone`
* `agent.id`
* `cloud.instance.id`

For products operating as managed services within cloud providers like AWS, Azure, and GCP, it is advised to label the fields listed below as dimension fields:

* `cloud.account.id`
* `cloud.region`
* `cloud.availability_zone`
* `cloud.provider`
* `agent.id`

Note that for some packages some of these fields do not hold any value, so make sure to only use the needed ones.

[discrete]
==== Integration specific fields

The `files.yml` file has the field mappings specific to a datastream of an integration. Some of these fields might need to be set as a dimension if the set of dimension fields in ECS is not enough to create a unique {ref}/tsds.html#tsid[`_tsid`].

Adding an inline comment prior to the dimension annotation is advised, detailing the rationale behind the choice of a particular field as a dimension field:

[source,yaml]
----
```
- name: wait_class
  type: keyword
  # Multiple events are generated based on the values of wait_class. Hence, it is a dimension
  dimension: true
  description: Every wait event belongs to a class of wait events.
```
----

[discrete]
=== Step 2: Set type for metric fields

Metrics are fields that contain numeric measurements, as well as aggregations and/or down sampling values based off of those measurements. Annotate each metric with the correct metric type. The {ref}/tsds.html#time-series-metric[currently supported values] are `gauge`, `counter`, and `null`.

Example of adding a metric type to a field:

[source,yaml]
----
- name: compactions_failed
  type: double
  metric_type: counter
  description: |
    Counter of TSM compactions by level that have failed due to error.
----

NOTE: Some of the aggregation functions are not supported for certain `metric_type` values. In such a scenario, please revisit to see if the selection of `metric_type` you made is indeed correct for that field. If valid, please create an issue in link:https://github.com/elastic/elasticsearch[elastic/elasticsearch] explaining the use case.

[discrete]
=== Step 3: Update Kibana version

Modify the `kibana.version` to at least `8.8.0` in the `manifest.yml` file of the package:

[source,yaml]
----
conditions:
 kibana.version: "^8.8.0"
----

[discrete]
=== Step 4: Enable `time_series` index mode

Add the changes to the `manifest.yml` file of the datastream as shown to enable the timeseries index mode:

[source,yaml]
----
elasticsearch:
  index_mode: "time_series"
----

[discrete]
[[integrations-dev-tsds-testing]]
=== Testing

* If the number of dimensions is insufficient, we will have loss of data. Consider testing this using the link:https://github.com/elastic/TSDB-migration-test-kit[TSDS migration test kit].
* Verify the dashboard is rendering the data properly. If certain visualisation do not work, consider migrating to {kibana-ref}/lens.html[Lens]. Remember that certain aggregation functions are not supported when a field has metric type `counter`, for example, `avg()`. Replace such aggregation functions with a supported aggregation type such as `max()` or `min()`.

[discrete]
[[integrations-dev-tsds-best-practices]]
=== Best practices

* Use {kibana-ref}/lens.html[Lens] as the preferred visualisation type.

* Always assess the number of unique values the field that is selected to be a dimension would hold, especially if it is a numeric field. A field that holds millions of unique values may not be an ideal candidate for becoming a dimension field.

* If the dimension field value length is very long (max limit is 1024B), consider transforming the value to hash value representation. {ref}/fingerprint-processor.html[Fingerprint processor] can be used for this purpose.

* In the field mapping files above each dimension field, add in-line comments stating the reason for selecting the field as a dimension field.

* As part of TSDS migration testing, you may discover other errors which may be unrelated to TSDS migration. Keep the pull request for TSDS migration free from such changes. This helps in obtaining quick PR approval.

[discrete]
[[integrations-dev-tsds-troubleshooting]]
=== Troubleshooting

[discrete]
==== Dropped documents

In the event that after enabling TSDS you notice that metrics data is being dropped from an index, the link:https://github.com/elastic/TSDB-migration-test-kit[TSDS test migration kit] can be used as a helpful debugging tool.

[discrete]
==== Conflicting field type

Fields having conflicting field type will not be considered as dimension. Resolve the field type ambiguity before defining a field as dimension field.

[discrete]
==== Identification of write index

When mappings are modified for a datastream, index rollover happens and a new index is created under the datastream. Even if there exists a new index, the data continues to go to the old index until the timestamp matches `index.time_series.start_time` of the newly created index.

An link:https://github.com/elastic/kibana/issues/150549[enhancement request] for Kibana is created to indicate the write index. Until then, refer to the index.time_series.start_time of indices and compare with the current time to identify the write index.

If you find this error (for reference, see link:https://github.com/elastic/integrations/issues/7345[integrations issue #7345] and link:https://github.com/elastic/elasticsearch/pull/98518[elasticsearch PR #98518]):

[source,console]
----
... (status=400): {"type":"illegal_argument_exception","reason":"the document timestamp [2023-08-07T00:00:00.000Z] is outside of ranges of currently writable indices [[2023-08-07T08:55:38.000Z,2023-08-07T12:55:38.000Z]]"}, dropping event!
----

Consider:

. Defining the `look_ahead` or `look_back_time` for each data stream. For example:
+
[source,yaml]
----
elasticsearch:
  index_mode: "time_series"
  index_template:
    settings:
      index.look_ahead_time: "10h"
----
+
NOTE: Updating the package with this does not cause an automatic rollover on the data stream. You have to do that manually.
+
. Updating the `timestamp` of the document being rejected.
. Finding a fix to receive the document without a delay.

[[testing-new-indexing-features]]
== How to test new indexing features

Elasticsearch has been adding new indexing modes and features that allow optimization of storage size and query performance.

We'd like to enable integration developers to start testing the ingest and query performance of enabling these features before we start making any changes in the integrations themselves or allowing end users to enable these from the Fleet UI.

Today, each of these can already be enabled by leveraging the `*@custom` component templates that Fleet installs for each integration data stream, to varying degrees of ease of use (details below). We could improve the UX around this for integration developers by adding an explicit API in Fleet to enable this, however it may not be necessary. See link:https://github.com/elastic/kibana/issues/132818[elastic/kibana#132818] for discussion around how a feature flag API could be added to ease this a bit more.

See the following instructions for testing new indexing features:

* <<integrations-dev-synthetic-source>>
* <<integrations-dev-doc-value-only-fields>>
* <<integrations-dev-test-tsds>>

[[integrations-dev-synthetic-source]]
=== Testing synthetic source

* For background, refer to link:elastic/elasticsearch#85649[#85649]
* For integrations support, refer to link:elastic/package-spec#340[#340]

This feature is quite easy to enable on an integration using the component template. Here's how to do this for the `nginx` substatus metrics, for example:

. Install the nginx package.
. Run this dev tools command:
+
[source,console]
----
PUT /_component_template/metrics-nginx.substatus@custom
{
  "template": {
    "settings": {},
    "mappings": {
      "_source": {
        "mode": "synthetic"
      }
    }
  },
  "_meta": {
    "package": {
      "name": "nginx"
    }
  }
}
----

. If a data stream already exists, rollover the data stream to get the new mappings: `POST metrics-nginx.substatus-default/_rollover`

One challenge with leveraging synthetic source is that it doesn't support keyword fields that have `ignore_above` configured. It may be worth removing this setting for testing on those fields. This can be done by editing the package in `dev` and installing it via `elastic-package` or overriding it via the custom component template, similar to the <<integrations-dev-doc-value-only-fields,`doc-value-only`>> example.

[[integrations-dev-doc-value-only-fields]]
=== Testing `doc-value-only` fields

* For background, refer to link:https://www.elastic.co/blog/whats-new-elasticsearch-kibana-cloud-8-1-0[Elasticsearch, Kibana, Elastic Cloud 8.1: Faster indexing, less disk storage, and smarter analytics capabilities].
* For integrations support, refer to link:https://github.com/elastic/integrations/issues/3419[#3419].

This feature is  more challenging with component templates because it requires adding `index: false` to every long and double field. Providing an API in Fleet would make this a bit easier. Here's how to do this manually:

. Install the `nginx` package.
. Get the mappings included with the package: `GET /_component_template/logs-nginx.access@package`.
. Copy the output into your favorite text editor, search for each `"type": "long"` and `"type": "double"`, and add `"index": false`.
. Update the custom component template with the new mappings. For example, here's how to set the long fields to `index: false`:
+
[source,console]
----
PUT /_component_template/merics-nginx.substatus@custom
{
  "template": {
    "settings": {},
    "mappings": {
      "properties": {
        "nginx": {
          "properties": {
            "stubstatus": {
              "properties": {
                "hostname": {
                  "ignore_above": 1024,
                  "type": "keyword"
                },
                "current": {
                  "type": "long",
                  "index": false
                },
                "waiting": {
                  "type": "long",
                  "index": false
                },
                "accepts": {
                  "type": "long",
                  "index": false
                },
                "handled": {
                  "type": "long",
                  "index": false
                },
                "writing": {
                  "type": "long",
                  "index": false
                },
                "dropped": {
                  "type": "long",
                  "index": false
                },
                "active": {
                  "type": "long",
                  "index": false
                },
                "reading": {
                  "type": "long",
                  "index": false
                },
                "requests": {
                  "type": "long",
                  "index": false
                }
              }
            }
          }
        }
      }
    }
  },
  "_meta": {
    "package": {
      "name": "nginx"
    }
  }
}
----

. If a data stream already exists, rollover the data stream to get the new mappings: `POST metrics-nginx.substatus-default/_rollover`

[[integrations-dev-test-tsds]]
=== Time-series indexing (TSDS)

* For background, refer to link:https://github.com/elastic/elasticsearch/issues/74660[#74660]
* For integrations support, refer to link:https://github.com/elastic/package-spec/issues/311[#311]

Usage of TSDS indexing requires the following:

* Mapping parameters must be added for `time_series_dimension` and `time_series_metric` on appropriate fields. This is already supported by the package ecosystem and Fleet, so packages can already define these options.
* The `mode: time_series` and `routing_path` index settings must be added, this can be done by editing the custom component template.

Note that the `routing_path` setting should correspond to fields with `time_series_dimension` specified. In the future, ES may automate this setting.

. Install the kubernetes package (already has TSDS mappings set up)
. Run this dev tools command:
+
[source,console]
----
PUT /_component_template/metrics-kubernetes.pod@custom
{
  "template": {
    "settings": {
      "index.mode": "time_series",
      "index.routing_path": ["kubernetes.pod.uid"]
    },
    "mappings": {}
  },
  "_meta": {
    "package": {
      "name": "kubernetes"
    }
  }
}
----

. If a data stream already existed, rollover the data stream to get the new mappings: `POST metrics-kubernetes.pod-default/_rollover`
