[[what-is-an-integration]]
= What is an integration?

An Elastic integration is a collection of assets that defines how to observe a specific product or service with the {stack}:

* Data ingest, storage, and transformation rules
* Configuration options
* Pre-built, custom dashboards and visualizations
* Documentation
* Tests

Integrations have a strict, well-defined structure, and offer a number of benefits over other ingest options:

* Structured around the service that is being observed--not the monitoring agent
* Easy, less error-prone configuration
* Fewer monitoring agents for users to install
* Deploy in just a few clicks
* Decoupled release process from the {stack}

[discrete]
[[how-integrations-work]]
== Integration lifecycle

. Create a source package
+
All integrations start as a source package.
You'll find most Elastic integrations in the https://github.com/elastic/integrations[`elastic/integrations`] repository,
but a package can live anywhere.
+
All packages must adhere to the <<package-spec,package specification>> -- a formal spec used for the creation and validation of new or updated integrations.

. Publish the integration to the package registry
+
Once an integration (package) has been created, it needs to be built. Built integrations are stored in the https://github.com/elastic/package-storage[Package Storage repository] and served up via the https://github.com/elastic/package-registry[{package-registry}].
The {fleet} UI in {kib} connects to the {package-registry} and allows users to discover, install, and configure Elastic Packages.
The {package-registry} can also be {fleet-guide}/air-gapped.html#air-gapped-diy-epr[deployed on-premise in air-gapped environments].

. Install the integration
+
Using {fleet} in {kib}, install the integration and add it to an {agent} policy.
When you install a package, its assets are unpacked and installed into {es} and {kib} using {stack} APIs.
In addition, configuration for the package is persisted in {es} as an {agent} policy.

. Add the policy with the integration to an {agent}.
+
Once the policy with an integration is added to an {agent},
the {agent} will begin to collect and ship data to the {stack} based on the Elastic integration.
+
Package assets may come into play here. For example, if a package installed ingest pipelines,
those will intercept the data and transform it before it is indexed.

. Visualize the results
+
Integrations can and should ship with custom dashboards and visualizations that are installed with the integration.
Use these for a tailored view of your {observability} data.

[[integration-definitions]]
== Definitions

[discrete]
=== Package

An Elastic Package, or simply package for short, contains the dashboards, visualisations, and configurations to monitor the logs and metrics of a particular technology or group of related services, such as “MySQL”, or “System”.

The package consists of:

* Name
* Zero or more dashboards and visualisations and Canvas workpads
* Zero or more ML job definitions
* Zero or more data stream index templates

The package is versioned.

[discrete]
=== Integration

An integration is a specific type of a package defining data streams used to observe a product using logs, metrics, and traces.

[discrete]
=== Data stream

A data stream is logical sub-division of an Integration package, dealing with a specific type of observable aspect of the service or product being observed. For example, the `mysql` package defines a data stream for collecting metrics and another data stream for collecting server logs.

A data stream defines all the assets needed to create an Elasticsearch data stream, for example: index templates and ingest pipelines. These assets are loaded into Elasticsearch when a user installs a package via the Fleet UI in Kibana.

A data stream also defines a policy template. Policy templates include variables that allow users to configure the data stream via the Fleet UI in Kibana. The resulting policy is interpreted by the Elastic Agent to collect relevant information from the product or service being observed.

Data streams are defined inside the `data_stream` folder located under the package's root directory. Each data stream is defined in it's own sub-folder.

The data stream consists of:

* Field definitions (`fields.yml` files)
* Zero or more ingest pipelines
* An Elastic Agent policy template

[discrete]
=== Development Extensions: `_dev` directories

The `_dev` directory is part of the link:https://github.com/elastic/package-spec[package-spec], and contains development resources. These development resources cover any types of files or folders needed only at development time. This includes resources needed for testing, but also includes any templates that might be used for generating documentation. In the future it could include other files or folders needed just at development time. It can be defined on the following levels:

. The package-level `_dev` folder contains files needed to set up the testing environment for that package. This environment setup is specified by files and folders in the `_dev/deploy` folder. For example, the `apache` package link:https://github.com/elastic/integrations/tree/main/packages/apache/_dev/deploy[specifies] how to spin up an Apache Docker container for testing.
. The data stream-level `_dev` folder contains test configuration files for various types of tests. For example, see the link:https://github.com/elastic/integrations/tree/main/packages/apache/data_stream/error/_dev/test[`_dev/test folder`] under the `apache/error` data stream.
The integrations have also link:https://github.com/elastic/elastic-package/blob/main/docs/howto/asset_testing.md[asset] and link:https://github.com/elastic/elastic-package/blob/main/docs/howto/static_testing.md[static] tests. They don't require config files, but configs can be used to mark them as optional.


