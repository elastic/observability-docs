include::{docs-root}/shared/versions/stack/{source_branch}.asciidoc[]
include::{docs-root}/shared/attributes.asciidoc[]

:doctype: book
:forum: https://discuss.elastic.co/

:package-spec-repo: https://github.com/elastic/package-spec

// Access package spec files with {package-spec-root}
:package-spec-version: 1
:package-spec: {package-spec-root}/versions/{package-spec-version}

= Integrations Developer Guide

// Included file description
// include::file-name.asciidoc[leveloffset=+1]

// Build these docs
// $GIT_HOME/docs/build_docs --doc $GIT_HOME/observability-docs/docs/en/integrations/index.asciidoc --resource $GIT_HOME/package-spec/versions --chunk 1 --open

== What is an integration?

// Ingest Observability data from popular services into the Elastic Stack
// https://github.com/elastic/integrations
// https://github.com/elastic/integrations/blob/master/docs/definitions.md
// This needs WORK

An Elastic integration is a collection of assets that defines how to observe a specific product with the Elastic Stack.
Integrations offer a number of benefits over other Observability options:

* Structured around the service that is being observed--not the monitoring agent
* Easy, less error-prone configuration
* Fewer monitoring agents for users to install
* Deploy in just a few clicks
* Decoupled release process from the Elastic Stack

The assets contained in an integration can define every step of the observability process:

* Data transformation, ingestion, and storage.
* Data visualization
* Elastic Agent configuration
* Documentation
* Tests

Integrations have a strict, well-defined structure.
This structure is described by the package specification.

[discrete]
=== Why build an integration?

* You want to observe _some_ service
* You observe that service by ingesting metrics and/or logs from it
* You want to visualize this data in a meaningful way

== Integration lifecycle

. Create a source package
+
All integrations start as a source package.
You'll find most Elastic integrations in the `elastic/integrations` repo,
but a package can live anywhere.

. Publish to the package registry
+
When an integration is ready to be published, bump the version in the manifest file
and publish the package to the Elastic Package Registry (EPR) using `elastic-package`
// the three stages of EPR reflect the maturity of a package
// snapshot
// staging
// production

. Install the package
+
After a package is published in the EPR, it is downloaded as a .zip file from the package-registry by Fleet inside Kibana.
The package is now ready to be installed.
+
Click **add integration** to install the integration and add it to an Elastic Agent policy.
When you install a package, its assets are unpacked and installed into Elasticsearch and Kibana using stack APIs.
In addition, configuration for the package is persisted in Elasticsearch as an Elastic Agent policy.

. Add the policy with the integration to an Elastic Agent
+
Once the policy with an integration is added to an Elastic Agent,
the agent will begin to collect and ship data to the Elastic Stack based on the Elastic integration.
+
Package assets may come into play here. For example, if a package installed ingest pipelines,
those will intercept the data, and transform it before it is indexed.

. Visualize
+
Integrations can ship with custom dashboards and visualizations that are installed with the integration.
Use these for a tailored view at your Observability data.

[discrete]
=== Repositories

// While this repository contains sources for Elastic Integrations, built Elastic Integrations are stored in the Package Storage repository and served up via the Package Registry. The Fleet UI in Kibana connects to the Package Registry and allows users to discover, install, and configure Elastic Packages.

[discrete]
==== Package specification

* Formal spec of what an Elastic package is
* What we use for validation of new or updated packages

Repo: https://github.com/elastic/package-spec

Right now it's on V1. Should we pull some of this content into the documentation?
https://github.com/elastic/package-spec/tree/master/versions/1

[discrete]
==== Package storage

* Backing store for the package registry service
* Whatever you see in this repo is what you can find via the package registry service
* 3 branches: snapshot, staging, production

Repo: https://github.com/elastic/package-storage

[discrete]
==== Package registry

* Source code for the package registry server

Repo: https://github.com/elastic/package-registry

[discrete]
==== Integrations

* Where most _Elastic_ packages are kept

Repo: https://github.com/elastic/integrations


== Integration specification

The following folders and files make up an integration:

`changelog.yml`::
The integration's changelog.
+
**required**

`manifest.yml`::
Integration metadata, like version, name, license level, description, category,
icon and screenshot mappings, and policy template definitions.
+
**required**

`_dev`::
Development resources.

**required**

`data_stream`::
Data stream assets, including ingest pipelines, field definitions, metadata, and sample events.

**required**

`docs`::
The built integration readme file.

**required**

// `img`::
// Screenshots of the integration.

`kibana`::
The integration's Kibana assets, like dashboards, visualizations, machine learning modules, etc.

**required**

[discrete]
=== Directory structure

[source,text]
----
apache
│   changelog.yml
│   manifest.yml
└───_dev
└───data_stream
└───docs
└───img
└───kibana
----

[discrete]
=== Spec

[source,yml]
----
include::{package-spec}/spec.yml[]
----

=== `data_stream` directory

==== `_dev/test`

==== `agent/stream`

==== `manifest.yml`

==== `sample_event.json`

=== `_dev` directory

=== `docs` directory

=== `img` directory

=== `kibana` directory

=== `changelog.yml` file

=== `manifest.yml` file


== Integration example: Apache

Consider the Apache integration...

Apache exposes metrics and logs.
Specifically, access logs, error logs, and status metrics.
Each of these different data types is considered a **data stream**.

****
**Data streams** are blah blah blah.

[%collapsible]
.Expand to learn more
====
stuff
====
****

Each data stream is a directory in the `apache/data_stream` directory:

[source,text]
----
apache
└───data_stream
│   └───access
│   └───error
│   └───status
----

Let's choose one of these data streams to dig into--access logs.

[discrete]
=== Ingest pipelines

Access logs (or any logs) should be parsed into structured data prior to ingesting them into Elasticsearch.
**Ingest pipelines** take care of the heavy lifting here.

****
**Ingest pipelines** let you perform common transformations on your data before indexing. For example, you can use pipelines to remove fields, extract values from text, and enrich your data.

A pipeline consists of a series of configurable tasks called processors. Each processor runs sequentially, making specific changes to incoming documents. After the processors have run, Elasticsearch adds the transformed documents to your data stream or index.

[%collapsible]
.Expand to learn more
====
stuff
====
****

Ingest pipelines are defined in the `elasticsearch/ingest_pipeline` directory.
They only apply to the datastream which they live in:

[source,text]
----
apache
└───data_stream
│   └───access
│   │   └───elasticsearch/ingest_pipeline
│   │          default.yml <1>
│   └───error
│   └───status
----
<1> The ingest pipeline for the access logs data stream lives here, in `default.yml`

[discrete]
=== Mappings

Ingest pipelines create fields in an Elasticsearch index, but don't define the fields themselves.
Each field needs a defined data type, or mapping.

****
**Mapping** is the process of defining how a document, and the fields it contains, are stored and indexed.
Each document is a collection of fields, which each have their own data type. When mapping your data, you create a mapping definition, which contains a list of fields that are pertinent to the document. A mapping definition also includes metadata fields, like the _source field, which customize how a document’s associated metadata is handled.

[%collapsible]
.Expand to learn more
====
stuff
====
****

Mappings are defined in the `fields` directory.
They only apply to the datastream which they live in.
The apache integration has four different field definitions:

[source,text]
----
apache
└───data_stream
│   └───access
│   │   └───elasticsearch/ingest_pipeline
│   │   │      default.yml
│   │   └───fields
│   │          agent.yml <1>
│   │          base-fields.yml <2>
│   │          ecs.yml <3>
│   │          fields.yml <4>
│   └───error
│   └───status
----
<1> ??
<2> `base-fields.yml` never changes and is required for all integrations
<3> Defines the relevant ECS fields
<4> Custom apache access log fields ??

[discrete]
=== ECS fields

****
**ECS**

Something about ECS here.

[%collapsible]
.Expand to learn more
====
stuff
====
****


Other things...


== `elastic-package`

`elastic-package` is a command line tool, written in Go, used for developing Elastic packages.
It can help you lint, format, test, build, and promote your packages.

// Currently, elastic-package only supports packages of type Elastic Integrations.

[discrete]
=== Get started

. Download and build the latest master of elastic-package binary:
+
[source,terminal]
----
git clone https://github.com/elastic/elastic-package.git
make build
----
+
TIP: Make sure that you've correctly set up the https://golang.org/doc/gopath_code.html#GOPATH[`$GOPATH` and `$PATH`]
environment variables. `elastic-package` must be accessible from your `$PATH`.

. Change into the directory of the package under development:
+
[source,terminal]
----
cd my-package
----

. Run the `help` command to see available commands
+
[source,terminal]
----
elastic-package help
----

[discrete]
=== Command reference

The following `elastic-package` commands are available.
For more details on a specific command, run `elastic-package help <command>`.

Some commands have a _global context_, meaning that they can be executed from anywhere.
Other commands have a _package context_; these must be executed from somewhere under a package's
root folder, and the command will only operate on the contents of that package.

// *************************
// The following is copied directly from
// https://github.com/elastic/elastic-package/blob/master/README.md
// *************************

[discrete]
==== `elastic-package help`

_Context: global_

Use this command to get a listing of all commands available under `elastic-package` and a brief
description of what each command does.

[discrete]
==== `elastic-package build`

_Context: package_

Use this command to build a package. Currently it supports only the "integration" package type.

Built packages are stored in the "build/" folder located at the root folder of the local Git repository checkout that contains your package folder. The command will also render the README file in your package folder if there is a corresponding template file present in "_dev/build/docs/README.md". All "_dev" directories under your package will be omitted.

Built packages are served up by the Elastic Package Registry running locally (see "elastic-package stack"). If you want a local package to be served up by the local Elastic Package Registry, make sure to build that package first using "elastic-package build".

Built packages can also be published to the global package registry service.

[discrete]
==== `elastic-package check`

_Context: package_

Use this command to verify if the package is correct in terms of formatting, validation and building.

It will execute the format, lint, and build commands all at once, in that order.

[discrete]
==== `elastic-package clean`

_Context: package_

Use this command to clean resources used for building the package.

The command will remove built package files (in build/), files needed for managing the development stack (in ~/.elastic-package/stack/development) and stack service logs (in ~/.elastic-package/tmp/service_logs).

[discrete]
==== `elastic-package create`

_Context: global_

Use this command to create a new package or add more data streams.

The command can help bootstrap the first draft of a package using embedded package template. It can be used to extend the package with more data streams.

For details on how to create a new package, review the [HOWTO guide](https://github.com/elastic/elastic-package/blob/master/docs/howto/create_new_package.md).

[discrete]
==== `elastic-package export`

_Context: package_

Use this command to export assets relevant for the package, e.g. Kibana dashboards.

[discrete]
==== `elastic-package format`

_Context: package_

Use this command to format the package files.

The formatter supports JSON and YAML format, and skips "ingest_pipeline" directories as it's hard to correctly format Handlebars template files. Formatted files are being overwritten.

[discrete]
==== `elastic-package install`

_Context: package_

Use this command to install the package in Kibana.

The command uses Kibana API to install the package in Kibana. The package must be exposed via the Package Registry.

[discrete]
==== `elastic-package lint`

_Context: package_

Use this command to validate the contents of a package using the package specification (see: https://github.com/elastic/package-spec).

The command ensures that the package is aligned with the package spec and the README file is up-to-date with its template (if present).

[discrete]
==== `elastic-package profiles`

_Context: global_

Use this command to add, remove, and manage multiple config profiles.

Individual user profiles appear in ~/.elastic-package/stack, and contain all the config files needed by the "stack" subcommand.
Once a new profile is created, it can be specified with the -p flag, or the ELASTIC_PACKAGE_PROFILE environment variable.
User profiles are not overwritten on upgrade of elastic-stack, and can be freely modified to allow for different stack configs.

[discrete]
==== `elastic-package promote`

_Context: global_

Use this command to move packages between the snapshot, staging, and production stages of the package registry.

This command is intended primarily for use by administrators.

It allows for selecting packages for promotion and opens new pull requests to review changes. Please be aware that the tool checks out an in-memory Git repository and switches over branches (snapshot, staging and production), so it may take longer to promote a larger number of packages.

[discrete]
==== `elastic-package publish`

_Context: package_

Use this command to publish a new package revision.

The command checks if the package hasn't been already published (whether it's present in snapshot/staging/production branch or open as pull request). If the package revision hasn't been published, it will open a new pull request.

[discrete]
==== `elastic-package service`

_Context: package_

Use this command to boot up the service stack that can be observed with the package.

The command manages lifecycle of the service stack defined for the package ("_dev/deploy") for package development and testing purposes.

[discrete]
==== `elastic-package stack`

_Context: global_

Use this command to spin up a Docker-based Elastic Stack consisting of Elasticsearch, Kibana, and the Package Registry. By default the latest released version of the stack is spun up but it is possible to specify a different version, including SNAPSHOT versions.

For details on how to connect the service with the Elastic stack, see the [service command](https://github.com/elastic/elastic-package/blob/master/README.md#elastic-package-service).

[discrete]
==== `elastic-package status [package]`

_Context: package_

Use this command to display the current deployment status of a package.

If a package name is specified, then information about that package is
returned, otherwise this command checks if the current directory is a
package directory and reports its status.

[discrete]
==== `elastic-package test`

_Context: package_

Use this command to run tests on a package. Currently, the following types of tests are available:

[discrete]
===== Asset Loading Tests
These tests ensure that all the Elasticsearch and Kibana assets defined by your package get loaded up as expected.

For details on how to run asset loading tests for a package, see the [HOWTO guide](https://github.com/elastic/elastic-package/blob/master/docs/howto/asset_testing.md).

[discrete]
===== Pipeline Tests
These tests allow you to exercise any Ingest Node Pipelines defined by your packages.

For details on how to configure pipeline test for a package, review the [HOWTO guide](https://github.com/elastic/elastic-package/blob/master/docs/howto/pipeline_testing.md).

[discrete]
===== Static Tests
These tests allow you to verify if all static resources of the package are valid, e.g. if all fields of the sample_event.json are documented.

For details on how to run static tests for a package, see the [HOWTO guide](https://github.com/elastic/elastic-package/blob/master/docs/howto/static_testing.md).

[discrete]
===== System Tests
These tests allow you to test a package's ability to ingest data end-to-end.

For details on how to configure amd run system tests, review the [HOWTO guide](https://github.com/elastic/elastic-package/blob/master/docs/howto/system_testing.md).

[discrete]
==== `elastic-package uninstall`

_Context: package_

Use this command to uninstall the package in Kibana.

The command uses Kibana API to uninstall the package in Kibana. The package must be exposed via the Package Registry.

[discrete]
==== `elastic-package version`

_Context: global_

Use this command to print the version of elastic-package that you have installed. This is especially useful when reporting bugs.

// *************************
// End COPIED CONTENT
// *************************


== Build a new integration (package?)

=== Overview

// This section might not belong here
// https://github.com/elastic/package-spec

==== Asset organization

==== Supported assets

==== Specification format

==== Specification versioning

=== Prerequisites

==== Install `elastic-package`

==== Spin up the Elastic Stack

// Step 1. Bring up the stack.
[source,terminal]
----
elastic-package stack up -v -d
----

=== Create a new package

// internal docs recommend copying a package that already exists.
// need more here

// copy nginx package
[source,terminal]
----
cd packages
cp -r nginx new_package
----

// Review all resources, remove unnecessary ones, adjusts manifests, create new data streams.
// need more here

=== Build

[source,terminal]
----
elastic-package build
----

// recycle the package-registry Docker container (run from inside of the integration directory)
// This allows you to refresh the Fleet UI and pick up the new package in Kibana
[source,terminal]
----
elastic-package stack up --services package-registry
----

=== Lint

// verify the package is aligned with the package-spec
[source,terminal]
----
elastic-package lint
----

// problems and potential solutions will be shown

=== Format

// format the package contents (JSON, YAML)
[source,terminal]
----
elastic-package format
----

=== Test

// https://github.com/elastic/elastic-package/tree/master/docs/howto
// https://github.com/elastic/integrations/blob/master/docs/testing_and_validation.md

=== Final checklist

// https://github.com/elastic/integrations/blob/master/docs/fine_tune_integration.md

==== Add an icon

==== Add screenshots

==== Create a readme file

==== Review artifacts

==== Define variable properties

==== Add sample events

=== Promote (?)

// https://github.com/elastic/integrations/blob/master/docs/developer_workflow_promote_release_integration.md

=== Open a PR (?)

// When the PR is merged, the CI will kick off a build job for the master branch, which can release your integration to the package-storage. It means that it will open a PR to the Package Storage/snapshot with the built integration if only the package version doesn't already exist in the storage (hasn't been released yet).

// When you are ready for your changes in the integration to be released, remember to bump up the package version. It is up to you, as the package developer, to decide how many changes you want to release in a single version. For example, you could implement a change in a PR and bump up the package version in the same PR. Or you could implement several changes across multiple PRs and then bump up the package version in the last of these PRs or in a separate follow up PR.

== Dashboards

// Not sure if this should be a separate section or not
// Kibana dashboards can be exported to local directories
[source,terminal]
----
elastic-package export
----

== Testing and validation

== Pull request review guidelines

== Tutorial: Hello world integration

// A tutorail for getting started from scratch
// Something super simple

/end