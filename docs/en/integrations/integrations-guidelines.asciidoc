[[integrations-guidelines]]
= Integrations guidelines

Refer to the following pages for some tips and recommendations for building integrations.

* <<general-guidelines>>
* <<dashboard-guidelines>>
* <<documentation-guidelines>>

[[general-guidelines]]
== General guidelines

IMPORTANT: The following guidelines capture general aspects of the integrations that can be improved and should not be treated as a mandatory list of requirements every package should adhere to. Some guidelines that are applicable to one integration can be completely irrelevant to another. Treat them as best effort.

While the guidelines focus on metrics, they are equally applicable to logs.

[discrete]
=== Data types

Given that all packages are basic, developers should use Basic types (for example `histogram`. `wildcard`, etc.) when applicable. Of course, for ECS (see below) we should use the type specified by ECS.

[discrete]
=== ECS compliance

An integration package should be compliant with the most recent version of ECS. This implies an increased amount of relevant ECS fields populated by an integration.

Starting with ECS 1.6, ECS is going to start using Basic types for some fields. Integration fields should be upgraded to the new types as part of the process.

[discrete]
=== Document all fields

All fields produced by an integration must be mapped by `fields.yml`. This guarantees that their index mapping is correct, and Kibana has enough information to deal with all fields.

[discrete]
==== Field limits

By default, data streams will have a `total_fields.limit` setting of 1000. Besides defined custom fields, this also includes dynamically generated ECS fields. If your data stream is expected to eventually house more than 1000 fields, set an explicit limit in the `manifest.yml` of the data stream:

[source,yaml]
----
elasticsearch:
  index_template:
    settings:
      index:
        mapping:
          total_fields:
            limit: 5000
----

NOTE: For backwards compatibility, the limit is automatically bumped to 10000 fields if there are more than 500 fields explicitly defined for a data stream, however newly created integrations should not rely on this behavior but instead assume a fixed limit of 1000 fields.

[discrete]
==== Specify metric types and units

As part of the field definition, there are two settings that add metadata which will help Kibana graphing it:

* `unit` applies to all data types, defines the units of the field. Examples of units are `byte` and `ms`. When using `percent` for percentages, the convention is to use 1 for 100%. You can find the full list of supported units in the link:https://github.com/elastic/package-spec/blob/ff8286d0c40ad76bb082e9c8ea78f4551c2519c1/spec/integration/data_stream/fields/fields.spec.yml#L103[package spec].

* `metric_type` applies to metric events only, to be added to metric fields. It defines their metric type. It can be of type `gauge` or `counter`. Counters are used for metrics that always increase over time, such as number of page visits. Gauges are used for amounts that can increase or decrease over time, such as the amount of memory being used.

The Elasticsearch documentation details the {ref}/mapping-field-meta.html[expected values for these two fields].

Other applications, including Kibana, can use the information provided by this metadata when accessing these fields. The `unit` is used when formatting the values of the field, and the `metric_type` can be used to provide better defaults when quering the data.

[discrete]
==== Specify dimensions

A set of fields of a data stream can be defined as dimensions. A set of dimensions
with the same values identify a single time series.

It is important to choose the set of fields carefully. They should be the minimal set
of dimensions required to properly identify any time series included in the data stream. Too few dimensions can mix data of multiple time series into a single one, while too many dimensions can impact performance.

A field can be configured as a dimension by setting `dimension: true` in its
definition.

Only fields of certain data types can be defined as dimensions. These data types
include keywords, IPs and numeric types.

Some guidelines to take into account when chosing dimensions:

* They can affect ingestion performance, it is recommended to have as few dimensions as possible. When selecting dimensions, try to avoid redundant ones, such as unique identifiers and names that refer to the same object.
* Also be careful with having too few dimensions. There can be only one document with the same timestamp for a given set of dimensions. This can lead to data loss if different objects produce the same dimensions.
* Changing dimensions can be a breaking change. A different set of dimensions produces a different time series, even if they select the same data.

Declaring dimensions is a requisite to use TSDB indexes. These indexes are
optimized for time series use cases, bringing disk storage savings and additional
queries and aggregations.

TSDB indexes can be enabled in data streams by setting `elasticsearch.index_mode: time_series` in their manifests.

[discrete]
=== Logs and Metrics UI compatibility

When applicable an integrataion package should provide the relevant fields for the Logs and Metrics Apps. This is especially relevant for integrations that are focused on compute-resources (VMs, containers, etc.). 

* Keep the {observability-guide}/logs-app-fields.html[Logs app fields] reference up to date.
* Keep the {observability-guide}/metrics-app-fields.html[Infrastructure app fields] reference up to date.

[discrete]
=== Subtracting metrics

An integration package should collect a reasonable amount of metrics for any target system. In some cases this may mean removing some metrics that Filebeat and Metricbeat are collecting today. Collecting too many metrics has implications on metric storage as well as relevance of the data provided to the user.

Potential candidates to remove:

* low-level garbage collector metrics
* internal metrics showing code flow (for example, `Got100Continue`, `Wait100Continue`)
* redundant metrics (for example, metric collection for MQ topics doesn't require collection of summary metrics)

[discrete]
=== Relevant metrics

This is probably the most important and hardest one of the guidelinesto satisfy, as it requires knowledge of every target system. Identifying relevant metrics should be considered case by case.

There are no well defined guidelines for this exercise. It can be as simple as finding everything in one place (for example the link:https://www.rabbitmq.com/monitoring.html[RabbitMQ documentation]) or as difficult as reviewing multiple sources including documentation, blog posts, and other integrations, and consolidating the discovered information in one place for revision. A recommendation is to only collect the metrics that are needed for dashboards and visualizations in general.

[discrete]
=== Keep the original message field

Log integrations should keep the original message field (recommended name: `event.original`) so that it shows up in the Logs UI. It will also be useful when users want to reindex the data after changing a pipeline. In addition, the message field can be used as source for the some future Runtime fields.

The original field should be user-configurable with the Kibana UI for better cost and storage management, and also consistency with other integrations.

[discrete]
=== Document storage efficiency

Every integration should strive to store collected data as efficiently as possible, which implies optimizing the way each integration generates documents. 

//TODO: this section would benefit from a separate document describing best practices for storing metrics in Elasticsearch efficiently).

[discrete]
=== Default datasets

When applicable, an integration package should provide a default dataset that aggregates a subset of the most relevant metrics across other data streams. Think of these as the metrics that are visualized on overview dashboards or are used for alerting. A guideline for creating a separate default dataset could be when the number of datasets in a package is more than three.

[discrete]
=== Updated versions

An integration package should support the most relevant versions of a target system. Some of our integrations support older versions of a target service/system, which were relevant at the time of implementation. Over time they can become outdated and require a revision, which can be as simple as testing the integration against the latest version and updating the compatibility section in the documentation, or it can mean refactoring the code to work with the latest version. For example, the Ceph module has recently been updated to support the latest version which had an entirely different way of collecting metrics. In order to accommodate both older and new versions in the module, metricsets were created in the module specifically for newer versions and it was noted in the documentation which metricsets to use.

[discrete]
=== Updated configuration defaults

An integration package should provide meaningful defaults, such as collection intervals (periods), enabled metricsets, and any other integration specific configuration parameters.
In the majority of cases users opt to use defaults. Hence, providing the relevant default values is crucial for the integration to be useful. In addition, integrations should strive to provide a one-click experience by providing the defaults that can cover 80% of use cases.

[discrete]
=== Updated docs

Integration packages should provide consistent and comprehensive documentation.
For more details, refer to the <<documentation-guidelines,documentation guidelines>>.

[discrete]
=== Updated integration content

Integration packages should provide out-of-the-box dashboards.
For more details, refer to the <<dashboard-guidelines,dashboard guidelines>>.

[discrete]
=== Content for elastic.co/integrations

Each integration will be listed on the public website `elastic.co/integrations` and the package registry will serve as the source of truth. As a result, documentation and screenshots should be high quality to showcase the integration. Please ensure to use `svg` for the logo and `png` for all other images. Any additional branding material should be reviewed carefully, for example:

* logo format and quality
* permission to use logos and trademarks

[discrete]
=== Curated user experiences

It's advised to set integration policies in Fleet. Every integration and agent should be visible in Fleet and users should be able to add the integration directly from the integration list. This leads to better cohesion since it provides a consistent experience across integrations, allow users to add several integrations at once, and avoids sending them back and forth between multiple apps. It also allows users to discover new integrations in the list.

Elastic products will also have the option to provide a curated UI for settings that are difficult to put in Fleet. It's up to the product to decide how much flexibility they want to provide in changing the configuration directly from Fleet. This will depend on the use case and if it makes sense. Some level of configuration is recommended though.

[discrete]
=== Asset tagging and metadata

When assets are installed through Fleet some metadata is added by default. 

For Elasticsearch assets such as index templates and ingest pipelines, a `_meta` property is added to the asset as follows:

[source,json]
----
{
  "managed_by": "fleet",
  "managed": true,
  "package": {
    "name": "<package name>"
  }
}
----

For Kibana assets, {kibana-ref}/managing-tags.html[tags] are generated in addition to the `_meta` property:

* One tag with a `name` matching the package's `title` property
* The `managed` tag, which Kibana uses to recognize "system" assets, or those that are installed by Kibana itself instead of generated by an end user


[[dashboard-guidelines]]
== Dashboard guidelines

A {kibana-ref}/dashboard.html[Kibana dashboard] is a set of one or more panels, also referred to as visualizations. Panels display data in charts, tables, maps, and more. Dashboards support several types of panels to display your data, and several options to create panels.

The goal of each integration dashboard is to:

* Provide a way to explore ingested data out of the box.
* Provide an overview of the monitored resources through installing the integration.

Each integration package should contain one or more dashboards.

[discrete]
=== Dashboard Best Practices

Following are recommended best practices for designing Kibana dashboards.

[discrete]
==== Build dashboards on stable versions

Avoid building dashboards on SNAPSHOT versions because as long as the release is not stable behavior changes might render your dashboard unusable. The only supported approach is to use a globally released version from the link:https://www.elastic.co/downloads/past-releases#kibana[official releases list].

[discrete]
==== Not too many visualizations per dashboard

Include only necessary visualizations inside a dashboard, and, when possible, split them across separate dashboards. Linking can be done:

* By using a Markdown visualization to improve performance
* Use {kibana-ref}/drilldowns.html[drilldowns] to connect dashboards where they make sense.

[discrete]
==== Out of date fields in dashboards

The dashboards must be updated to reflect any changes to field names or types. If a pull request updates a field name or type, make sure it is correctly updated in any dashboard the field is being used in.

[discrete]
==== Add visualizations by value, not by reference

Kibana visualizations can be added into a dashboard by value or by reference. Historically, adding by value did not exist. Switching to value has the advantage that the dashboards are fully self contained and only need a single request to be installed.

To achieve this:

* Migrate existing dashboards from `by reference` to `by value`.
* Create new dashboards adding visualizations by value.

A migration script is available to help with the migration: link:https://github.com/elastic/visualizations_integrations_tools[flash1293/legacy_vis_analyzer]

[discrete]
==== Choose the context of your Dashboard

You should always try to understand as much as possible what kind of context your users need to interact with the dashboard. Keep the minimal context needed by answering the following questions:

* Who is going to use this dashboard?
* How much time will the users have?
* What is the main goal of this dashboard and what are any secondary goals?
* What kind of charts can help users identify insights in the most immediate and clear way?

[discrete]
==== Organisation and hierarchy matters in your dashboards

Keep the following guidelines in mind when positioning your elements on dashboards:

* Keep related visualizations close to each other.
+
image::images/grouping-in-visualizations.png[Closely grouped visualizations]
  
* Use Markdown to create blocks of related content.
+
image::images/markdown-grouping.png[Markdown grouping in visualizations]

* Reading Direction
+
Most people are used to reading from top to bottom. Place at the top of your page the most important charts and the ones that could give a brief and immediate summary of the context. A good general guidelines is to increase the level of detail as you approach the bottom of the dashboard. This way, users interested in getting all the information can obtain it without requiring too much effort, and other users can gather what they need from only a quick glance at the topmost dashboards.

* Central focal point
+
Placing a big chart at the center of a dashboard, especially one with prominent visual shapes such as rectangles, helps to reinforce a natural visual focal point that lies in the center of the interface.
+
image::images/rows-in-visualizations.png[Central focal point in visualization]

[discrete]
==== Use Margins

Kibana dashboards offer the possibility to apply margins between visualizations, and this is highly recommended.
Margins create separation between charts, which is an important visual feature, and they help users to identify when two elements belong together. At the same time, the added space makes the interface appear more clean and elegant.

[discrete]
=== Visualization Best Practices

Following are recommended best practices for designing Kibana vizualizations.

[discrete]
==== Lens vs TSVB visualizations

**Always use Lens**, when possible. It's the best choice to be consistent and up to date.

When possible, migrate dashboards from TSVB to Lens. If it's not possible, please engage with the Kibana team to identify any gaps that prevent full TSVB to Lens dashboard migration.

[discrete]
==== Visualizations should contain a filter

Kibana visualizations can define a filter to avoid performance issues when querying all metrics (`metrics-*`) or logs (`logs-*`) indices.

It is recommended to set a filter in each visualization at least by the required `data_stream.dataset`. For more details, refer to the the link:https://www.elastic.co/blog/an-introduction-to-the-elastic-data-stream-naming-scheme[Elastic data stream naming scheme].

As much as possible, avoid using general filters, that is filters with `-*`.  Combine multiple fields and values inside a filter with AND/OR operators. Although your filter might become more complex, it will avoid extra queries.

Example: 

image::images/filter-in-visualization.png[Filter in a visualization]

[discrete]
==== Do not use library visualizations

Do not use the visualizations that appear in **Analytics > Visualize library**. Instead, define visualizations as part of the dashboard. This is the default when creating new panels by clicking **Add new visualization** on the dashboard. If some panels are already saved to the library, you can unlink them and delete them from the library

There are some cases where library visualizations are preferable. It makes sense, for example, if a given visualization always has to be exactly the same on multiple dashboards or if its users frequently look at the visualization without looking at the whole dashboard.

[discrete]
=== Use dashboard-native controls

The **Input controls** visualization type is deprecated in favor of **Controls** embedded into the dashboard itself. The *Controls* dropdown in the Dashboard menu bar should be used. Refer to {kibana-ref}/add-controls.html[Filter dashboard data with controls] for more information.

[discrete]
==== Keep Consistent Color

Use color to distinguish categories, represent quantity/density, and highlight data. When using color in this way, be aware that too many colors in a single chart can create noise and hinder quick comprehension.

link:https://elastic.github.io/eui/#/elastic-charts/creating-charts[Elastic UI] provides guidance for correct color choice.
Colors provided there for visualization have been tested for accessibility contrast. By using them, you are sure properly serve the largest possible audience.

If your dashboard is made to identify specific behaviors, it might be interesting to consider a color setting that could help to point those out. Use a neutral color for generic elements and an accented color for the things that you want to highlight.

image::images/colors-in-visualizations.png[Colors in visualizations]

[discrete]
=== Titles in Visualisations matter

Titles can have a strong visual impact on dashboards, especially when there are a lot of small charts. Two principles can generally be followed:

* Remove unnecessary or repetitive titles when the information is already explained or written within the chart.
* When a title is needed, make it self explanatory and exhaustive. This way, you will be able to remove axis titles and other specifications leaving more space for the chart itself.

image::images/titles-in-visualizations.png[Titles in visualizations]

[[documentation-guidelines]]
== Documentation guidelines

The goal of each integration's documentation is to:

* Help the reader understand the benefits the integration offers and how Elastic can help with their use case.
Inform the reader of any requirements, including system compatibility, supported versions of third-party products, permissions needed, and more.
* Provide a comprehensive list of collected fields and the data and metric types for each. The reader can reference this information while evaluating the integration, interpreting collected data, or troubleshooting issues.
* Set the reader up for a successful installation and setup by connecting them with any other resources they'll need.
* Each integration document should contain several sections, and you should use consistent headings to make it easier for a single user to evaluate and use multiple integrations.

** <<idg-docs-guidelines-overview>>
** <<idg-docs-guidelines-datastreams>>
** <<idg-docs-guidelines-requirements>>
** <<idg-docs-guidelines-setup>>
** <<idg-docs-guidelines-troubleshooting>>
** <<idg-docs-guidelines-reference>>

Some considerations when these documentation files are written at `_dev/build/docs/*.md`:

* These files follow the Markdown syntax and leverage the use of link:https://github.com/elastic/elastic-package/blob/main/docs/howto/add_package_readme.md[documentation templates].
* There are some available functions or placeholders (`fields`, `event`, `url`) that can be used to help you write documentation. For more detail, refer to link:https://github.com/elastic/elastic-package/blob/main/docs/howto/add_package_readme.md#placeholders[placeholders].
* Regarding the `url` placeholder, this placeholder should be used to add links to the link:https://www.elastic.co/guide/index.html[Elastic documentation guides] in your documentation:
** The file containing all of the defined links is in the root of the directory: [`links_table.yml`](../links_table.yml)
** If needed, more links to Elastic documentation guides can be added into that file.
** Example usage:
*** In the documentation files (`_dev/build/docs/*.md`), `{{ url "getting-started-observability" "Elastic guide" }}` generates a link to the Observability Getting Started guide.

[discrete]
[[idg-docs-guidelines-overview]]
=== Overview

The overview section explains what the integration is, defines the third-party product that is providing data, establishes its relationship to the larger ecosystem of Elastic products, and helps the reader understand how it can be used to solve a tangible problem.

The overview should answer the following questions:

* What is the integration?
* What is the third-party product that is providing data?
* What can you do with it?
** General description
** Basic example

[discrete]
==== Template

Use this template language as a starting point, replacing `<placeholder text>` with details about the integration:

[source,text]
----
The <name> integration allows you to monitor <service>. <service> is <definition>.

Use the <name> integration to <function>. Then visualize that data in Kibana, create alerts to notify you if something goes wrong, and reference <data stream type> when troubleshooting an issue.

For example, if you wanted to <use case> you could <action>. Then you can <visualize|alert|troubleshoot> by <action>.
----

[discrete]
==== Example

[source,text]
----
The AWS CloudFront integration allows you to monitor your AWS CloudFront usage. AWS CloudFront is a content delivery network (CDN) service.

Use the AWS CloudFront integration to collect and parse logs related to content delivery. Then visualize that data in Kibana, create alerts to notify you if something goes wrong, and reference logs when troubleshooting an issue.

For example, you could use the data from this integration to know when there are more than some number of failed requests for a single piece of content in a given time period. You could also use the data to troubleshoot the underlying issue by looking at additional context in the logs like the number of unique users (by IP address) who experienced the issue, the source of the request, and more.
----

[discrete]
[[idg-docs-guidelines-datastreams]]
=== Datastreams

The data streams section provides a high-level overview of the kind of data that is collected by the integration. This is helpful since it can be difficult to quickly derive an understanding from just the reference sections (since they're so long).

The data streams section should include:

* A list of the types of data streams collected by the integration
* A summary of each type of data stream included and a link to the relevant reference section:
** Logs
** Metrics
* Notes (optional)

[discrete]
==== Template

Use this template language as a starting point, replacing `<placeholder text>` with details about the integration:

[source,text]
----
## Data streams

The <name> integration collects two types of data streams: logs and metrics.

**Logs** help you keep a record of events happening in <service>.
Log data streams collected by the <name> integration include <select data streams>, and more. See more details in the <Logs reference>.

**Metrics** give you insight into the state of <service>.
Metric data streams collected by the <name> integration include <select data streams> and more. See more details in the [Metrics]<#metrics-reference>.

<!-- etc. -->

<!-- Optional notes -->
----

[discrete]
==== Example

[source,text]
----
The System integration collects two types of data: logs and metrics.

Logs help you keep a record of events that happen on your machine. Log data streams collected by the System integration include application, system, and security events on machines running Windows or auth and syslog events on machines running macOS or Linux. See more details in the Logs reference.

Metrics give you insight into the state of the machine. Metric data streams collected by the System integration include CPU usage, load statistics, memory usage, information on network behavior, and more. See more details in the Metrics reference.

You can enable and disable individual data streams. If all data streams are disabled and the System integration is still enabled, Fleet uses the default data streams.
----

[discrete]
[[idg-docs-guidelines-requirements]]
=== Requirements

The requirements section helps readers to confirm that the integration will work with their systems.

* Elastic prerequisites (for example, a self-managed or Cloud deployment)
* System compatibility
* Supported versions of third-party products
* Permissions needed
* Anything else that could block a user from successfully using the integration

[discrete]
==== Template

Use this template language as a starting point, including any other requirements for the integration:

[source,text]
----
## Requirements

You need Elasticsearch for storing and searching your data and Kibana for visualizing and managing it.
You can use our hosted Elasticsearch Service on Elastic Cloud, which is recommended, or self-manage the Elastic Stack on your own hardware.

<!-- Other requirements -->
----

[discrete]
==== Example

[source,text]
----
You need Elasticsearch for storing and searching your data and Kibana for visualizing and managing it. You can use our hosted Elasticsearch Service on Elastic Cloud, which is recommended, or self-manage the Elastic Stack on your own hardware.

Each data stream collects different kinds of metric data, which may require dedicated permissions to be fetched and may vary across operating systems. Details on the permissions needed for each data stream are available in the Metrics reference.
----

For a much more detailed example, refer to the link:https://github.com/elastic/integrations/blob/main/packages/aws/_dev/build/docs/README.md#requirements[AWS integration requirements].

[discrete]
[[idg-docs-guidelines-setup]]
=== Setup

The setup section points the reader to the Observability {observability-guide}/observability-get-started.html[Getting started guide] for generic, step-by-step instructions.

This section should also include any additional setup instructions beyond what's included in the guide, which may include instructions to update the configuration of a third-party service. For example, for the Cisco ASA integration, users need to configure their Cisco device following the link:https://documentation.meraki.com/General_Administration/Monitoring_and_Reporting/Syslog_Server_Overview_and_Configuration#Configuring_a_Syslog_Server[steps found in the Cisco documentation].

NOTE: When possible, use links to point to third-party documentation for configuring non-Elastic products since workflows may change without notice.

[discrete]
==== Template

Use this template language as a starting point, including any other setup instructions for the integration:

[source,text]
----
## Setup

<!-- Any prerequisite instructions -->

For step-by-step instructions on how to set up an integration, see the
{{ url "getting-started-observability" "Getting started" }} guide.

<!-- Additional set up instructions -->
----

[discrete]
==== Example

[source,text]
----
Before sending logs to Elastic from your Cisco device, you must configure your device according to <<Cisco's documentation on configuring a syslog server>>.

After you've configured your device, you can set up the Elastic integration. For step-by-step instructions on how to set up an integration, see the <<Getting started>> guide.
----

[discrete]
[[idg-docs-guidelines-troubleshooting]]
=== Troubleshooting (optional)

The troubleshooting section is optional.
It should contain information about special cases and exceptions that aren't necessary for getting started or won't be applicable to all users.

[discrete]
==== Template

There is no standard format for the troubleshooting section.

[discrete]
==== Example

[source,text]
----
>Note that certain data streams may access `/proc` to gather process information,
>and the resulting `ptrace_may_access()` call by the kernel to check for
>permissions can be blocked by
>[AppArmor and other LSM software](https://gitlab.com/apparmor/apparmor/wikis/TechnicalDoc_Proc_and_ptrace), even though the System module doesn't use `ptrace` directly.
>
>In addition, when running inside a container the proc filesystem directory of the host
>should be set using `system.hostfs` setting to `/hostfs`.
----

[discrete]
[[idg-docs-guidelines-reference]]
=== Reference

Readers might use the reference section while evaluating the integration, interpreting collected data, or troubleshooting issues.

There can be any number of reference sections (for example, `## Metrics reference`, `## Logs reference`).
Each reference section can contain one or more subsections, such as one for each individual data stream (for example, `### Access Logs` and `### Error logs`).

Each reference section should contain detailed information about:

* A list of the log or metric types we support within the integration and a link to the relevant third-party documentation.
* (Optional) An example event in JSON format.
* Exported fields for logs, metrics, and events with actual types (for example, `counters`, `gauges`, `histograms` vs. `longs` and `doubles`). Fields should be generated using the instructions in link:https://github.com/elastic/integrations/blob/main/docs/fine_tune_integration.md[Fine-tune the integration].
// Note: Update the above link when the fine tuning Markdown docs have been migrated to this guide.
* ML Modules jobs.

[discrete]
==== Template

[source,text]
----
<!-- Repeat for both Logs and Metrics if applicable -->
## <Logs|Metrics> reference

<!-- Repeat for each data stream of the current type -->
### <Data stream name>

The `<data stream name>` data stream provides events from <source> of the following types: <list types>.

<!-- Optional -->
<!-- #### Example -->
<!-- An example event for `<data stream name>` looks as following: -->
<!-- <code block with example> -->

#### Exported fields

<insert table>
----

[discrete]
==== Example

[source,text]
----
>## Logs reference
>
>### PAN-OS
>
>The `panos` data stream provides events from Palo Alto Networks device of the following types: [GlobalProtect](https://docs.paloaltonetworks.com/pan-os/10-2/pan-os-admin/monitoring/use-syslog-for-monitoring/syslog-field-descriptions/globalprotect-log-fields), [HIP Match](https://docs.paloaltonetworks.com/pan-os/10-2/pan-os-admin/monitoring/use-syslog-for-monitoring/syslog-field-descriptions/hip-match-log-fields), [Threat](https://docs.paloaltonetworks.com/pan-os/10-2/pan-os-admin/monitoring/use-syslog-for-monitoring/syslog-field-descriptions/threat-log-fields), [Traffic](https://docs.paloaltonetworks.com/pan-os/10-2/pan-os-admin/monitoring/use-syslog-for-monitoring/syslog-field-descriptions/traffic-log-fields) and [User-ID](https://docs.paloaltonetworks.com/pan-os/10-2/pan-os-admin/monitoring/use-syslog-for-monitoring/syslog-field-descriptions/user-id-log-fields).
>
>#### Example
>
>An example event for `panos` looks as following:
>
>(code block)
>
>#### Exported fields
>
>(table of fields)
----
